{
  "topics": {
    "1": {
      "readingsComplete": [
        0,
        1
      ],
      "notes": "",
      "lastModified": 1762701355360,
      "readingUserNotes": {
        "0": "<h1 class=\"text-2xl font-bold mt-1 text-text-100\">Comprehensive Summary: LLM Foundations - Architecture, Attention, and Training</h1>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Model Architectures: The Three Pillars</h2>\n<p class=\"whitespace-normal break-words\"><strong>Encoder-Only (BERT)</strong>: Bidirectional architecture that excels at understanding language through complete context awareness. Best for classification, sentiment analysis, and tasks requiring deep comprehension without generation. The encoder sees the entire input simultaneously, making it powerful for discriminative tasks.</p>\n<p class=\"whitespace-normal break-words\"><strong>Decoder-Only (GPT)</strong>: Unidirectional architecture with context flowing forward only. Despite this limitation, GPT-style models achieve remarkable natural language understanding through generative pre-training. They handle textual entailment, question answering, and excel at generation tasks. Modern LLMs (GPT-4, Claude, LLaMA) universally adopt this architecture for its simplicity, scalability, and prompting flexibility.</p>\n<p class=\"whitespace-normal break-words\"><strong>Encoder-Decoder (T5, Sequence-to-Sequence)</strong>: Bidirectional encoder paired with unidirectional decoder. The \"text-to-text\" philosophy converts every language problem into input-text → output-text format. Ideal for translation, summarization, and question answering where clear source-target separation exists.</p>\n<p class=\"whitespace-normal break-words\"><strong>Mixture of Experts (MoE)</strong>: Architectural overlay applicable to any base architecture. Converts dense models to sparse by using multiple expert models with sparse gating functions that route inputs to top-K experts. Dramatically scales model capacity with minimal computational overhead, achieving efficiency during inference while generalizing well across tasks.</p>\n<p class=\"whitespace-normal break-words\"><strong>Multimodal Models</strong>: Combine separate encoders for different modalities (CNNs for images, transformers for text). Features from multiple modalities are fused or weighted via attention mechanisms. Joint representations capture cross-modal interactions for tasks like image captioning, text-to-image generation, and visual question answering.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Tokenization: From Text to Numbers</h2>\n<p class=\"whitespace-normal break-words\">Tokenization fragments text into basic building blocks called tokens, creating vocabulary mappings from tokens to numeric IDs suitable for deep learning.</p>\n<p class=\"whitespace-normal break-words\"><strong>The Process</strong>:</p>\n<ol class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-decimal space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Normalization</strong>: Lowercase conversion, punctuation handling, stemming, lemmatization, accent removal</li>\n<li class=\"whitespace-normal break-words\"><strong>Segmentation</strong>: Recognizing word/sentence boundaries at word, subword, or character granularity</li>\n</ol>\n<p class=\"whitespace-normal break-words\"><strong>Why Subword Tokenization Dominates</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Word-level tokenizers create massive vocabularies and struggle with out-of-vocabulary words</li>\n<li class=\"whitespace-normal break-words\">Character-level tokenizers produce overly long sequences with less meaningful individual tokens</li>\n<li class=\"whitespace-normal break-words\"><strong>Subword tokenizers</strong> split rare words into meaningful subunits based on common character n-grams, reducing vocabulary size while handling unseen words gracefully</li>\n</ul>\n<p class=\"whitespace-normal break-words\"><strong>Popular Algorithms</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>BPE (Byte Pair Encoding)</strong>: Starts with characters, iteratively merges frequent adjacent pairs, achieves compression and faster decoding</li>\n<li class=\"whitespace-normal break-words\"><strong>WordPiece</strong>: Similar to BPE but merges based on maximizing training data likelihood (probabilistic approach)</li>\n<li class=\"whitespace-normal break-words\"><strong>Unigram</strong>: Starts with large vocabulary, removes tokens based on loss function until reaching desired size</li>\n<li class=\"whitespace-normal break-words\"><strong>SentencePiece</strong>: Learns subword units from raw text using Unigram or BPE, operates on raw text without pre-tokenization</li>\n</ul>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Attention Mechanisms: The Heart of Transformers</h2>\n<p class=\"whitespace-normal break-words\"><strong>Why Attention Emerged</strong>: Traditional RNN encoder-decoder models couldn't scale with input sequence length. Attention enabled decoders to selectively weight the most relevant input tokens rather than compressing everything into fixed vectors.</p>\n<p class=\"whitespace-normal break-words\"><strong>Self-Attention (Scaled Dot-Product Attention)</strong>:\nCreates context-aware representations by allowing each token to attend to all others in the same sequence. Each input token projects into Query (Q), Key (K), and Value (V) matrices. The mechanism computes attention scores via scaled dot-product of Q and K, then weights the V matrix accordingly. This produces context-enriched representations where \"bank\" near \"river\" differs from \"bank\" near \"deposits.\"</p>\n<p class=\"whitespace-normal break-words\"><strong>Multi-Head Attention</strong>:\nUses multiple attention heads in parallel, functioning like CNN kernels attending to different aspects of the sequence. Different heads specialize in various linguistic phenomena—syntax, semantics, short-range vs. long-range dependencies, coreference resolution. This emergent specialization happens naturally during training.</p>\n<p class=\"whitespace-normal break-words\"><strong>Cross-Attention</strong>:\nInstead of single input sequence (self-attention), cross-attention involves two sequences—typically encoder output and decoder state. This creates the bridge in encoder-decoder models, allowing the decoder to dynamically focus on relevant source positions while generating each target token.</p>\n<p class=\"whitespace-normal break-words\"><strong>FlashAttention</strong>:\nOptimizes the quadratic memory bottleneck of attention by using tiling to load Q, K, V blocks from GPU HBM (slow) to SRAM (fast) for computation, then writing back. Avoids storing large attention matrices during forward pass by recomputing during backpropagation in SRAM. Achieves 2-4x speedup for longer sequences. FlashAttention-2 doubles this with sequence parallelism and better work partitioning.</p>\n<p class=\"whitespace-normal break-words\"><strong>Multi-Query Attention (MQA)</strong>:\nMultiple query heads attend to the <strong>same</strong> single head of key and value projections. Dramatically reduces KV cache size and memory bandwidth during autoregressive decoding, enabling faster inference with minor quality degradation compared to multi-head attention.</p>\n<p class=\"whitespace-normal break-words\"><strong>Grouped-Query Attention (GQA)</strong>:\nInterpolates between multi-head and multi-query attention by sharing single K and V heads across <strong>groups</strong> of query heads (not all heads). Overcomes MQA's quality degradation while retaining inference speedup. Existing multi-head models can adopt GQA through up-training with just 5% of original training compute—no full retraining required.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Positional Encoding: Teaching Models About Order</h2>\n<p class=\"whitespace-normal break-words\">Word order matters crucially in language. Positional encoding assigns position information to each token since transformers process sequences in parallel without inherent order awareness.</p>\n<p class=\"whitespace-normal break-words\"><strong>Sinusoidal Positional Encoding (Original Transformer)</strong>:\nCombines absolute position encoding with word embeddings using sinusoidal functions. <strong>Limitation</strong>: Cannot extrapolate to sequences longer than those seen during training, restricting real-world applications.</p>\n<p class=\"whitespace-normal break-words\"><strong>Relative Position Encoding</strong>:\nCombines content representations (Q and K vectors) with trainable positional representations based on relative distance between query and key (clipped beyond certain distance). Enables length flexibility.</p>\n<p class=\"whitespace-normal break-words\"><strong>RoPE (Rotary Position Embeddings)</strong>:\nSynthesizes absolute and relative position embeddings. Encodes absolute position using rotation matrices while incorporating relative position dependency in self-attention multiplicatively. Provides sequence length flexibility with decaying inter-token dependency as distance increases, enabling extrapolation to longer sequences at inference.</p>\n<p class=\"whitespace-normal break-words\"><strong>ALiBi (Attention with Linear Biases)</strong>:\nDoesn't add positional embeddings to word embeddings. Instead, biases query-key attention scores with penalty proportional to distance between them. The penalty increases linearly with distance. No additional parameters, negligible memory increase, and strong extrapolation to sequences far longer than training. Avoids \"early token curse\" and efficiently exploits longer context histories.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Training at Scale: Parallelism Strategies</h2>\n<p class=\"whitespace-normal break-words\">Training billion-parameter models on trillions of tokens requires sophisticated memory and compute optimization.</p>\n<p class=\"whitespace-normal break-words\"><strong>Tensor Parallelism (Intra-Layer)</strong>:\nSplits individual operations (like matrix multiplication) across GPUs. Each GPU handles part of the computation within a single layer. Requires additional communication to synchronize results but enables massive layers that wouldn't fit on single GPUs.</p>\n<p class=\"whitespace-normal break-words\"><strong>Pipeline Parallelism (Inter-Layer)</strong>:\nSplits model layers across GPUs—each device computes its assigned layers and passes activations to the next stage. <strong>Challenge</strong>: \"Bubble time\" where some devices wait idle while others compute, wasting resources.</p>\n<p class=\"whitespace-normal break-words\"><strong>Sequence Parallelism</strong>:\nExpands tensor parallelism by recognizing that some transformer components (not previously parallelized) are independent along the sequence dimension. Splitting these along sequence dimension distributes both compute and activation memory across devices. Smaller distributed activations mean more can be saved for backward pass.</p>\n<p class=\"whitespace-normal break-words\"><strong>Selective Activation Recomputation</strong>:\nComplements sequence parallelism. Instead of checkpointing entire transformer layers (memory-intensive), checkpoints only parts with high memory footprint but low recomputation cost. Different activations require different recomputation overhead—this technique optimizes the tradeoff.</p>\n<p class=\"whitespace-normal break-words\"><strong>Data Parallelism</strong>:\nSplits dataset into shards, each device gets full model copy and trains on its shard. After backpropagation, gradients are all-reduced to synchronize model parameters across devices. Parallelizes along batch dimension.</p>\n<p class=\"whitespace-normal break-words\"><strong>Fully Sharded Data Parallelism (FSDP)</strong>:\nShards both model parameters and training data uniformly across workers. Computation for each micro-batch is local to each GPU. Configurable sharding strategies match physical network topology. Overlaps communication with computation through operation reordering and parameter prefetching. Optimizes memory by limiting inflight unsharded parameters. Achieves near-linear scalability for significantly larger models.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Quantization Aware Training (QAT)</h2>\n<p class=\"whitespace-normal break-words\"><strong>Quantization</strong>: Running model computations in reduced precision (INT8, INT4) instead of full precision (FP32/FP16) for faster inference, lower memory, and reduced costs.</p>\n<p class=\"whitespace-normal break-words\"><strong>QAT Approach</strong>:\nIncorporates quantization effects during training itself. Forward pass quantizes weights and activations to low-precision representations, mimicking deployment conditions. Backward pass computes gradients using full-precision values. Model learns parameters robust to quantization errors, resulting in trained models that maintain accuracy when quantized post-training. Superior to post-training quantization which can cause significant accuracy degradation.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Key Architectural Tradeoffs&nbsp;</h2>\n<p class=\"whitespace-normal break-words\"><strong>When to use Encoder-Only</strong>: Tasks requiring bidirectional understanding without generation—classification, named entity recognition, sentiment analysis.</p>\n<p class=\"whitespace-normal break-words\"><strong>When to use Decoder-Only</strong>: General-purpose LLMs, generation tasks, and any scenario requiring flexible prompting. Dominant for modern large-scale models due to simplicity and scalability.</p>\n<p class=\"whitespace-normal break-words\"><strong>When to use Encoder-Decoder</strong>: Clear input-output transformation with different structures/modalities—translation, abstractive summarization, speech recognition, image captioning.</p>\n<p class=\"whitespace-normal break-words\"><strong>MQA vs. GQA</strong>: MQA for maximum inference speed with acceptable quality loss; GQA for near-multi-head quality with most of MQA's speed benefits.</p>\n<p class=\"whitespace-normal break-words\"><strong>RoPE vs. ALiBi</strong>: RoPE for combining absolute and relative positioning with good extrapolation; ALiBi for superior extrapolation to very long sequences without positional embeddings.</p>\n<p class=\"whitespace-normal break-words\"><strong>Parallelism Strategy</strong>: Combine tensor (for layers too large for single GPU), pipeline (for very deep models), sequence (for long sequences), and data parallelism (for training throughput). Use FSDP for massive models requiring parameter sharding.</p>",
        "1": "<div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h1 class=\"text-2xl font-bold mt-1 text-text-100\">\"Attention Is All You Need\" - The Paper That Changed Everything</h1></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Big Idea (In Plain English)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Before this 2017 paper, everyone building language models used <strong>recurrent neural networks (RNNs)</strong> - models that read text one word at a time, like you reading left-to-right. The problem? They're slow to train because you can't parallelize them (you have to wait for word 1 before processing word 2), and they forget information from early in long sentences.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Vaswani and team at Google asked</strong>: What if we threw out RNNs entirely and built everything from attention mechanisms alone?</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The answer was the <strong>Transformer</strong> - and it revolutionized AI.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Core Problem with Old Models (RNNs/LSTMs)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Sequential bottleneck</strong>: RNNs process text sequentially - word 1, then word 2, then word 3. You can't parallelize this. It's like reading a book one word at a time with your finger, never looking ahead.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Information loss</strong>: By the time an RNN finishes reading a 50-word sentence, it has compressed everything into a single hidden state vector. Early words get \"forgotten\" or overwritten.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Distance matters too much</strong>: For an RNN to connect word 1 to word 50, the signal has to pass through 49 intermediate steps. Information degrades over that distance.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Even with attention mechanisms added to RNNs (which was already happening), you still had the sequential processing problem.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Revolutionary Solution: Pure Attention</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The Transformer's radical move: <strong>remove recurrence entirely</strong>. Build everything from attention mechanisms that let every word directly look at every other word simultaneously. No sequential processing. No information bottleneck.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This is like going from reading word-by-word with your finger to seeing the entire page at once and letting your eyes jump to any word that matters for understanding context.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Architecture Breakdown</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Encoder (6 Stacked Layers)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each encoder layer has two components:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>1. Multi-Head Self-Attention</strong>: Every word looks at every other word in the input simultaneously. When processing \"bank\" in \"river bank,\" it can immediately see \"river\" and understand context. With 8 attention heads running in parallel, different heads learn different patterns - syntax, semantics, long-range dependencies.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>2. Feed-Forward Network</strong>: After attention enriches each word's representation, a simple neural network processes each position independently to add non-linear transformations.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Both components use <strong>residual connections</strong> (add input to output) and <strong>layer normalization</strong> (stabilize training).</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Decoder (6 Stacked Layers)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each decoder layer has three components:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>1. Masked Self-Attention</strong>: The decoder looks at its own partial output, but with a critical restriction - position 5 can only see positions 1-4, not future positions. This prevents \"cheating\" during training and ensures sequential generation during inference.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>2. Cross-Attention</strong>: This is where the magic happens. The decoder queries the encoder's output to figure out which source words matter for the current generation step. When translating \"bank\" to \"banque,\" cross-attention focuses the decoder on \"bank\" in the English source.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>3. Feed-Forward Network</strong>: Same as encoder - position-wise processing.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">The Attention Mechanism (Scaled Dot-Product)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The mathematical formula is simple but powerful:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention(Q, K, V) = softmax(QK^T / √d_k) × V</strong></p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Breaking this down:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Q (Query)</strong>: \"What am I looking for?\"</li>\n<li class=\"whitespace-normal break-words\"><strong>K (Key)</strong>: \"What information do I have?\"</li>\n<li class=\"whitespace-normal break-words\"><strong>V (Value)</strong>: \"What's the actual information?\"</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The model computes similarity scores between queries and keys (QK^T), scales them by √d_k to prevent gradients from vanishing, applies softmax to get weights (which sum to 1), then takes a weighted sum of values.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Why scaling by √d_k matters</strong>: Without it, dot products get very large for high-dimensional vectors, pushing softmax into regions with tiny gradients where learning stops.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Multi-Head Attention (8 Heads)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Instead of one attention mechanism, they use 8 parallel ones with different learned parameters. It's like having 8 different students read the same sentence, each focusing on different aspects:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Head 1: syntactic relationships</li>\n<li class=\"whitespace-normal break-words\">Head 2: semantic similarity</li>\n<li class=\"whitespace-normal break-words\">Head 3: coreference (pronouns)</li>\n<li class=\"whitespace-normal break-words\">Head 4: positional patterns</li>\n<li class=\"whitespace-normal break-words\">And so on...</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each head uses d_k = d_v = 64 dimensions (512 total / 8 heads). Total compute cost stays similar to single-head attention with full 512 dimensions.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Positional Encoding (Critical Innovation)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Problem</strong>: Attention has no inherent sense of word order. \"Dog bites man\" vs. \"Man bites dog\" would look identical to pure attention.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Solution</strong>: Add positional information to word embeddings using sine and cosine functions:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</li>\n<li class=\"whitespace-normal break-words\">PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Why sinusoidal functions? They create smooth patterns where position k+offset can be represented as a linear function of position k. This theoretically allows the model to extrapolate to longer sequences than seen during training.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">They also tested learned positional embeddings and got nearly identical results, but kept sinusoids for potential extrapolation benefits.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Why This Works Better Than RNNs</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">1. Parallelization</h3>\n<p class=\"whitespace-normal break-words\"><strong>RNN</strong>: Must process 50 words sequentially = 50 sequential operations\n<strong>Transformer</strong>: Processes all 50 words simultaneously = O(1) sequential operations</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This makes training <strong>dramatically</strong> faster. What took days now takes hours.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">2. Constant Path Length</h3>\n<p class=\"whitespace-normal break-words\"><strong>RNN</strong>: Connecting word 1 to word 50 requires signal to pass through 49 intermediate hidden states = O(n) path length\n<strong>Transformer</strong>: Every word directly attends to every other word = O(1) path length</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Shorter paths mean easier learning of long-range dependencies.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">3. Computational Complexity</h3>\n<p class=\"whitespace-normal break-words\">For typical sentence lengths (n &lt; d, where n = sequence length, d = dimension):</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Self-attention</strong>: O(n² × d) - quadratic in sequence length but parallelizable</li>\n<li class=\"whitespace-normal break-words\"><strong>RNN</strong>: O(n × d²) - linear in sequence but sequential</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Self-attention is faster when sentences are shorter than the model's hidden dimension (which is almost always true - sentences are typically 20-50 words, dimensions are 512-1024).</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">4. Better Long-Range Modeling</h3>\n<p class=\"whitespace-normal break-words\">RNNs struggle to remember information from 30+ steps ago even with LSTMs. Transformers directly connect all positions, so remembering is trivial - it's just an attention weight.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Training Recipe</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Dataset</strong>: WMT 2014 English-German (4.5M sentence pairs) and English-French (36M sentence pairs)</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Tokenization</strong>: Byte-pair encoding with 37K token vocabulary (shared between source and target for German, 32K for French)</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Hardware</strong>: 8 NVIDIA P100 GPUs</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Base model: 0.4 seconds per step, trained 100K steps (12 hours total)</li>\n<li class=\"whitespace-normal break-words\">Big model: 1.0 seconds per step, trained 300K steps (3.5 days total)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Hyperparameters</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">N = 6 layers (both encoder and decoder)</li>\n<li class=\"whitespace-normal break-words\">d_model = 512 (base) or 1024 (big)</li>\n<li class=\"whitespace-normal break-words\">d_ff = 2048 (base) or 4096 (big) - feed-forward inner dimension</li>\n<li class=\"whitespace-normal break-words\">h = 8 (base) or 16 (big) attention heads</li>\n<li class=\"whitespace-normal break-words\">Dropout = 0.1 (base) or 0.3 (big)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Optimizer</strong>: Adam with custom learning rate schedule:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Warm up: Linear increase for first 4,000 steps</li>\n<li class=\"whitespace-normal break-words\">Then decay: Proportional to inverse square root of step number</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This warmup is crucial - starting with high learning rates destabilizes training for Transformers.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Regularization</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Dropout on attention weights, feed-forward outputs, and embeddings</li>\n<li class=\"whitespace-normal break-words\">Label smoothing (ε = 0.1) - makes model less confident, improves generalization</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Results (Mind-Blowing for 2017)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Machine Translation</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>English-to-German</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Previous best: 26.36 BLEU (ConvS2S ensemble)</li>\n<li class=\"whitespace-normal break-words\">Transformer big: <strong>28.4 BLEU</strong> (+2.0 improvement!)</li>\n<li class=\"whitespace-normal break-words\">Training cost: <strong>1/10th</strong> of previous state-of-the-art</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>English-to-French</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Previous best: 41.29 BLEU (ConvS2S ensemble)</li>\n<li class=\"whitespace-normal break-words\">Transformer big: <strong>41.8 BLEU</strong></li>\n<li class=\"whitespace-normal break-words\">Training cost: <strong>1/4th</strong> of previous state-of-the-art</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Even the base Transformer model (smaller, faster) beat all previous single models and most ensembles.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">English Constituency Parsing (Generalization Test)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">To prove this wasn't just good for translation, they tested on a completely different task - parsing sentence structure.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>WSJ only (40K sentences)</strong>: 91.3 F1 score - competitive with specialized models</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Semi-supervised (17M sentences)</strong>: 92.7 F1 score - beat almost everything except one specialized generative model</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This demonstrated that Transformers <strong>generalize</strong> across different NLP tasks, not just translation.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Model Variations (Ablation Studies)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">They tested different configurations to understand what matters:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention heads</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">1 head: 0.9 BLEU worse than 8 heads</li>\n<li class=\"whitespace-normal break-words\">16-32 heads: Quality drops (too much parallelism fragments learning)</li>\n<li class=\"whitespace-normal break-words\"><strong>Sweet spot: 8 heads</strong></li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Model size</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Bigger models consistently better (as expected)</li>\n<li class=\"whitespace-normal break-words\">Big model (213M params) outperformed base (65M params)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention key dimension</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Reducing d_k hurts performance</li>\n<li class=\"whitespace-normal break-words\">Suggests determining compatibility isn't trivial - dot product works but needs sufficient dimensionality</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Dropout</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Critical for avoiding overfitting</li>\n<li class=\"whitespace-normal break-words\">Without dropout, performance degraded significantly</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Positional encoding</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Sinusoidal vs. learned embeddings performed nearly identically</li>\n<li class=\"whitespace-normal break-words\">Kept sinusoidal for theoretical extrapolation properties</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Why This Paper Changed Everything</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">1. <strong>Killed RNNs for NLP</strong></h3>\n<p class=\"whitespace-normal break-words\">Within 2 years, virtually all state-of-the-art NLP models used Transformers. RNNs became legacy technology.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">2. <strong>Enabled Massive Scaling</strong></h3>\n<p class=\"whitespace-normal break-words\">The parallelization breakthrough meant you could train much larger models on much more data. This led directly to:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">BERT (2018) - encoder-only Transformer</li>\n<li class=\"whitespace-normal break-words\">GPT-2/3 (2019/2020) - decoder-only Transformer</li>\n<li class=\"whitespace-normal break-words\">T5 (2019) - encoder-decoder Transformer</li>\n<li class=\"whitespace-normal break-words\">Modern LLMs with billions/trillions of parameters</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">3. <strong>Attention Became Universal</strong></h3>\n<p class=\"whitespace-normal break-words\">Self-attention is now used everywhere - not just NLP, but computer vision (Vision Transformers), speech (Whisper), multimodal models (CLIP, DALL-E), reinforcement learning, protein folding (AlphaFold)...</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">4. <strong>Established Core Principles</strong></h3>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Attention &gt; Recurrence for sequence modeling</li>\n<li class=\"whitespace-normal break-words\">Parallelization matters more than architectural complexity</li>\n<li class=\"whitespace-normal break-words\">Positional encoding can replace sequential processing</li>\n<li class=\"whitespace-normal break-words\">Multi-head attention captures diverse patterns</li>\n<li class=\"whitespace-normal break-words\">Residual connections + layer norm stabilize deep networks</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Limitations They Identified (Foreshadowing Future Work)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Quadratic cost</strong>: Self-attention is O(n²) in sequence length. For very long sequences (thousands of tokens), this becomes prohibitive. They noted that restricted attention to local neighborhoods could help.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This limitation drove later innovations:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Sparse attention patterns (Reformer, Longformer)</li>\n<li class=\"whitespace-normal break-words\">Linear attention approximations</li>\n<li class=\"whitespace-normal break-words\">Flash Attention (memory optimization you learned about)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Sequential decoding</strong>: Despite parallelizing training, generation is still sequential (one token at a time). Each new token requires a full forward pass.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This led to:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Speculative decoding</li>\n<li class=\"whitespace-normal break-words\">Non-autoregressive models</li>\n<li class=\"whitespace-normal break-words\">Continuous efforts to speed up inference</li></ul></div></div>"
      },
      "subtopicStudyGuides": {
        "0": "<h1 class=\"text-2xl font-bold mt-1 text-text-100\">Deep Dive: Encoder-Decoder Architectures for LLMs</h1>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Core Intuition</h2>\n<p class=\"whitespace-normal break-words\">Imagine you're translating a sentence from English to French. You don't translate word-by-word as you read—instead, you first read and <em>understand</em> the entire English sentence, holding its meaning in your mind, and then you express that meaning in French. This two-stage process of \"understand, then generate\" is exactly what encoder-decoder architectures formalize.</p>\n<p class=\"whitespace-normal break-words\">The encoder-decoder paradigm emerged from a fundamental insight: many AI tasks involve transforming one sequence into another sequence, where the input and output can have different lengths, different structures, and even different modalities. Traditional neural networks struggled with this because they needed fixed-size inputs and outputs. The encoder-decoder architecture solved this elegantly by separating the problem into two stages: first compress the input into a meaningful representation (encoding), then expand that representation into the desired output (decoding).</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Architecture: A Tale of Two Modules</h2>\n<h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">The Encoder: Building Understanding</h3>\n<p class=\"whitespace-normal break-words\">The encoder's job is to read the input sequence and build a rich, compressed representation of its meaning. Think of it as a student reading a textbook chapter and taking detailed notes that capture all the important concepts. In the original sequence-to-sequence models, the encoder was typically a recurrent neural network (RNN or LSTM) that processed the input token by token, updating its hidden state at each step. The final hidden state was meant to contain a summary of the entire input sequence.</p>\n<p class=\"whitespace-normal break-words\">However, this approach had a critical flaw: compressing an entire sequence into a single fixed-size vector creates an information bottleneck. Imagine trying to summarize a 50-word sentence in just a few numbers—you'd inevitably lose important details. This is where the attention mechanism revolutionized the field, and subsequently where Transformers took over.</p>\n<p class=\"whitespace-normal break-words\">In modern Transformer-based encoder-decoders, the encoder doesn't compress everything into a single vector. Instead, it produces a sequence of contextualized representations—one for each input token.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Real Applications: Where Encoder-Decoder Shines</h2><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Machine Translation: The Original Use Case</h3><p class=\"whitespace-normal break-words\">Machine translation is the canonical encoder-decoder application. The task has a clear structure: you have a complete source sentence in one language, and you need to produce a complete target sentence in another language. The encoder processes the entire source sentence, building representations that capture not just individual word meanings but also grammatical structure, idioms, and context. The decoder then generates the translation, using cross-attention to align with the source.</p><p class=\"whitespace-normal break-words\">What makes this work so well is that translation requires understanding the <em>entire</em> source before generating. Consider translating \"The bank approved the loan\" versus \"She sat on the bank.\" The word \"bank\" needs completely different translations depending on context, and the encoder's bidirectional self-attention captures this.</p><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Abstractive Summarization: Creating New Text</h3><p class=\"whitespace-normal break-words\">\n\n\n\n\n</p><p class=\"whitespace-normal break-words\">Summarization is fascinating because the decoder isn't just selecting words from the input—it's generating new phrases that capture the essence of a longer text. Models like BART and PEGASUS excel here. BART (Bidirectional and Auto-Regressive Transformer) is particularly clever: it's trained by corrupting documents (masking spans, shuffling sentences, etc.) and then learning to reconstruct the original. This denoising objective teaches the model both to understand corrupted text (encoder) and to generate clean text (decoder).</p><p class=\"whitespace-normal break-words\"><br></p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Core Intuition: Two Minds Working Together</h2><p class=\"whitespace-normal break-words\">Imagine you're a professional translator at the United Nations. When a French diplomat speaks, you don't interrupt them mid-sentence to start translating. Instead, you listen carefully to their complete thought—understanding the grammar, capturing the nuance, grasping the full context of what they're saying. Only after you've fully comprehended their message do you begin speaking in English, drawing on your understanding to produce a natural, flowing translation that captures both meaning and intent.</p><p class=\"whitespace-normal break-words\">This is exactly how encoder-decoder architectures work. They formalize the intuition that understanding and generation are fundamentally different cognitive processes that benefit from specialized handling. The encoder is like your listening comprehension—it takes in the entire input, processes it bidirectionally (looking both forward and backward), and builds a rich internal representation of meaning. The decoder is like your speaking production—it generates output sequentially, one word at a time, constantly referring back to that understanding to stay faithful to the source.</p><p class=\"whitespace-normal break-words\">The revolutionary insight that led to encoder-decoder models was recognizing that many AI tasks involve <strong>sequence transduction</strong>: transforming one sequence into another where the relationship between input and output isn't simple or one-to-one. You can't just map the fifth word of an English sentence to the fifth word of its French translation because languages have different word orders, different ways of expressing concepts, and different grammatical structures. The encoder-decoder architecture handles this elegantly by decoupling comprehension from generation.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Historical Journey: From Bottlenecks to Breakthroughs</h2><p class=\"whitespace-normal break-words\">Early sequence-to-sequence models, introduced by Sutskever, Vinyals, and Le at Google in 2014, used recurrent neural networks for both encoder and decoder. The encoder would read the input sentence word by word, updating a hidden state vector at each step. This final hidden state was meant to capture everything about the input—its meaning, its structure, its context. The decoder would then take this single vector and unroll it into the output sequence.</p><p class=\"whitespace-normal break-words\">The problem was immediately obvious: you're trying to compress an entire sentence, paragraph, or document into a single fixed-size vector. Imagine trying to summarize a 50-word sentence about quantum mechanics into just 512 numbers. Critical information gets lost. The model struggles with long sequences because by the time it finishes encoding, the beginning has been compressed away, overwritten by later words.</p><p class=\"whitespace-normal break-words\">This is where <strong>attention</strong> changed everything. In 2014, Bahdanau, Cho, and Bengio introduced attention mechanisms for neural machine translation. Instead of compressing everything into one vector, the encoder now produces a sequence of vectors—one for each input word—and the decoder can look back at all of them. At each generation step, the decoder computes attention scores that essentially ask: \"Given what I'm trying to say right now, which parts of the input should I focus on?\"</p><p class=\"whitespace-normal break-words\">Consider translating \"The old man the boats\" to French. This is a famous garden-path sentence where \"man\" is actually a verb meaning \"to operate.\" A human translator needs to read the full sentence to understand this, then produce \"Les personnes âgées manoeuvrent les bateaux.\" When the decoder generates \"manoeuvrent\" (operate), attention allows it to focus specifically on \"man\" in the source, understanding from context that it's a verb, not a noun. Without attention, this contextual understanding would be lost in the compression.</p><p class=\"whitespace-normal break-words\">The Transformer architecture, introduced by Vaswani et al. in their landmark 2017 paper \"Attention Is All You Need,\" took attention to its logical conclusion: what if attention was <em>all</em> you used? They removed recurrence entirely, replacing it with self-attention mechanisms that allow every word to directly interact with every other word, regardless of distance. This solved both the compression problem and the sequential processing bottleneck that made RNNs slow to train.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Encoder: Building Deep Understanding</h2><p class=\"whitespace-normal break-words\">The encoder's job is deceptively simple: read the input and understand it. But \"understanding\" in this context means something quite specific and powerful. The encoder must build representations where each word's meaning is enriched by its full context—grammatical, semantic, and pragmatic.</p><p class=\"whitespace-normal break-words\">Let's explore this with a concrete example. Consider the sentence: \"The bank can guarantee deposits will eventually cover future transactions.\" The word \"bank\" is ambiguous—it could be a financial institution or a riverbank. \"Guarantee\" could be a verb or a noun. \"Cover\" could mean financial coverage or physical covering. The encoder must resolve all these ambiguities before any translation or summarization can occur.</p><p class=\"whitespace-normal break-words\">The encoder uses <strong>self-attention</strong> to achieve this. Every word attends to every other word in the sentence. When processing \"bank,\" the self-attention mechanism looks at all surrounding words and notices \"deposits,\" \"guarantee,\" and \"transactions\"—strong signals that this is a financial bank, not a geographical feature. The representation of \"bank\" that emerges is therefore deeply contextualized; it doesn't just represent the abstract concept of \"bank\" but specifically \"bank in the context of this financial sentence.\"</p><p class=\"whitespace-normal break-words\">This happens in multiple layers, with each layer building increasingly abstract representations. The first layer might capture basic syntax—\"bank\" is a noun, \"can\" is an auxiliary verb, \"guarantee\" is the main verb. The second layer might capture clause structure—there's a main clause \"the bank can guarantee deposits\" and a subordinate clause \"deposits will eventually cover future transactions.\" Higher layers capture semantic roles, relationships between entities, and pragmatic meaning.</p><p class=\"whitespace-normal break-words\">What makes Transformer encoders so powerful is that this attention is bidirectional and global. Unlike recurrent models that process left-to-right, encoders can look at the entire sentence simultaneously. When encoding \"bank\" at position 1, the model can see \"transactions\" at position 10, immediately accessing long-range context. This is why BERT (Bidirectional Encoder Representations from Transformers) was so revolutionary—its encoder could truly understand language bidirectionally, something impossible for earlier left-to-right models.</p><p class=\"whitespace-normal break-words\">The output of the encoder is a sequence of contextualized embeddings, sometimes called \"memory\" or \"encoder states.\" For our example sentence, you'd have rich representations for each of the ten tokens, where each representation encodes not just that word but how it relates to and depends on every other word in the sequence.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Decoder: Generating with Guidance</h2><p class=\"whitespace-normal break-words\">The decoder's task is more constrained and more complex than you might think. It needs to generate an output sequence one token at a time, but it must do this while maintaining three critical properties: coherence with what it's already generated, faithfulness to the source input, and fluency in the target language.</p><p class=\"whitespace-normal break-words\">Let's walk through generating a French translation of \"The bank can guarantee deposits\" → \"La banque peut garantir les dépôts\" step by step to see what the decoder does at each moment.</p><p class=\"whitespace-normal break-words\"><strong>Step 1</strong>: The decoder starts with a special start-of-sequence token. It uses self-attention to process this token (though there's not much to process yet), then uses <strong>cross-attention</strong> to look at all the encoder's representations of the English sentence. The cross-attention mechanism computes: \"I'm about to generate the first word of French output—which parts of the English input are most relevant?\" The attention focuses heavily on \"The\" and \"bank.\" The decoder generates \"La\" (the).</p><p class=\"whitespace-normal break-words\"><strong>Step 2</strong>: Now the decoder has [\"La\"]. It uses self-attention on this short sequence to understand its own partial output, then cross-attention back to the English. This time, having already produced the determiner, the attention shifts strongly to \"bank.\" The decoder generates \"banque.\"</p><p class=\"whitespace-normal break-words\"><strong>Step 3</strong>: With [\"La\", \"banque\"], the decoder's self-attention recognizes it has a noun phrase. Cross-attention now focuses on \"can,\" identifying the next element to translate. It generates \"peut.\"</p><p class=\"whitespace-normal break-words\"><strong>Step 4</strong>: The pattern continues. At [\"La\", \"banque\", \"peut\"], cross-attention focuses on \"guarantee.\" The decoder generates \"garantir.\"</p><p class=\"whitespace-normal break-words\"><strong>Step 5</strong>: Having translated the main verb, cross-attention shifts to \"deposits.\" The decoder generates \"les.\"</p><p class=\"whitespace-normal break-words\"><strong>Step 6</strong>: Finally, with \"les\" produced, attention focuses on the noun \"deposits\" to generate \"dépôts,\" and then produces an end-of-sequence token.</p><p class=\"whitespace-normal break-words\">The crucial mechanism here is <strong>cross-attention</strong>, which implements a learnable, soft alignment between source and target sequences. Unlike traditional machine translation that used hard alignments (word 1 maps to word 1, etc.), cross-attention learns that sometimes one word maps to multiple words, or multiple words map to one word, or the order completely changes. In German-to-English translation, the verb often moves from the end of a German clause to early in the English clause—cross-attention handles this naturally.</p><p class=\"whitespace-normal break-words\">What's particularly elegant is that the decoder uses <strong>causal self-attention</strong>, meaning when generating position 5, it can only look at positions 1-4 of its own output, not future positions. This prevents \"cheating\" during training and ensures the model learns to generate sequentially, as it must do during actual inference. The decoder is effectively learning: \"Given what I've said so far and what the encoder understood from the input, what should I say next?\"</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Multi-Head Attention: Multiple Perspectives on Meaning</h2><p class=\"whitespace-normal break-words\">Both encoder and decoder use multiple attention heads in parallel, and this design choice reveals something profound about language understanding. Different linguistic phenomena require attending to different aspects of the input simultaneously.</p><p class=\"whitespace-normal break-words\">Consider the sentence: \"She told her sister that she loved her husband.\" There are multiple \"her\"s and \"she\"s, and resolving the references requires different types of attention. One attention head might focus on syntactic structure, recognizing that \"she\" as subject of \"loved\" likely refers back to \"she\" who \"told.\" Another head might focus on semantic plausibility—\"her husband\" most likely belongs to \"her sister,\" not to \"she\" who's doing the telling, because the sentence structure suggests new information. A third head might focus on discourse coherence patterns it learned during training.</p><p class=\"whitespace-normal break-words\">Research by analyzing attention patterns in trained models has revealed fascinating specialization. Some heads learn to pay attention primarily to the previous token (capturing local context). Some heads learn to attend to syntactically related words—the head of a phrase attends to its modifiers. Some heads learn semantic relationships, attending between co-referring entities across long distances. This emergent specialization happens naturally through training; the model discovers that having multiple parallel attention mechanisms with different learned parameters allows it to capture richer representations.</p><p class=\"whitespace-normal break-words\">For your earnings call analysis project, imagine encoding an earnings transcript. Different attention heads might specialize in: numerical relationships (linking \"revenue\" to its specific figure), temporal information (connecting quarterly comparisons), entity relationships (tracking mentions of the same product line), and sentiment markers (connecting hedging language like \"challenging environment\" to specific business segments).</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Magic of Cross-Attention: Aligning Source and Target</h2><p class=\"whitespace-normal break-words\">Cross-attention is where the \"encoder-decoder\" architecture truly earns its name. This mechanism creates a dynamic bridge between what the encoder understood and what the decoder is generating. Unlike the encoder's self-attention (source attending to source) and the decoder's self-attention (target attending to target), cross-attention implements <strong>attention from target to source</strong>.</p><p class=\"whitespace-normal break-words\">Let's examine a more complex translation to see why this matters. Consider translating the English sentence: \"Despite the challenging market conditions that persisted throughout the quarter, the company delivered strong results\" to French: \"Malgré les conditions de marché difficiles qui ont persisté tout au long du trimestre, l'entreprise a livré de solides résultats.\"</p><p class=\"whitespace-normal break-words\">Notice how the structure differs. English uses \"despite\" at the beginning with a dependent clause, then the main clause. French mirrors this structure, but word-for-word alignment is messy. \"Challenging market conditions\" becomes \"conditions de marché difficiles\"—the adjective moves after the noun, and \"market\" becomes a prepositional phrase. \"Throughout the quarter\" becomes \"tout au long du trimestre\"—completely different words expressing the same meaning. \"Delivered strong results\" becomes \"a livré de solides résultats\"—note \"strong\" moves before \"results\" in French.</p><p class=\"whitespace-normal break-words\">When the decoder generates \"difficiles,\" cross-attention must focus on \"challenging market conditions,\" understanding that this single French adjective captures \"challenging\" but needs to come after \"conditions de marché.\" When generating \"tout au long du trimestre,\" cross-attention focuses on \"throughout the quarter,\" but the alignment isn't one-to-one. The decoder has learned that this French phrase is the idiomatic way to express \"throughout,\" even though the literal words differ.</p><p class=\"whitespace-normal break-words\">Cross-attention weights are sometimes visualized as heatmaps showing which source positions the decoder attends to when generating each target position. These visualizations reveal beautiful patterns: diagonal alignments for similar structures, fan-out patterns where one source word generates multiple target words, and convergence patterns where multiple source words compress into one target word.</p><p class=\"whitespace-normal break-words\">The learning of these alignments is entirely supervised by translation examples—the model never receives explicit word alignment annotations. It discovers alignments as a by-product of learning to translate accurately. This is powerful because alignments can be probabilistic and context-dependent. The English word \"get\" might align to different French words depending on context: \"obtenir\" (obtain), \"devenir\" (become), \"comprendre\" (understand), or \"arriver\" (arrive). Cross-attention learns these context-sensitive mappings automatically.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Training Dynamics: Teacher Forcing and Its Consequences</h2><p class=\"whitespace-normal break-words\">Understanding how encoder-decoder models are trained reveals important insights about their behavior and limitations. During training, we use a technique called <strong>teacher forcing</strong>: at each decoding step, instead of feeding the model's own prediction as input, we feed it the ground truth target token from the training data.</p><p class=\"whitespace-normal break-words\">Why? Because training would be impossibly slow otherwise. Imagine training on a translation dataset. If we let the decoder use its own (initially random) predictions, it would produce garbage, and learning from garbage is difficult. Teacher forcing gives the decoder a stable, informative signal at every step: \"Here's what you should have generated; now try to generate the next correct token.\"</p><p class=\"whitespace-normal break-words\">But this creates a subtle problem called <strong>exposure bias</strong>. During training, the decoder always sees perfect prefixes. If the true target is \"The cat sat on the mat,\" the decoder at step 3 always sees \"The cat\" as prefix, never \"The dog\" or \"The car.\" But during inference—when you actually use the model—there's no teacher. If the model generates \"The dog\" at step 2, it must continue from there, even though it never trained on prefixes starting with \"dog\" in this context.</p><p class=\"whitespace-normal break-words\">This manifests in interesting ways. Encoder-decoder models sometimes exhibit error accumulation: one wrong word early in generation throws off the distribution, leading to more errors downstream. They can also be brittle to slight variations in input that push them into states they didn't see during training. Modern training techniques try to address this—scheduled sampling gradually reduces teacher forcing, and reinforcement learning techniques train the model to handle its own imperfect generations—but exposure bias remains a fundamental challenge.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Applications: Where Encoder-Decoder Excels</h2><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Machine Translation: The Canonical Domain</h3><p class=\"whitespace-normal break-words\">Machine translation is the original and still primary application of encoder-decoder models. The task has perfect structure for this architecture: a complete source sentence in one language needs to become a complete target sentence in another language. The encoder can see the entire source before generating anything, allowing it to resolve ambiguities, understand idioms, and capture discourse-level coherence.</p><p class=\"whitespace-normal break-words\">Consider translating idiomatic expressions. \"It's raining cats and dogs\" doesn't mean literal feline and canine precipitation. When translating to French (\"Il pleut des cordes\" - literally \"it's raining ropes\"), the encoder must recognize this as an idiom, understand its meaning as \"heavy rain,\" and the decoder must produce the idiomatic French equivalent, not a literal translation. The encoder's bidirectional processing allows it to recognize idiomatic patterns; the decoder's access to full context lets it generate appropriate target idioms.</p><p class=\"whitespace-normal break-words\">Or consider grammatical gender in translation. Translating \"The doctor arrived; she was tired\" to Spanish requires knowing \"doctor\" is feminine (\"La doctora llegó; estaba cansada\") from \"she.\" The encoder processes \"she\" after \"doctor,\" using self-attention to link them and determine gender. The decoder then correctly generates feminine forms throughout the Spanish translation. This forward reference resolution is natural for encoder-decoder but would be challenging for a strictly left-to-right model.</p><p class=\"whitespace-normal break-words\">Models like MarianMT, mBART (multilingual BART), and mT5 (multilingual T5) use encoder-decoder architecture for translation across dozens or hundreds of language pairs. They're trained on massive parallel corpora, learning not just word alignments but deep structural correspondences between languages. Some can even perform zero-shot translation—translating between language pairs never seen together during training by using English as a pivot language internally.</p><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Abstractive Summarization: Creating Novel Text</h3><p class=\"whitespace-normal break-words\">Summarization is fascinating because it requires true understanding and generation, not just copying. Extractive summarization (selecting sentences from the source) is simpler; abstractive summarization (writing new sentences that capture meaning) is genuinely creative.</p><p class=\"whitespace-normal break-words\">Consider summarizing a news article about a company merger. The article might have 800 words spread across 15 paragraphs discussing history, financial terms, regulatory approval, executive quotes, and market implications. An abstractive summary might be: \"Tech giant Acme Corp announced its acquisition of startup DataFlow for $2.3 billion, expanding its artificial intelligence capabilities in enterprise software.\"</p><p class=\"whitespace-normal break-words\">Notice what happened: information from multiple paragraphs (price from paragraph 3, AI focus from paragraph 7, enterprise software from paragraph 12) got synthesized into one coherent sentence. Quoted text got paraphrased. Proper nouns got preserved. Background details got omitted. This requires the encoder to build a structured understanding of key information and relationships, and the decoder to generate fluent, informative prose that wasn't in the original.</p><p class=\"whitespace-normal break-words\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</p><p class=\"whitespace-normal break-words\">BART and PEGASUS are specifically designed for this. BART uses a clever pretraining strategy: it corrupts text through various methods (deleting words, shuffling sentences, masking spans) and trains the encoder-decoder to reconstruct the original. This teaches robust understanding (encoder handles corrupted text) and faithful generation (decoder recreates clean text). PEGASUS uses \"gap sentence generation,\" training the model to generate sentences that were removed from documents—directly practicing the skill of creating coherent text that captures information from context.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Comparing Architectures: Encoder-Decoder vs. Decoder-Only</h2><p class=\"whitespace-normal break-words\">While encoder-decoder dominated 2017-2020, decoder-only models (GPT series, LLaMA, Mistral, Claude) now dominate large language model research. Why did this shift happen?</p><p class=\"whitespace-normal break-words\"><strong>Decoder-only models</strong> like GPT are architecturally simpler. They have one module, not two. Every token attends to previous tokens (causal attention), building representations and generating predictions simultaneously. Training is straightforward: predict the next token in a massive text corpus. This scales extremely well—modern decoder-only models reach hundreds of billions of parameters, trained on trillions of tokens.</p><p class=\"whitespace-normal break-words\"><strong>Architectural efficiency matters at scale.</strong> At inference, decoder-only generates with one forward pass per token. Encoder-decoder requires running the encoder once (not too expensive), then running the decoder with cross-attention for every generated token (more expensive due to additional attention computations). For long documents or high throughput requirements, this overhead compounds.</p><p class=\"whitespace-normal break-words\"><strong>Prompting is more flexible.</strong> Decoder-only models handle any task through in-context learning and prompting. Want translation? Provide examples: \"English: Hello French: Bonjour, English: Goodbye French: Au revoir, English: Thank you French:\" and the model completes \"Merci.\" Want summarization? \"Article: [long text] Summary:\" and it generates. The model learns from pure text prediction, but emergent capabilities allow task-following through context.</p><p class=\"whitespace-normal break-words\"><strong>However, encoder-decoder has specific advantages:</strong></p><p class=\"whitespace-normal break-words\"><strong>Bidirectional encoding.</strong> When the task truly requires understanding the complete input before generating, encoder-decoder wins. The encoder sees the entire source, using bidirectional self-attention. Decoder-only models process causally—when reading position 5, they can't see position 6. For translation, this matters: translating a sentence with a surprise ending or nested clauses benefits from seeing everything first.</p><p class=\"whitespace-normal break-words\"><strong>Explicit source-target separation.</strong> When input and output are truly distinct—different languages, different modalities, different levels of abstraction—encoder-decoder's architectural separation is intuitive. The encoder specializes in understanding source characteristics; the decoder specializes in generating target characteristics. Decoder-only must do both with one architecture.</p><p class=\"whitespace-normal break-words\"><strong>Cross-attention interpretability.</strong> Cross-attention weights show explicit alignments between source and target. For debugging translation models, understanding what went wrong in summarization, or ensuring factual grounding, inspecting cross-attention provides insight. Decoder-only models mix source processing and generation, making attribution harder.</p><p class=\"whitespace-normal break-words\"><strong>Parameter efficiency for specific tasks.</strong> For a specific task like English-French translation, an encoder-decoder model might achieve better performance with fewer parameters than a general-purpose decoder-only model prompted to translate. The architectural inductive bias helps.</p><p class=\"whitespace-normal break-words\">Consider a concrete example: translating technical documentation. A 600M parameter encoder-decoder model fine-tuned on technical translation might outperform a 7B parameter decoder-only model prompted to translate, because the encoder-decoder's architecture matches the task structure perfectly. But the decoder-only model can also summarize, answer questions, and write code—it trades task-specific optimization for generality.</p><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Modern Landscape: Where Things Stand</h2><p class=\"whitespace-normal break-words\">Today's NLP landscape shows interesting segmentation. <strong>General-purpose LLMs</strong> are decoder-only: GPT-4, Claude, Gemini, LLaMA. Their goal is flexible intelligence across all language tasks, making decoder-only's simplicity and prompting flexibility dominant.</p><p class=\"whitespace-normal break-words\"><strong>Specialized systems</strong> still use encoder-decoder: speech recognition (Whisper), neural machine translation in production systems (Google Translate uses encoder-decoder at its core), specialized summarization engines, and multimodal systems.</p><p class=\"whitespace-normal break-words\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n</p><p class=\"whitespace-normal break-words\"><strong>Research continues</strong> on hybrid approaches. Some models use encoder-decoder pretraining followed by decoder-only fine-tuning. Some use encoder-decoder for specific subtasks within larger decoder-only systems. \"Mixture of Experts\" models might route translation tasks to encoder-decoder components while handling other tasks decoder-only.</p>",
        "1": "<h1 class=\"text-2xl font-bold mt-1 text-text-100\"><br></h1>",
        "2": "<h1>Extracting Embeddings from Encoder &amp; Decoder Models - Compact Study Guide</h1>\n<h2>Core Concepts</h2>\n<p><strong>Embeddings</strong> are dense vector representations that capture the semantic meaning of tokens or sequences. These vectors typically have dimensions ranging from 768 to 4096 depending on the model size. The fundamental idea is that similar concepts end up close together in this high-dimensional space.</p>\n<p><strong>Encoder models</strong> like BERT and RoBERTa use bidirectional attention, meaning each token can see both previous and future tokens in the sequence. This makes them excellent for understanding context from all directions. <strong>Decoder models</strong> like GPT and LLaMA use unidirectional (causal) attention, processing tokens strictly left-to-right where each position only sees previous tokens. <strong>Encoder-decoder architectures</strong> like T5 and BART combine both: the encoder creates a bidirectional representation of the input, and the decoder generates output autoregressively.</p>\n<h2>Key Extraction Points</h2>\n<h3>Encoder Models (e.g., BERT)</h3>\n<p>When extracting embeddings from encoder models, you have several options depending on your use case. You can extract <strong>token embeddings</strong> which give you a representation for every individual token from any layer in the network. For a <strong>sequence-level embedding</strong> representing the entire input, you typically use either the CLS token (a special token BERT adds at the start) or perform mean pooling across all token embeddings.</p>\n<p>The choice of <strong>which layer</strong> to extract from matters significantly. The last layer provides general-purpose representations suitable for most tasks. Middle layers tend to capture more syntactic information about sentence structure. Early layers focus on lexical features and are closer to the raw token embeddings. For most applications, the last or second-to-last layer works best.</p>\n<h3>Decoder Models (e.g., GPT)</h3>\n<p>With decoder models, the <strong>last token embedding</strong> is crucial because it has seen the entire sequence due to causal masking - each token only attends to previous positions, so the final token's representation incorporates information from all preceding tokens. While you can access <strong>all token embeddings</strong>, remember that each one only contains context from tokens before it, not after. You can also access <strong>hidden states</strong> from intermediate layers for more granular control over what level of abstraction you're extracting.</p>\n<h2>Practical Code Patterns</h2>\n<h3>Encoder Extraction (HuggingFace Transformers)</h3>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"bert-base-uncased\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"bert-base-uncased\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\ntext <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Extract my embeddings\"</span>\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> output_hidden_states<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Multiple extraction options:</span>\nlast_hidden <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, seq_len, hidden_dim]</span>\npooled <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>pooler_output  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, hidden_dim] - CLS token processed</span>\ncls_embedding <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> last_hidden<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Raw CLS token</span>\nmean_pool <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> last_hidden<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>mean<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Mean pooling across tokens</span>\nall_layers <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>hidden_states  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Tuple of all layer outputs</span></code></pre><p></p><p></p>\n<p>The <code>last_hidden_state</code> gives you embeddings for every token in your sequence with shape batch size by sequence length by hidden dimension. The <code>pooler_output</code> is BERT's processed CLS token that's been passed through an additional layer. You can also manually extract the CLS token as the first position, or create a mean-pooled representation by averaging across the sequence dimension.</p>\n<h3>Decoder Extraction (GPT-style)</h3>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"gpt2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"gpt2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\ntext <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Extract my embeddings\"</span>\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> output_hidden_states<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \nlast_hidden <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, seq_len, hidden_dim]</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># For sequence embedding, use LAST token (has seen full context)</span>\nsequence_embedding <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> last_hidden<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, hidden_dim]</span></code></pre><p></p><p></p>\n<p>For decoder models, the critical difference is that you want the <strong>last token</strong> position for sequence-level embeddings, not the first. This is because of causal masking - only the final token has accumulated information from the entire input sequence.</p>\n<h3>Encoder-Decoder Extraction (T5)</h3>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> T5EncoderModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Encoder only</span>\nencoder <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> T5EncoderModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"t5-base\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"t5-base\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"translate: Hello\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    encoder_outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> encoder<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> encoder_outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Encoder embeddings</span></code></pre><p></p><p></p>\n<p>With encoder-decoder models, you can extract embeddings from either component separately. The encoder embeddings capture the input representation with full bidirectional context, while decoder embeddings would be extracted during generation and have causal attention patterns.</p>\n<h2>Critical Differences Between Architectures</h2>\n<p>The attention mechanism fundamentally differs between encoders and decoders. Encoders use bidirectional attention where every token can attend to every other token in the sequence, allowing for rich contextual understanding. Decoders use causal attention flowing strictly left-to-right, where each position only sees previous tokens - this prevents information leakage during generation.</p>\n<p>For sequence-level representations, encoders typically use the CLS token or mean pooling since all tokens have seen the full context. Decoders must use the last token because it's the only position that has accumulated information from all previous positions through the causal chain.</p>\n<p>Encoders excel at classification tasks, sentence similarity, and any application requiring bidirectional understanding. Decoders are designed for generation tasks and next-token prediction. When choosing between them, consider whether you need to understand existing text (encoder) or generate new text (decoder).</p>\n<h2>PyTorch Low-Level Extraction</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Access specific layer embeddings</span>\nlayer_num <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">6</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Middle layer example</span>\nlayer_embedding <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>hidden_states<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span>layer_num<span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get attention weights for interpretability</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> output_attentions<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nattention <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>attentions  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Tuple per layer</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Batch processing with padding awareness</span>\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Text 1\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Text 2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n    padding<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n    truncation<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n    return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span>\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nattention_mask <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'attention_mask'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Masked mean pooling (properly ignore padding tokens)</span>\nembeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> last_hidden <span class=\"token\" style=\"color: rgb(64, 120, 242);\">*</span> attention_mask<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\npooled <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">sum</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">/</span> attention_mask<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">sum</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> keepdim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>When working with batches, you'll have padding tokens that shouldn't influence your embeddings. The attention mask tells you which tokens are real versus padding. For mean pooling, you need to multiply embeddings by the attention mask and normalize by the actual sequence length rather than the padded length. This ensures padding tokens don't dilute your representations.</p>\n<h2>Common Patterns for LLM Work</h2>\n<p><strong>Sentence embeddings</strong> can be created using specialized models from the sentence-transformers library, or by applying mean pooling to standard model outputs. For <strong>retrieval systems</strong>, you encode queries and documents separately into the same embedding space, then compute cosine similarity to find matches. When <strong>fine-tuning</strong>, you typically extract embeddings from the layer just before your task-specific classification or regression head. For <strong>dimensionality reduction</strong> and visualization, apply techniques like PCA or UMAP after extracting your high-dimensional embeddings to project them into 2D or 3D space.</p>\n<h2>NVIDIA/Production Considerations</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Mixed precision for memory efficiency and speed</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cuda<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>amp<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>autocast<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Efficient batch processing</span>\nembeddings_list <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">for</span> batch <span class=\"token\" style=\"color: rgb(166, 38, 164);\">in</span> dataloader<span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        emb <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>batch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n        embeddings_list<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>append<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        \nembeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cat<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>embeddings_list<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>For production deployments and NVIDIA certification work, use mixed precision training with automatic mixed precision (AMP) to reduce memory footprint and increase throughput. When processing large datasets, batch your inputs and accumulate embeddings in a list before concatenating, which is more memory efficient than keeping everything on GPU. Always use <code>torch.no_grad()</code> when extracting embeddings for inference to prevent unnecessary gradient computation.</p>\n<h2>Quick Reference: What to Extract</h2>\n<p>For <strong>classification tasks</strong>, use the CLS token from encoder models or the last token from decoder models as your sequence representation. For <strong>similarity and search applications</strong>, mean-pooled embeddings work best, but make sure to mask out padding tokens when pooling. When doing <strong>analysis or interpretability work</strong>, extract hidden states from specific layers - earlier layers capture surface features while deeper layers encode more abstract semantics. For <strong>transfer learning</strong>, the second-to-last layer often generalizes better than the final layer. In <strong>contrastive learning</strong> setups, normalize your embeddings to unit length before computing cosine similarity to ensure the metric focuses on direction rather than magnitude.</p>",
        "3": "<h2>Core Concepts</h2>\n<p><strong>Sampling</strong> is the process of selecting the next token during text generation from the probability distribution the model outputs. At each step, an LLM produces logits (raw scores) for every token in its vocabulary, which are converted to probabilities via softmax. How you sample from this distribution dramatically affects output quality, diversity, coherence, and creativity.</p>\n<p>The fundamental tension in sampling is between <strong>exploitation</strong> (choosing high-probability tokens for coherent, safe outputs) and <strong>exploration</strong> (sampling lower-probability tokens for diverse, creative outputs). Different techniques offer different balances along this spectrum, and the right choice depends on your application - factual Q&amp;A needs consistency while creative writing benefits from diversity.</p>\n<p><strong>Temperature</strong> is a scaling parameter applied to logits before softmax that controls the \"sharpness\" of the probability distribution. <strong>Deterministic methods</strong> like greedy decoding always pick the most likely token, while <strong>stochastic methods</strong> introduce randomness. <strong>Constrained sampling</strong> techniques limit the sampling space to improve quality without sacrificing all diversity.</p>\n<h2>Greedy Decoding</h2>\n<p>Greedy decoding is the simplest approach - at each step, select the token with the highest probability. This is completely deterministic and will always produce the same output for the same input. While computationally efficient, greedy decoding often produces repetitive or generic text because it gets stuck in local optima. The model might repeat phrases or fall into loops because it never explores alternative paths.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoModelForCausalLM<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModelForCausalLM<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"gpt2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"gpt2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\nprompt <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"The future of AI is\"</span>\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>prompt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Greedy decoding</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">False</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Greedy when do_sample=False</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>decode<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> skip_special_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Greedy decoding works best for tasks requiring factual accuracy or when you want reproducible outputs, like translating structured data or extracting specific information. It's terrible for creative tasks or when you need multiple diverse outputs.</p>\n<h2>Beam Search</h2>\n<p>Beam search maintains multiple hypotheses (beams) simultaneously and explores the top-k most probable sequences at each step. Instead of committing to one token choice, it keeps track of several partial sequences and their cumulative probabilities. At each step, it expands all beams, scores all possible continuations, and keeps only the top-k complete sequences.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Beam search with multiple beams</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    num_beams<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">5</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Keep 5 hypotheses</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">False</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    early_stopping<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Stop when all beams hit EOS</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Beam search finds higher-quality sequences than greedy decoding by avoiding early commitment to suboptimal paths. However, it still tends toward generic, high-probability outputs and doesn't solve the repetition problem entirely. The computational cost scales linearly with beam width - using 5 beams is 5x slower than greedy decoding. You can return multiple diverse outputs by setting <code>num_return_sequences</code> up to the beam width.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Return multiple beam search results</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    num_beams<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">5</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    num_return_sequences<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">3</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Return top 3 beams</span>\n    early_stopping<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Beam search excels at tasks like machine translation, summarization, and code generation where you want high-quality, coherent outputs but don't need much creativity. It's less suitable for open-ended generation or when diversity matters.</p>\n<h2>Temperature Sampling</h2>\n<p>Temperature sampling modifies the probability distribution by dividing logits by a temperature parameter before applying softmax. A temperature of 1.0 uses the model's original distribution. Lower temperatures (0.1-0.9) make the distribution sharper, concentrating probability on high-likelihood tokens for more focused, conservative outputs. Higher temperatures (1.1-2.0) flatten the distribution, giving lower-probability tokens more chance and increasing randomness and creativity.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Low temperature - more focused</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Must enable sampling</span>\n    temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.7</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># More conservative</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># High temperature - more random</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1.5</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># More creative/chaotic</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>The mathematical formula is: <code>softmax(logits / temperature)</code>. As temperature approaches zero, this becomes equivalent to greedy decoding (argmax). As temperature increases toward infinity, all tokens become equally likely (uniform distribution). Temperature is your primary knob for controlling the creativity-coherence tradeoff.</p>\n<p>For factual tasks, use temperatures between 0.3-0.7. For creative writing, try 0.8-1.2. For maximum diversity or brainstorming, go up to 1.5-2.0, though outputs may become incoherent. Very high temperatures (&gt;2.0) usually produce nonsense.</p>\n<h2>Top-K Sampling</h2>\n<p>Top-k sampling restricts the sampling pool to only the k most probable tokens at each step, then redistributes probability mass uniformly among these top-k candidates. All other tokens are given zero probability. This prevents the model from sampling extremely unlikely tokens that could derail generation while maintaining diversity among reasonable choices.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Top-k sampling</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    top_k<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Only consider top 50 tokens</span>\n    temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1.0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Can combine with temperature</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>A typical top-k value is 50, meaning you only sample from the 50 most likely tokens. Smaller k (10-20) produces more focused outputs, while larger k (100-200) allows more diversity. The main limitation is that k is fixed - sometimes the top token has 90% probability and you should focus narrowly, while other times probability is spread across many tokens and you want broader sampling. This inflexibility led to the development of top-p sampling.</p>\n<p>Top-k works well for conversational AI and general text generation where you want to avoid nonsense but maintain variety. It's less ideal when the optimal sampling pool size varies significantly across generation steps.</p>\n<h2>Top-P (Nucleus) Sampling</h2>\n<p>Top-p sampling, also called nucleus sampling, dynamically selects the smallest set of tokens whose cumulative probability exceeds threshold p. Instead of a fixed number of tokens, you sample from a variable-sized pool that adapts to the model's confidence. When the model is very confident, only a few tokens might be needed to reach probability p. When uncertain, many tokens might be included.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Top-p (nucleus) sampling</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    top_p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.9</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Sample from top 90% probability mass</span>\n    temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1.0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Typical top-p values range from 0.9 to 0.95. A value of 0.9 means you sample from the smallest set of tokens that together account for 90% of the probability mass, discarding the low-probability tail. Lower p values (0.7-0.85) make sampling more conservative, while higher values (0.95-0.99) allow more diversity. Top-p adapts better than top-k to varying confidence levels and generally produces higher-quality outputs.</p>\n<p>Top-p is now the preferred method for most open-ended generation tasks. It's used by default in many LLM APIs and works well for chatbots, creative writing, and code generation.</p>\n<h2>Combining Techniques</h2>\n<p>The most powerful approach is combining multiple sampling techniques. You can use temperature to control overall randomness, top-p to prevent sampling from the unreasonable tail, and top-k as an additional safety constraint. These techniques apply sequentially: first temperature scales the distribution, then top-k filters to k candidates, then top-p further filters within those k, and finally you sample from what remains.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Combined sampling - industry best practice</span>\noutputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.8</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Moderate creativity</span>\n    top_k<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Limit to 50 candidates</span>\n    top_p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.9</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Further filter to 90% cumulative probability</span>\n    repetition_penalty<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1.2</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Penalize repeated tokens</span>\n    pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>eos_token_id\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>The <code>repetition_penalty</code> parameter (typically 1.0-1.5) reduces the probability of tokens that already appeared in the generated sequence, helping avoid repetitive text. Values above 1.0 penalize repetition, while 1.0 applies no penalty. Too high (&gt;1.5) can make outputs incoherent as the model tries too hard to avoid repetition.</p>\n<h2>Advanced Techniques</h2>\n<p><strong>Min-p sampling</strong> is a newer technique that filters out tokens with probability below p times the maximum probability. Unlike top-p which uses cumulative probability, min-p uses relative probability. This can maintain diversity while avoiding very unlikely tokens more effectively than top-k.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Min-p sampling (if supported by your library)</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Tokens with prob &lt; (max_prob * min_p) are filtered</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Not in standard transformers yet, but available in some inference engines</span></code></pre><p></p><p></p>\n<p><strong>Contrastive search</strong> balances model confidence and diversity by penalizing tokens that are too similar to previous context. It uses a degeneration penalty that measures similarity between candidate tokens and already-generated text, encouraging the model to avoid repetitive patterns while staying coherent.</p>\n<p><strong>Mirostat sampling</strong> dynamically adjusts the sampling parameters during generation to maintain a target level of perplexity, providing consistent output quality. It's useful for long-form generation where you want steady creativity throughout.</p>\n<h2>Low-Level Implementation</h2>\n<p>Understanding the mechanics helps you implement custom techniques or debug issues:</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>functional <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> F\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Manual temperature + top-p sampling implementation</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">sample_with_temperature_and_topp</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>logits<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1.0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> top_p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.9</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Apply temperature</span>\n    logits <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> logits <span class=\"token\" style=\"color: rgb(64, 120, 242);\">/</span> temperature\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Convert to probabilities</span>\n    probs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> F<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>softmax<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>logits<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Sort probabilities in descending order</span>\n    sorted_probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> sorted_indices <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>sort<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> descending<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Calculate cumulative probabilities</span>\n    cumulative_probs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cumsum<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sorted_probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Find cutoff index where cumulative prob exceeds top_p</span>\n    cutoff_index <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>where<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>cumulative_probs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">&gt;</span> top_p<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">if</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">len</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>cutoff_index<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">&gt;</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        cutoff_index <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> cutoff_index<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n        <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Zero out probabilities beyond cutoff</span>\n        sorted_probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span>cutoff_index <span class=\"token\" style=\"color: rgb(64, 120, 242);\">+</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span>\n        sorted_probs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> sorted_probs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">/</span> sorted_probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">sum</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Renormalize</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Sample from the filtered distribution</span>\n    next_token_idx <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>multinomial<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sorted_probs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> num_samples<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    next_token <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> sorted_indices<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span>next_token_idx<span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n    \n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">return</span> next_token\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Use during generation loop</span>\ninput_ids <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>encode<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>prompt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">for</span> _ <span class=\"token\" style=\"color: rgb(166, 38, 164);\">in</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">range</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>max_new_tokens<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>input_ids<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        next_token_logits <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>logits<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n        next_token <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> sample_with_temperature_and_topp<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n            next_token_logits<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n            temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.8</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n            top_p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.9</span>\n        <span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        input_ids <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cat<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span>input_ids<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> next_token<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>This low-level implementation shows exactly what happens: logits are scaled by temperature, converted to probabilities via softmax, sorted to find the nucleus, filtered to the top-p mass, renormalized, and finally sampled. Understanding this helps you create custom sampling strategies or combine techniques in novel ways.</p>\n<h2>Practical Guidelines for Choosing Techniques</h2>\n<p>For <strong>factual question answering or information extraction</strong>, use greedy decoding or very low temperature (0.1-0.3) to maximize accuracy and reproducibility. For <strong>machine translation or summarization</strong>, beam search with 3-5 beams gives high-quality results by exploring multiple hypotheses. For <strong>conversational AI</strong>, use temperature 0.7-0.9 with top-p 0.9 to balance coherence and naturalness.</p>\n<p>For <strong>creative writing or brainstorming</strong>, use temperature 0.9-1.2 with top-p 0.9-0.95 to encourage diverse, interesting outputs. For <strong>code generation</strong>, use temperature 0.2-0.4 or beam search since correctness matters more than creativity. For <strong>long-form content generation</strong>, add repetition penalty 1.1-1.3 to prevent loops and maintain variety.</p>\n<p>When you need <strong>multiple diverse outputs</strong>, either use beam search with <code>num_return_sequences</code> or run sampling multiple times with different random seeds. When <strong>latency is critical</strong>, avoid beam search and stick to sampling with small batch sizes. When <strong>quality is paramount</strong>, accept the computational cost of beam search or use larger values of top-k and top-p with careful temperature tuning.</p>\n<h2>Production and NVIDIA Deployment Considerations</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Efficient batched generation with padding</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoModelForCausalLM<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModelForCausalLM<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"gpt2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    torch_dtype<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>float16<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Use FP16 for speed</span>\n    device_map<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"auto\"</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Automatic GPU placement</span>\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Batch inputs with padding</span>\nprompts <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Prompt 1\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Prompt 2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Prompt 3\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\ninputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    prompts<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n    return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"pt\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> \n    padding<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    truncation<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span>\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>to<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"cuda\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Generate with mixed precision</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cuda<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>amp<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>autocast<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>generate<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n        <span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        max_new_tokens<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">100</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        do_sample<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        temperature<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.8</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        top_p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.9</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        top_k<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">50</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        pad_token_id<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>pad_token_id<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n        use_cache<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Enable KV cache for efficiency</span>\n    <span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Enable the <strong>KV cache</strong> with <code>use_cache=True</code> to avoid recomputing attention for previously generated tokens - this dramatically speeds up autoregressive generation. Use <strong>mixed precision</strong> (FP16 or BF16) to reduce memory usage and increase throughput on modern GPUs. For serving at scale, consider <strong>batched inference</strong> where you process multiple requests simultaneously, though this complicates dynamic batching since different requests finish at different times.</p>\n<p>For <strong>streaming generation</strong>, many frameworks support token-by-token yielding so users see outputs incrementally rather than waiting for complete sequences. This improves perceived latency for long generations. Consider <strong>speculative decoding</strong> where a smaller draft model generates candidates that a larger model verifies - this can speed up sampling by 2-3x for large models.</p>\n<p>When deploying sampling techniques, <strong>log your parameters</strong> (temperature, top-p, top-k) alongside generations for reproducibility and debugging. Different models may behave differently with the same parameters, so always validate on your specific model. Monitor for <strong>degeneration patterns</strong> like repetition loops or incoherence and adjust penalties accordingly. Finally, consider making sampling parameters <strong>user-configurable</strong> in applications so users can tune creativity versus consistency to their preferences.</p>",
        "5": "<h2>Core Concept</h2>\n<p><strong>Embeddings</strong> are dense vector representations that map discrete objects (words, tokens, sentences, images, users, products) into continuous vector space where semantic or functional similarity corresponds to geometric proximity. Instead of representing a word like \"king\" as a one-hot vector of dimension 50,000 (vocabulary size) with a single 1 and 49,999 zeros, an embedding represents it as a dense vector of perhaps 300 floating-point numbers like [0.2, -0.5, 0.8, ...]. This transformation from discrete symbols to continuous vectors is what enables neural networks to process and learn from language and other structured data.</p>\n<p>The fundamental insight is that <strong>meaning can be encoded geometrically</strong>. Words with similar meanings end up close together in embedding space, and relationships between words manifest as consistent vector offsets. The classic example is: vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\"). This isn't just a neat trick - it reveals that embeddings capture relational structure, not just individual word meanings. Embeddings are learned from data rather than hand-crafted, allowing the model to discover patterns and relationships automatically.</p>\n<p><strong>Why embeddings matter</strong> for LLMs: They're the bridge between human language (discrete symbols) and neural network computations (continuous operations like matrix multiplication). Every piece of text input to an LLM first gets converted to embeddings, every intermediate layer produces embeddings, and the final output logits are converted back from embeddings to token probabilities. Understanding embeddings is understanding how LLMs represent and manipulate information.</p>\n<h2>Types of Embeddings</h2>\n<p><strong>Token embeddings</strong> (or word embeddings) map individual tokens to vectors. Classic approaches include Word2Vec, GloVe, and FastText, which learn static representations where each word always gets the same vector regardless of context. In modern transformers, token embeddings are just the first layer - they're context-independent lookup tables that get contextualized through attention mechanisms in later layers.</p>\n<p><strong>Positional embeddings</strong> encode the position of a token in the sequence. Since transformers process all tokens in parallel (unlike RNNs which process sequentially), they need explicit position information. Learned positional embeddings assign a unique vector to each position index up to the maximum sequence length. Sinusoidal positional embeddings use fixed trigonometric functions to generate position vectors, which can generalize to longer sequences than seen during training. Rotary Position Embeddings (RoPE) encode position by rotating query and key vectors in attention, which has become popular in recent LLMs like LLaMA.</p>\n<p><strong>Contextual embeddings</strong> (or contextualized representations) are produced by models like BERT and GPT where the same word gets different embeddings based on surrounding context. \"Bank\" in \"river bank\" versus \"bank account\" receives different vector representations after the model processes context through attention layers. These are what you extract from intermediate transformer layers - they're no longer static but dynamically computed for each specific usage.</p>\n<p><strong>Sentence embeddings</strong> represent entire sentences or paragraphs as single vectors. Naive approaches like averaging token embeddings lose information. Sophisticated methods like sentence-BERT use siamese networks and contrastive training to learn meaningful sentence-level representations optimized for similarity tasks. These are crucial for semantic search, clustering, and retrieval applications.</p>\n<p><strong>Multimodal embeddings</strong> map different data types (text, images, audio) into a shared space. CLIP embeds both images and text such that a photo of a cat is near the text \"a photo of a cat\" in vector space. This enables cross-modal retrieval where you can search images with text queries or vice versa.</p>\n<h2>How Embeddings Are Learned</h2>\n<p>Embeddings are learned through <strong>backpropagation</strong> as part of end-to-end neural network training. In a basic setup, the embedding layer is a learnable lookup table - a matrix of shape [vocab_size, embedding_dim] where each row is one token's embedding. When you feed token ID 5234 to the model, it simply retrieves row 5234 from this matrix. During training, gradients flow back to these embedding vectors and update them to minimize loss on the training objective.</p>\n<p><strong>Word2Vec</strong> introduced two influential training approaches. The <strong>Skip-gram</strong> objective trains the model to predict context words given a center word - if you see \"cat\", predict nearby words like \"furry\", \"pet\", \"meow\". The <strong>CBOW</strong> (Continuous Bag of Words) objective does the reverse: predict the center word from surrounding context. Both objectives force similar words to have similar embeddings because they appear in similar contexts. The famous word arithmetic (king - man + woman = queen) emerges naturally from this distributional training.</p>\n<p><strong>GloVe</strong> (Global Vectors) learns embeddings by factorizing a word co-occurrence matrix. It constructs a matrix counting how often each word pair appears together in a corpus, then learns embeddings such that their dot product approximates these co-occurrence statistics. This combines global corpus statistics with local context, often producing high-quality static embeddings.</p>\n<p><strong>Modern transformer embeddings</strong> are learned jointly with the entire model during pre-training tasks like masked language modeling (BERT predicts hidden words) or next-token prediction (GPT predicts the next word). The embeddings evolve to support whatever representations the model needs for its task. Unlike Word2Vec which explicitly optimizes embedding quality, transformer embeddings are just one component optimized to minimize overall task loss.</p>\n<h2>Properties of Good Embeddings</h2>\n<p><strong>Semantic similarity</strong> means similar concepts have similar vectors. You can measure this with cosine similarity - vectors pointing in similar directions have high cosine similarity (near 1.0), while orthogonal vectors have similarity near 0. Good embeddings cluster related words: all fruits group together, all verbs of motion group together, etc. This property enables semantic search where you find documents relevant to a query by finding nearest neighbors in embedding space.</p>\n<p><strong>Dimensionality</strong> is a crucial hyperparameter. Lower dimensions (50-100) are memory-efficient but may not capture complex relationships. Higher dimensions (300-1024) capture more nuance but risk overfitting and require more data. Modern LLMs use embeddings from 768 (BERT-base) to 4096 or larger (GPT-3). There's no universal optimal size - it depends on vocabulary size, task complexity, and available training data.</p>\n<p><strong>Compositionality</strong> refers to how well embeddings combine to form meaningful representations. Vector arithmetic like \"king - man + woman = queen\" demonstrates compositional structure. More generally, good embeddings allow you to combine word vectors (through addition, concatenation, or learned combinations) to represent phrases and sentences meaningfully.</p>\n<p><strong>Geometric structure</strong> in embedding spaces often exhibits interpretable directions. In Word2Vec, there might be a \"gender\" dimension where vectors offset consistently between male/female word pairs, or a \"tense\" dimension for verb conjugations. In sentence embeddings trained with contrastive learning, you want clear separation between dissimilar sentences and tight clustering of paraphrases.</p>\n<h2>Basic Implementation</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>nn <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> nn\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Simple embedding layer - just a lookup table</span>\nvocab_size <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">10000</span>\nembedding_dim <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">300</span>\n\nembedding_layer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>Embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> embedding_dim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Input: token IDs</span>\ntoken_ids <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>tensor<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">15</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">234</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">5678</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">42</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># A sequence of 4 tokens</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Output: embeddings for each token</span>\nembeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> embedding_layer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>token_ids<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Shape: [4, 300]</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Shape: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">embeddings</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">shape</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"First token embedding: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">embeddings</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token string-interpolation interpolation format-spec\">10]</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># First 10 dims</span></code></pre><p></p><p></p>\n<p>The embedding layer is initialized randomly (usually from a normal distribution), then learned during training. Each token ID acts as an index into the embedding matrix. This operation is fully differentiable - gradients computed on the embeddings propagate back to update the embedding matrix.</p>\n<h2>Computing Similarity</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>functional <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> F\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get embeddings for two words</span>\nword1_id <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>tensor<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">100</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nword2_id <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>tensor<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">200</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\nemb1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> embedding_layer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>word1_id<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Shape: [1, 300]</span>\nemb2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> embedding_layer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>word2_id<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Shape: [1, 300]</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Cosine similarity</span>\ncosine_sim <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> F<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cosine_similarity<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> emb2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Cosine similarity: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">cosine_sim</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Euclidean distance</span>\neuclidean_dist <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>dist<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> emb2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> p<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">2</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Euclidean distance: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">euclidean_dist</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Dot product (unnormalized similarity)</span>\ndot_product <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>dot<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> emb2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Dot product: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">dot_product</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p><strong>Cosine similarity</strong> measures the angle between vectors, ranging from -1 (opposite) to 1 (identical), with 0 meaning orthogonal. It's invariant to vector magnitude, so \"cat\" and \"cats\" might be similar even if their magnitudes differ. This is usually preferred for semantic similarity.</p>\n<p><strong>Euclidean distance</strong> measures straight-line distance in embedding space. Smaller distances mean more similar. Unlike cosine similarity, it's affected by magnitude - longer vectors will have larger distances even if pointing in similar directions. Some embedding methods explicitly optimize for Euclidean distance.</p>\n<p><strong>Dot product</strong> combines both magnitude and direction. Higher dot products mean more similar and/or larger magnitude. This is what attention mechanisms use - the dot product of query and key embeddings determines attention weights.</p>\n<h2>Sentence and Document Embeddings</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoModel\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Load model for contextualized embeddings</span>\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'sentence-transformers/all-MiniLM-L6-v2'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'sentence-transformers/all-MiniLM-L6-v2'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">get_sentence_embedding</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Tokenize</span>\n    inputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'pt'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> padding<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> truncation<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get model outputs</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Mean pooling - average token embeddings with attention mask</span>\n    attention_mask <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'attention_mask'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n    token_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state\n    \n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Mask padding tokens and average</span>\n    input_mask_expanded <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> attention_mask<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">-</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>expand<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>token_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">float</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    sum_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">sum</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>token_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">*</span> input_mask_expanded<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    sum_mask <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>clamp<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>input_mask_expanded<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">sum</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">min</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">1e-9</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">return</span> sum_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">/</span> sum_mask\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Use it</span>\nsentence1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"The cat sat on the mat\"</span>\nsentence2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"A feline rested on the rug\"</span>\nsentence3 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"I love eating pizza\"</span>\n\nemb1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> get_sentence_embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sentence1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nemb2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> get_sentence_embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sentence2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nemb3 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> get_sentence_embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sentence3<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Compare similarities</span>\nsim_12 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> F<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cosine_similarity<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> emb2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nsim_13 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> F<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cosine_similarity<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> emb3<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Similarity between similar sentences: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">sim_12</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token string-interpolation interpolation format-spec\">.4f</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Similarity between different sentences: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">sim_13</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token string-interpolation interpolation format-spec\">.4f</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>The mean pooling operation is critical - simply averaging all token embeddings treats padding tokens equally with real tokens, diluting the representation. Proper mean pooling uses the attention mask to only average over actual tokens. Many sentence embedding models use specialized pooling strategies: CLS token pooling (use the first token's embedding), max pooling (take the maximum value across sequence for each dimension), or learned weighted combinations.</p>\n<h2>Visualizing Embeddings</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> sklearn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>manifold <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> TSNE\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> sklearn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>decomposition <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> PCA\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> matplotlib<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>pyplot <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> plt\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> numpy <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> np\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Assume we have embeddings for many words</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Shape: [num_words, embedding_dim]</span>\nword_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> embedding_layer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>weight<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>detach<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cpu<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>numpy<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Sample some words for visualization</span>\nnum_words <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">500</span>\nsampled_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> word_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>num_words<span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Reduce to 2D using t-SNE</span>\ntsne <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> TSNE<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>n_components<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">2</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> random_state<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">42</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> perplexity<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">30</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nembeddings_2d <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tsne<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>fit_transform<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sampled_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Plot</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>figure<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>figsize<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">12</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">8</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>scatter<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>embeddings_2d<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> embeddings_2d<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">1</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> alpha<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.5</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>title<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Word Embeddings Visualization\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>xlabel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Dimension 1\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>ylabel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Dimension 2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nplt<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>show<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Alternatively, use PCA (faster, linear)</span>\npca <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> PCA<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>n_components<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">2</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nembeddings_2d_pca <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> pca<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>fit_transform<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>sampled_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p><strong>t-SNE</strong> (t-distributed Stochastic Neighbor Embedding) is great for visualization because it preserves local structure - similar words cluster tightly together. However, it's non-linear and computationally expensive for large datasets, doesn't preserve global structure (distances between clusters aren't meaningful), and can't be applied to new points without recomputing everything.</p>\n<p><strong>PCA</strong> (Principal Component Analysis) is faster and preserves global variance structure but may not capture complex non-linear relationships as well as t-SNE. It's deterministic (same input always gives same output) and can project new embeddings into the same space. For quick exploration, PCA is often sufficient. For publication-quality visualizations showing clusters, t-SNE or UMAP are better.</p>\n<p><strong>UMAP</strong> (Uniform Manifold Approximation and Projection) is newer and often superior to t-SNE - it's faster, preserves both local and global structure better, and can be applied to new data. It's becoming the standard for embedding visualization.</p>\n<h2>Training Custom Embeddings from Scratch</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>nn <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> nn\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>optim <span class=\"token\" style=\"color: rgb(166, 38, 164);\">as</span> optim\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Simple Skip-gram-style embedding training</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">class</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">SkipGramModel</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>Module<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">__init__</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> embedding_dim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        <span class=\"token\" style=\"color: rgb(80, 161, 79);\">super</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>__init__<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>Embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> embedding_dim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>output <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>Linear<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>embedding_dim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        \n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">forward</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> center_word<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get center word embedding</span>\n        embed <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>center_word<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, embedding_dim]</span>\n        \n        <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Predict context words</span>\n        logits <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> self<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>output<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>embed<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># [batch, vocab_size]</span>\n        \n        <span class=\"token\" style=\"color: rgb(166, 38, 164);\">return</span> logits\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Initialize model</span>\nvocab_size <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">10000</span>\nembedding_dim <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">100</span>\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> SkipGramModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> embedding_dim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\noptimizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> optim<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>Adam<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>parameters<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> lr<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0.001</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ncriterion <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> nn<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>CrossEntropyLoss<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Training loop (simplified)</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Assume we have pairs of (center_word, context_word)</span>\ncenter_words <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>randint<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">32</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Batch of 32</span>\ncontext_words <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>randint<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> vocab_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">32</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Their contexts</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Forward pass</span>\nlogits <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>center_words<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Compute loss</span>\nloss <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> criterion<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>logits<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> context_words<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Backward pass</span>\noptimizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>zero_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nloss<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>backward<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\noptimizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>step<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># After training, extract learned embeddings</span>\nlearned_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>weight<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>detach<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Learned embedding matrix shape: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">learned_embeddings</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">shape</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>This simplified Skip-gram implementation shows the basic idea: given a center word, predict context words. The embedding layer learns representations such that words with similar contexts (and thus similar meanings) end up with similar embeddings. Real implementations use negative sampling to make training efficient - instead of computing softmax over the entire vocabulary (expensive), you only compare the true context word against a few randomly sampled \"negative\" words.</p>\n<h2>Contextual vs Static Embeddings</h2>\n<p><strong>Static embeddings</strong> like Word2Vec and GloVe assign each word a single fixed vector regardless of context. \"Bank\" always gets the same embedding whether it means financial institution or river bank. These are simple, efficient, and work well for many applications, but they can't handle polysemy (multiple meanings) or capture subtle contextual variations.</p>\n<p><strong>Contextual embeddings</strong> from transformers compute different vectors for the same word based on surrounding context. In BERT, \"bank\" in \"I went to the bank\" versus \"river bank\" gets different embeddings after processing through attention layers. These capture much richer semantics but require running the full model for each new text - you can't pre-compute and store them like static embeddings.</p>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> BertTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> BertModel\n\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> BertTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'bert-base-uncased'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> BertModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'bert-base-uncased'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Same word in different contexts</span>\ntext1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"I deposited money at the bank\"</span>\ntext2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"We sat on the river bank\"</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get contextual embeddings for \"bank\" in each sentence</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">get_word_embedding</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> word_index<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    inputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'pt'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Get embedding for specific token position</span>\n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">return</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> word_index<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Find position of \"bank\" in each tokenized sentence</span>\ninputs1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'pt'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\ninputs2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'pt'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Usually you'd identify the token position programmatically</span>\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Here we'll assume \"bank\" is at position 6 in both (after subwords)</span>\nbank_emb1 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> get_word_embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">6</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\nbank_emb2 <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> get_word_embedding<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>text2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">6</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># These will be different!</span>\nsimilarity <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> F<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cosine_similarity<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>bank_emb1<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> bank_emb2<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>unsqueeze<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">print</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">f\"Similarity of 'bank' in different contexts: </span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">{</span><span class=\"token string-interpolation interpolation\">similarity</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token string-interpolation interpolation\">item</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token string-interpolation interpolation format-spec\">.4f</span><span class=\"token string-interpolation interpolation\" style=\"color: rgb(56, 58, 66);\">}</span><span class=\"token string-interpolation\" style=\"color: rgb(80, 161, 79);\">\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>The contextual embeddings for \"bank\" will differ significantly between the financial and geographical contexts, whereas static embeddings would give identical vectors. This makes contextual embeddings much more powerful for understanding language, which is why they've become dominant in modern NLP.</p>\n<h2>Embedding Dimensionality and Information</h2>\n<p>The <strong>intrinsic dimensionality</strong> of embeddings is often much lower than the nominal embedding dimension. A 768-dimensional BERT embedding might only use 50-100 dimensions for most of its information content. You can often compress embeddings significantly with minimal quality loss using dimensionality reduction techniques like PCA or learned projection layers. This is useful for reducing storage and computation costs in production systems.</p>\n<p><strong>Layer choice matters</strong> when extracting embeddings from transformers. Earlier layers capture lexical and syntactic information - word identity, parts of speech, simple patterns. Middle layers encode more complex syntactic structures and local semantic relationships. Later layers specialize for the pre-training task and may be less generalizable. For most downstream tasks, the second-to-last layer often works best, though this varies by model and application.</p>\n<h2>Applications of Embeddings</h2>\n<p><strong>Semantic search</strong> uses embeddings to find relevant documents. Encode all documents into embeddings offline, store them in a vector database, then encode user queries at runtime and find nearest neighbors using cosine similarity or approximate nearest neighbor algorithms like FAISS. This enables fuzzy matching where \"how to cook pasta\" retrieves documents about \"preparing spaghetti\" even without exact keyword overlap.</p>\n<p><strong>Clustering and classification</strong> benefit enormously from embeddings. Instead of treating text as discrete tokens, you can cluster sentence embeddings with k-means to find thematic groups in large corpora. For classification, feed embeddings into simple models like logistic regression - the hard work of representation is done by pre-trained embeddings.</p>\n<p><strong>Recommendation systems</strong> use embeddings to represent users and items in shared space. User-item interactions (clicks, purchases) train embeddings such that users are near items they like. Finding recommendations becomes nearest neighbor search in embedding space.</p>\n<p><strong>Anomaly detection</strong> identifies outliers in embedding space. Normal examples cluster tightly, while anomalies have embeddings far from the cluster center. This works for detecting spam, fraud, unusual behavior, or data quality issues.</p>\n<p><strong>Transfer learning</strong> leverages embeddings trained on massive datasets for specialized tasks with limited data. Use pre-trained BERT embeddings as features for domain-specific classification, fine-tune sentence embeddings on your paraphrase data, or adapt multilingual embeddings for low-resource languages.</p>\n<h2>Production Considerations</h2>\n<p></p><p></p><p></p><div class=\"relative\"><div class=\"flex items-center justify-center transition-all opacity-100 scale-100\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 transition-all opacity-100 scale-100\" aria-hidden=\"true\"><path d=\"M10 1.5C11.1097 1.5 12.0758 2.10424 12.5947 3H14.5C15.3284 3 16 3.67157 16 4.5V16.5C16 17.3284 15.3284 18 14.5 18H5.5C4.67157 18 4 17.3284 4 16.5V4.5C4 3.67157 4.67157 3 5.5 3H7.40527C7.92423 2.10424 8.89028 1.5 10 1.5ZM5.5 4C5.22386 4 5 4.22386 5 4.5V16.5C5 16.7761 5.22386 17 5.5 17H14.5C14.7761 17 15 16.7761 15 16.5V4.5C15 4.22386 14.7761 4 14.5 4H12.958C12.9853 4.16263 13 4.32961 13 4.5V5.5C13 5.77614 12.7761 6 12.5 6H7.5C7.22386 6 7 5.77614 7 5.5V4.5C7 4.32961 7.0147 4.16263 7.04199 4H5.5ZM12.54 13.3037C12.6486 13.05 12.9425 12.9317 13.1963 13.04C13.45 13.1486 13.5683 13.4425 13.46 13.6963C13.1651 14.3853 12.589 15 11.7998 15C11.3132 14.9999 10.908 14.7663 10.5996 14.4258C10.2913 14.7661 9.88667 14.9999 9.40039 15C8.91365 15 8.50769 14.7665 8.19922 14.4258C7.89083 14.7661 7.48636 15 7 15C6.72386 15 6.5 14.7761 6.5 14.5C6.5 14.2239 6.72386 14 7 14C7.21245 14 7.51918 13.8199 7.74023 13.3037L7.77441 13.2373C7.86451 13.0913 8.02513 13 8.2002 13C8.40022 13.0001 8.58145 13.1198 8.66016 13.3037C8.88121 13.8198 9.18796 14 9.40039 14C9.61284 13.9998 9.9197 13.8197 10.1406 13.3037L10.1748 13.2373C10.2649 13.0915 10.4248 13.0001 10.5996 13C10.7997 13 10.9808 13.1198 11.0596 13.3037C11.2806 13.8198 11.5874 13.9999 11.7998 14C12.0122 14 12.319 13.8198 12.54 13.3037ZM12.54 9.30371C12.6486 9.05001 12.9425 8.93174 13.1963 9.04004C13.45 9.14863 13.5683 9.44253 13.46 9.69629C13.1651 10.3853 12.589 11 11.7998 11C11.3132 10.9999 10.908 10.7663 10.5996 10.4258C10.2913 10.7661 9.88667 10.9999 9.40039 11C8.91365 11 8.50769 10.7665 8.19922 10.4258C7.89083 10.7661 7.48636 11 7 11C6.72386 11 6.5 10.7761 6.5 10.5C6.5 10.2239 6.72386 10 7 10C7.21245 10 7.51918 9.8199 7.74023 9.30371L7.77441 9.2373C7.86451 9.09126 8.02513 9 8.2002 9C8.40022 9.00008 8.58145 9.11981 8.66016 9.30371C8.88121 9.8198 9.18796 10 9.40039 10C9.61284 9.99978 9.9197 9.81969 10.1406 9.30371L10.1748 9.2373C10.2649 9.09147 10.4248 9.00014 10.5996 9C10.7997 9 10.9808 9.11975 11.0596 9.30371C11.2806 9.8198 11.5874 9.99989 11.7998 10C12.0122 10 12.319 9.81985 12.54 9.30371ZM10 2.5C8.89543 2.5 8 3.39543 8 4.5V5H12V4.5C12 3.39543 11.1046 2.5 10 2.5Z\"></path></svg></div><div class=\"flex items-center justify-center absolute top-0 left-0 transition-all opacity-0 scale-50\" style=\"width: 20px; height: 20px;\"><svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\" class=\"shrink-0 absolute top-0 left-0 transition-all opacity-0 scale-50\" aria-hidden=\"true\"><path d=\"M15.1883 5.10908C15.3699 4.96398 15.6346 4.96153 15.8202 5.11592C16.0056 5.27067 16.0504 5.53125 15.9403 5.73605L15.8836 5.82003L8.38354 14.8202C8.29361 14.9279 8.16242 14.9925 8.02221 14.9989C7.88203 15.0051 7.74545 14.9526 7.64622 14.8534L4.14617 11.3533L4.08172 11.2752C3.95384 11.0811 3.97542 10.817 4.14617 10.6463C4.31693 10.4755 4.58105 10.4539 4.77509 10.5818L4.85321 10.6463L7.96556 13.7586L15.1161 5.1794L15.1883 5.10908Z\"></path></svg></div></div><p></p><p></p><p>python</p><p></p><pre><code><span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Efficient batch embedding for production</span>\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">from</span> transformers <span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> AutoTokenizer\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">import</span> torch\n\nmodel <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoModel<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n    <span class=\"token\" style=\"color: rgb(80, 161, 79);\">'sentence-transformers/all-MiniLM-L6-v2'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n    torch_dtype<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span>torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>float16  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Use FP16 for efficiency</span>\n<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>to<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'cuda'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\ntokenizer <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> AutoTokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>from_pretrained<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'sentence-transformers/all-MiniLM-L6-v2'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(166, 38, 164);\">def</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">batch_encode</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>texts<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> batch_size<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">32</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n    <span class=\"token triple-quoted-string\" style=\"color: rgb(80, 161, 79);\">\"\"\"Efficiently encode large lists of texts\"\"\"</span>\n    all_embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n    \n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">for</span> i <span class=\"token\" style=\"color: rgb(166, 38, 164);\">in</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">range</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">len</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>texts<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> batch_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n        batch <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> texts<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span>i<span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>i<span class=\"token\" style=\"color: rgb(64, 120, 242);\">+</span>batch_size<span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>\n        \n        inputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> tokenizer<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>\n            batch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n            padding<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n            truncation<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">True</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n            max_length<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">512</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span>\n            return_tensors<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'pt'</span>\n        <span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>to<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">'cuda'</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n        \n        <span class=\"token\" style=\"color: rgb(166, 38, 164);\">with</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>no_grad<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cuda<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>amp<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>autocast<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span>\n            outputs <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> model<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(64, 120, 242);\">**</span>inputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n            embeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> outputs<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>last_hidden_state<span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">:</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># CLS token</span>\n            \n        all_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>append<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cpu<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n    \n    <span class=\"token\" style=\"color: rgb(166, 38, 164);\">return</span> torch<span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span>cat<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>all_embeddings<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> dim<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">0</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span>\n\n<span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Use it</span>\ntexts <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">[</span><span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Document 1\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(80, 161, 79);\">\"Document 2\"</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> <span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">.</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">]</span> <span class=\"token\" style=\"color: rgb(64, 120, 242);\">*</span> <span class=\"token\" style=\"color: rgb(183, 107, 1);\">1000</span>  <span class=\"token\" style=\"color: rgb(160, 161, 167); font-style: italic;\"># Large dataset</span>\nembeddings <span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span> batch_encode<span class=\"token\" style=\"color: rgb(56, 58, 66);\">(</span>texts<span class=\"token\" style=\"color: rgb(56, 58, 66);\">,</span> batch_size<span class=\"token\" style=\"color: rgb(64, 120, 242);\">=</span><span class=\"token\" style=\"color: rgb(183, 107, 1);\">64</span><span class=\"token\" style=\"color: rgb(56, 58, 66);\">)</span></code></pre><p></p><p></p>\n<p>Use <strong>batching</strong> to maximize GPU utilization and throughput. Process multiple texts simultaneously rather than one at a time. Choose batch size based on available GPU memory - larger batches are more efficient but use more memory. Use <strong>mixed precision</strong> (FP16) to reduce memory usage and increase speed with minimal quality loss on modern GPUs.</p>\n<p><strong>Vector databases</strong> like Pinecone, Weaviate, Milvus, or FAISS are essential for production semantic search at scale. They implement approximate nearest neighbor algorithms that can search billions of vectors in milliseconds. Exact nearest neighbor search is O(n) in the number of vectors - fine for thousands but impractical for millions. ANN algorithms like HNSW or IVF trade slight accuracy for massive speed improvements.</p>\n<p><strong>Caching embeddings</strong> is crucial - computing embeddings is expensive, so cache them whenever possible. For static documents, compute embeddings offline and store them. Only recompute when content changes. For user queries, consider caching frequent queries to avoid redundant computation.</p>\n<p><strong>Normalization</strong> is important for many applications. L2-normalizing embeddings (dividing by their magnitude) ensures all vectors have unit length, making cosine similarity equivalent to dot product and simplifying certain algorithms. Many embedding models are trained to produce normalized embeddings.</p>\n<p>Understanding embeddings deeply - from mathematical foundations to implementation details to production deployment - is fundamental to working with LLMs and modern AI systems. They're the representational substrate on which everything else is built.</p>"
      },
      "subtopicSummaries": {
        "0": "<h1 class=\"text-2xl font-bold mt-1 text-text-100\">Summary: Encoder-Decoder Architectures for LLMs</h1>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Core Concept</h2>\n<p class=\"whitespace-normal break-words\">Encoder-decoder architectures formalize the \"understand, then generate\" approach to sequence transformation tasks. Like a UN translator who listens to a complete thought before speaking, these models separate comprehension (encoder) from generation (decoder), making them ideal for tasks where input and output have different lengths, structures, or modalities.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Two-Stage Architecture</h2>\n<p class=\"whitespace-normal break-words\"><strong>The Encoder</strong> reads the entire input and builds rich, contextualized representations through bidirectional self-attention. It resolves ambiguities by letting every word attend to every other word—so \"bank\" in \"river bank\" versus \"bank deposits\" gets correctly understood from surrounding context. Multiple layers build increasingly abstract representations from syntax to semantics.</p>\n<p class=\"whitespace-normal break-words\"><strong>The Decoder</strong> generates output sequentially, using three attention mechanisms:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Self-attention</strong> on its own partial output (what have I said so far?)</li>\n<li class=\"whitespace-normal break-words\"><strong>Cross-attention</strong> to the encoder (which source words matter now?)</li>\n<li class=\"whitespace-normal break-words\"><strong>Causal masking</strong> to prevent looking ahead during generation</li>\n</ul>\n<p class=\"whitespace-normal break-words\"><strong>Cross-attention</strong> is the critical bridge—it learns soft, context-dependent alignments between source and target. When translating \"challenging market conditions\" to French \"conditions de marché difficiles,\" cross-attention dynamically focuses on relevant source positions even when word order changes.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Historical Evolution</h2>\n<p class=\"whitespace-normal break-words\"><strong>2014</strong>: Early seq2seq models compressed entire inputs into single fixed vectors—creating information bottlenecks. Bahdanau's attention mechanism solved this by letting decoders look at all encoder positions.</p>\n<p class=\"whitespace-normal break-words\"><strong>2017</strong>: Transformers (\"Attention Is All You Need\") replaced recurrence entirely with self-attention, enabling parallel processing and true bidirectional understanding.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Training Challenges</h2>\n<p class=\"whitespace-normal break-words\"><strong>Teacher forcing</strong> speeds training by feeding ground truth tokens rather than model predictions, but creates <strong>exposure bias</strong>—the model never trains on its own mistakes, leading to error accumulation at inference when it must handle imperfect generations.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Primary Applications</h2>\n<p class=\"whitespace-normal break-words\"><strong>Machine Translation</strong>: The canonical use case. Perfect for handling idioms, grammatical gender, and structural differences between languages. Models like mBART and mT5 handle dozens of language pairs, even zero-shot translation.</p>\n<p class=\"whitespace-normal break-words\"><strong>Abstractive Summarization</strong>: BART and PEGASUS excel at synthesizing information across long documents into coherent novel text. BART's denoising pretraining (corrupting then reconstructing text) teaches robust understanding and faithful generation.</p>\n<p class=\"whitespace-normal break-words\"><strong>T5 Philosophy</strong>: Frames every NLP task as text-to-text transformation—classification, QA, translation, summarization—all handled by one unified encoder-decoder.</p>\n<p class=\"whitespace-normal break-words\"><strong>Multimodal Tasks</strong>: Whisper (speech-to-text) and image captioning use encoder-decoder to bridge different modalities.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Encoder-Decoder vs. Decoder-Only</h2>\n<p class=\"whitespace-normal break-words\"><strong>Why decoder-only dominates modern LLMs</strong> (GPT, Claude, LLaMA):</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Simpler architecture (one module vs. two)</li>\n<li class=\"whitespace-normal break-words\">More efficient inference (one forward pass per token)</li>\n<li class=\"whitespace-normal break-words\">Flexible prompting handles any task without fine-tuning</li>\n<li class=\"whitespace-normal break-words\">Scales better to massive parameters</li>\n</ul>\n<p class=\"whitespace-normal break-words\"><strong>Where encoder-decoder still wins</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Tasks requiring full bidirectional understanding before generation</li>\n<li class=\"whitespace-normal break-words\">Clear source-target separation (different languages/modalities)</li>\n<li class=\"whitespace-normal break-words\">Interpretability through cross-attention alignments</li>\n<li class=\"whitespace-normal break-words\">Parameter efficiency for specialized tasks</li>\n</ul>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Modern Landscape</h2>\n<p class=\"whitespace-normal break-words\"><strong>General-purpose LLMs</strong>: Decoder-only for flexibility and scale</p>\n<p class=\"whitespace-normal break-words\"><strong>Specialized systems</strong>: Encoder-decoder for translation, speech recognition, and multimodal tasks where architectural inductive bias matches task structure</p>\n<p class=\"whitespace-normal break-words\"><strong>Key insight</strong>: A 600M encoder-decoder fine-tuned for translation can outperform a 7B decoder-only model prompted to translate—but the decoder-only model handles hundreds of tasks, trading specialized optimization for generality.</p>",
        "1": "<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Critical Exam Concepts</h2>\n<p class=\"whitespace-normal break-words\"><strong>Why √d_k scaling?</strong> Prevents large dot products th<span style=\"background-color: rgb(255, 245, 157);\">at <mark style=\"\">push softmax into vanishing gradient regions.</mark></span></p>\n<p class=\"whitespace-normal break-words\"><strong>Why multi-head?</strong> Different heads learn different patterns (syntax, semantics, position), capturing richer representations.</p>\n<p class=\"whitespace-normal break-words\"><strong>Self vs. cross-attention?</strong> Self: Q/K/V from same sequence. Cross: Q from one sequence, K/V from another (encoder-decoder bridge).</p>\n<p class=\"whitespace-normal break-words\"><strong>Why positional encoding?</strong> Transformers have no inherent position awareness; it injects sequence order.</p>\n<p class=\"whitespace-normal break-words\"><strong>Transformers vs. RNNs?</strong> Transformers: parallelizable (O(1) sequential ops), direct connections (O(1) path), faster training. RNNs: sequential bottleneck (O(n) ops), degradation over distance.</p>\n<p class=\"whitespace-normal break-words\"><strong>Causal masking?</strong> Prevents decoder from seeing future tokens during training, ensuring model learns sequential generation matching inference.</p>\n<p class=\"whitespace-normal break-words\"><strong>Residual connections?</strong> Add input to sublayer output (x + F(x)), enabling gradient flow in deep networks.</p>\n<p class=\"whitespace-normal break-words\"><strong>Encoder vs. decoder-only?</strong> Encoder: bidirectional for understanding. Decoder-only: causal for generation. Decoder-only dominates because it's simpler, scales better, and handles any task via prompting.</p>\n<hr class=\"border-border-300 my-2\">\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Architecture Selection Guide</h2>\n<p class=\"whitespace-normal break-words\"><strong>Use Encoder-Only when</strong>: Classification, sentiment analysis, named entity recognition, extractive QA - tasks needing understanding without generation.</p>\n<p class=\"whitespace-normal break-words\"><strong>Use Decoder-Only when</strong>: Text generation, chat, code generation, general-purpose LLMs - benefits from simplicity and prompting flexibility.</p>\n<p class=\"whitespace-normal break-words\"><strong>Use Encoder-Decoder when</strong>: Translation, summarization, speech recognition - tasks with clear input-output separation where bidirectional encoding helps.</p>\n<hr class=\"border-border-300 my-2\">\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Quick Formula Reference</h2>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Attention</strong>: softmax(QK^T / √d_k) × V</li>\n<li class=\"whitespace-normal break-words\"><strong>Multi-head</strong>: Concat(head₁...head_h) × W^O</li>\n<li class=\"whitespace-normal break-words\"><strong>Positional encoding</strong>: PE(pos,2i) = sin(pos/10000^(2i/d_model))</li>\n<li class=\"whitespace-normal break-words\"><strong>Feed-forward</strong>: max(0, xW₁+b₁)W₂+b₂</li>\n<li class=\"whitespace-normal break-words\"><strong>Residual</strong>: LayerNorm(x + Sublayer(x))</li>\n</ul>\n<hr class=\"border-border-300 my-2\">\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Bottom Line</h2>\n<p class=\"whitespace-normal break-words\">Transformers revolutionized AI by replacing sequential processing with parallel attention mechanisms. Self-attention lets every word directly look at every other word, creating rich contextual representations. Multi-head attention captures diverse patterns simultaneously. The architecture comes in three variants optimized for different tasks, with decoder-only dominating modern LLMs due to simplicity and scale. Understanding attention (Q/K/V, scaling, softmax), the three variants (encoder/decoder/both), positional encoding, and computational tradeoffs prepares you for certification questions and real-world applications.</p>",
        "2": "",
        "5": ""
      },
      "readingCompletedAt": {
        "0": 1762649484956,
        "1": 1762649744852
      },
      "readingNotes": {
        "0": "<h1 class=\"text-2xl font-bold mt-1 text-text-100\">Comprehensive Summary: LLM Foundations - Architecture, Attention, and Training</h1>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Model Architectures: The Three Pillars</h2>\n<p class=\"whitespace-normal break-words\"><strong>Encoder-Only (BERT)</strong>: Bidirectional architecture that excels at understanding language through complete context awareness. Best for classification, sentiment analysis, and tasks requiring deep comprehension without generation. The encoder sees the entire input simultaneously, making it powerful for discriminative tasks.</p>\n<p class=\"whitespace-normal break-words\"><strong>Decoder-Only (GPT)</strong>: Unidirectional architecture with context flowing forward only. Despite this limitation, GPT-style models achieve remarkable natural language understanding through generative pre-training. They handle textual entailment, question answering, and excel at generation tasks. Modern LLMs (GPT-4, Claude, LLaMA) universally adopt this architecture for its simplicity, scalability, and prompting flexibility.</p>\n<p class=\"whitespace-normal break-words\"><strong>Encoder-Decoder (T5, Sequence-to-Sequence)</strong>: Bidirectional encoder paired with unidirectional decoder. The \"text-to-text\" philosophy converts every language problem into input-text → output-text format. Ideal for translation, summarization, and question answering where clear source-target separation exists.</p>\n<p class=\"whitespace-normal break-words\"><strong>Mixture of Experts (MoE)</strong>: Architectural overlay applicable to any base architecture. Converts dense models to sparse by using multiple expert models with sparse gating functions that route inputs to top-K experts. Dramatically scales model capacity with minimal computational overhead, achieving efficiency during inference while generalizing well across tasks.</p>\n<p class=\"whitespace-normal break-words\"><strong>Multimodal Models</strong>: Combine separate encoders for different modalities (CNNs for images, transformers for text). Features from multiple modalities are fused or weighted via attention mechanisms. Joint representations capture cross-modal interactions for tasks like image captioning, text-to-image generation, and visual question answering.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Tokenization: From Text to Numbers</h2>\n<p class=\"whitespace-normal break-words\">Tokenization fragments text into basic building blocks called tokens, creating vocabulary mappings from tokens to numeric IDs suitable for deep learning.</p>\n<p class=\"whitespace-normal break-words\"><strong>The Process</strong>:</p>\n<ol class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-decimal space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Normalization</strong>: Lowercase conversion, punctuation handling, stemming, lemmatization, accent removal</li>\n<li class=\"whitespace-normal break-words\"><strong>Segmentation</strong>: Recognizing word/sentence boundaries at word, subword, or character granularity</li>\n</ol>\n<p class=\"whitespace-normal break-words\"><strong>Why Subword Tokenization Dominates</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Word-level tokenizers create massive vocabularies and struggle with out-of-vocabulary words</li>\n<li class=\"whitespace-normal break-words\">Character-level tokenizers produce overly long sequences with less meaningful individual tokens</li>\n<li class=\"whitespace-normal break-words\"><strong>Subword tokenizers</strong> split rare words into meaningful subunits based on common character n-grams, reducing vocabulary size while handling unseen words gracefully</li>\n</ul>\n<p class=\"whitespace-normal break-words\"><strong>Popular Algorithms</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>BPE (Byte Pair Encoding)</strong>: Starts with characters, iteratively merges frequent adjacent pairs, achieves compression and faster decoding</li>\n<li class=\"whitespace-normal break-words\"><strong>WordPiece</strong>: Similar to BPE but merges based on maximizing training data likelihood (probabilistic approach)</li>\n<li class=\"whitespace-normal break-words\"><strong>Unigram</strong>: Starts with large vocabulary, removes tokens based on loss function until reaching desired size</li>\n<li class=\"whitespace-normal break-words\"><strong>SentencePiece</strong>: Learns subword units from raw text using Unigram or BPE, operates on raw text without pre-tokenization</li>\n</ul>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Attention Mechanisms: The Heart of Transformers</h2>\n<p class=\"whitespace-normal break-words\"><strong>Why Attention Emerged</strong>: Traditional RNN encoder-decoder models couldn't scale with input sequence length. Attention enabled decoders to selectively weight the most relevant input tokens rather than compressing everything into fixed vectors.</p>\n<p class=\"whitespace-normal break-words\"><strong>Self-Attention (Scaled Dot-Product Attention)</strong>:\nCreates context-aware representations by allowing each token to attend to all others in the same sequence. Each input token projects into Query (Q), Key (K), and Value (V) matrices. The mechanism computes attention scores via scaled dot-product of Q and K, then weights the V matrix accordingly. This produces context-enriched representations where \"bank\" near \"river\" differs from \"bank\" near \"deposits.\"</p>\n<p class=\"whitespace-normal break-words\"><strong>Multi-Head Attention</strong>:\nUses multiple attention heads in parallel, functioning like CNN kernels attending to different aspects of the sequence. Different heads specialize in various linguistic phenomena—syntax, semantics, short-range vs. long-range dependencies, coreference resolution. This emergent specialization happens naturally during training.</p>\n<p class=\"whitespace-normal break-words\"><strong>Cross-Attention</strong>:\nInstead of single input sequence (self-attention), cross-attention involves two sequences—typically encoder output and decoder state. This creates the bridge in encoder-decoder models, allowing the decoder to dynamically focus on relevant source positions while generating each target token.</p>\n<p class=\"whitespace-normal break-words\"><strong>FlashAttention</strong>:\nOptimizes the quadratic memory bottleneck of attention by using tiling to load Q, K, V blocks from GPU HBM (slow) to SRAM (fast) for computation, then writing back. Avoids storing large attention matrices during forward pass by recomputing during backpropagation in SRAM. Achieves 2-4x speedup for longer sequences. FlashAttention-2 doubles this with sequence parallelism and better work partitioning.</p>\n<p class=\"whitespace-normal break-words\"><strong>Multi-Query Attention (MQA)</strong>:\nMultiple query heads attend to the <strong>same</strong> single head of key and value projections. Dramatically reduces KV cache size and memory bandwidth during autoregressive decoding, enabling faster inference with minor quality degradation compared to multi-head attention.</p>\n<p class=\"whitespace-normal break-words\"><strong>Grouped-Query Attention (GQA)</strong>:\nInterpolates between multi-head and multi-query attention by sharing single K and V heads across <strong>groups</strong> of query heads (not all heads). Overcomes MQA's quality degradation while retaining inference speedup. Existing multi-head models can adopt GQA through up-training with just 5% of original training compute—no full retraining required.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Positional Encoding: Teaching Models About Order</h2>\n<p class=\"whitespace-normal break-words\">Word order matters crucially in language. Positional encoding assigns position information to each token since transformers process sequences in parallel without inherent order awareness.</p>\n<p class=\"whitespace-normal break-words\"><strong>Sinusoidal Positional Encoding (Original Transformer)</strong>:\nCombines absolute position encoding with word embeddings using sinusoidal functions. <strong>Limitation</strong>: Cannot extrapolate to sequences longer than those seen during training, restricting real-world applications.</p>\n<p class=\"whitespace-normal break-words\"><strong>Relative Position Encoding</strong>:\nCombines content representations (Q and K vectors) with trainable positional representations based on relative distance between query and key (clipped beyond certain distance). Enables length flexibility.</p>\n<p class=\"whitespace-normal break-words\"><strong>RoPE (Rotary Position Embeddings)</strong>:\nSynthesizes absolute and relative position embeddings. Encodes absolute position using rotation matrices while incorporating relative position dependency in self-attention multiplicatively. Provides sequence length flexibility with decaying inter-token dependency as distance increases, enabling extrapolation to longer sequences at inference.</p>\n<p class=\"whitespace-normal break-words\"><strong>ALiBi (Attention with Linear Biases)</strong>:\nDoesn't add positional embeddings to word embeddings. Instead, biases query-key attention scores with penalty proportional to distance between them. The penalty increases linearly with distance. No additional parameters, negligible memory increase, and strong extrapolation to sequences far longer than training. Avoids \"early token curse\" and efficiently exploits longer context histories.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Training at Scale: Parallelism Strategies</h2>\n<p class=\"whitespace-normal break-words\">Training billion-parameter models on trillions of tokens requires sophisticated memory and compute optimization.</p>\n<p class=\"whitespace-normal break-words\"><strong>Tensor Parallelism (Intra-Layer)</strong>:\nSplits individual operations (like matrix multiplication) across GPUs. Each GPU handles part of the computation within a single layer. Requires additional communication to synchronize results but enables massive layers that wouldn't fit on single GPUs.</p>\n<p class=\"whitespace-normal break-words\"><strong>Pipeline Parallelism (Inter-Layer)</strong>:\nSplits model layers across GPUs—each device computes its assigned layers and passes activations to the next stage. <strong>Challenge</strong>: \"Bubble time\" where some devices wait idle while others compute, wasting resources.</p>\n<p class=\"whitespace-normal break-words\"><strong>Sequence Parallelism</strong>:\nExpands tensor parallelism by recognizing that some transformer components (not previously parallelized) are independent along the sequence dimension. Splitting these along sequence dimension distributes both compute and activation memory across devices. Smaller distributed activations mean more can be saved for backward pass.</p>\n<p class=\"whitespace-normal break-words\"><strong>Selective Activation Recomputation</strong>:\nComplements sequence parallelism. Instead of checkpointing entire transformer layers (memory-intensive), checkpoints only parts with high memory footprint but low recomputation cost. Different activations require different recomputation overhead—this technique optimizes the tradeoff.</p>\n<p class=\"whitespace-normal break-words\"><strong>Data Parallelism</strong>:\nSplits dataset into shards, each device gets full model copy and trains on its shard. After backpropagation, gradients are all-reduced to synchronize model parameters across devices. Parallelizes along batch dimension.</p>\n<p class=\"whitespace-normal break-words\"><strong>Fully Sharded Data Parallelism (FSDP)</strong>:\nShards both model parameters and training data uniformly across workers. Computation for each micro-batch is local to each GPU. Configurable sharding strategies match physical network topology. Overlaps communication with computation through operation reordering and parameter prefetching. Optimizes memory by limiting inflight unsharded parameters. Achieves near-linear scalability for significantly larger models.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Quantization Aware Training (QAT)</h2>\n<p class=\"whitespace-normal break-words\"><strong>Quantization</strong>: Running model computations in reduced precision (INT8, INT4) instead of full precision (FP32/FP16) for faster inference, lower memory, and reduced costs.</p>\n<p class=\"whitespace-normal break-words\"><strong>QAT Approach</strong>:\nIncorporates quantization effects during training itself. Forward pass quantizes weights and activations to low-precision representations, mimicking deployment conditions. Backward pass computes gradients using full-precision values. Model learns parameters robust to quantization errors, resulting in trained models that maintain accuracy when quantized post-training. Superior to post-training quantization which can cause significant accuracy degradation.</p>\n<h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Key Architectural Tradeoffs&nbsp;</h2>\n<p class=\"whitespace-normal break-words\"><strong>When to use Encoder-Only</strong>: Tasks requiring bidirectional understanding without generation—classification, named entity recognition, sentiment analysis.</p>\n<p class=\"whitespace-normal break-words\"><strong>When to use Decoder-Only</strong>: General-purpose LLMs, generation tasks, and any scenario requiring flexible prompting. Dominant for modern large-scale models due to simplicity and scalability.</p>\n<p class=\"whitespace-normal break-words\"><strong>When to use Encoder-Decoder</strong>: Clear input-output transformation with different structures/modalities—translation, abstractive summarization, speech recognition, image captioning.</p>\n<p class=\"whitespace-normal break-words\"><strong>MQA vs. GQA</strong>: MQA for maximum inference speed with acceptable quality loss; GQA for near-multi-head quality with most of MQA's speed benefits.</p>\n<p class=\"whitespace-normal break-words\"><strong>RoPE vs. ALiBi</strong>: RoPE for combining absolute and relative positioning with good extrapolation; ALiBi for superior extrapolation to very long sequences without positional embeddings.</p>\n<p class=\"whitespace-normal break-words\"><strong>Parallelism Strategy</strong>: Combine tensor (for layers too large for single GPU), pipeline (for very deep models), sequence (for long sequences), and data parallelism (for training throughput). Use FSDP for massive models requiring parameter sharding.</p>",
        "1": "<div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h1 class=\"text-2xl font-bold mt-1 text-text-100\">\"Attention Is All You Need\" - The Paper That Changed Everything</h1></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Big Idea (In Plain English)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Before this 2017 paper, everyone building language models used <strong>recurrent neural networks (RNNs)</strong> - models that read text one word at a time, like you reading left-to-right. The problem? They're slow to train because you can't parallelize them (you have to wait for word 1 before processing word 2), and they forget information from early in long sentences.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Vaswani and team at Google asked</strong>: What if we threw out RNNs entirely and built everything from attention mechanisms alone?</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The answer was the <strong>Transformer</strong> - and it revolutionized AI.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Core Problem with Old Models (RNNs/LSTMs)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Sequential bottleneck</strong>: RNNs process text sequentially - word 1, then word 2, then word 3. You can't parallelize this. It's like reading a book one word at a time with your finger, never looking ahead.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Information loss</strong>: By the time an RNN finishes reading a 50-word sentence, it has compressed everything into a single hidden state vector. Early words get \"forgotten\" or overwritten.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Distance matters too much</strong>: For an RNN to connect word 1 to word 50, the signal has to pass through 49 intermediate steps. Information degrades over that distance.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Even with attention mechanisms added to RNNs (which was already happening), you still had the sequential processing problem.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Revolutionary Solution: Pure Attention</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The Transformer's radical move: <strong>remove recurrence entirely</strong>. Build everything from attention mechanisms that let every word directly look at every other word simultaneously. No sequential processing. No information bottleneck.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This is like going from reading word-by-word with your finger to seeing the entire page at once and letting your eyes jump to any word that matters for understanding context.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Architecture Breakdown</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Encoder (6 Stacked Layers)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each encoder layer has two components:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>1. Multi-Head Self-Attention</strong>: Every word looks at every other word in the input simultaneously. When processing \"bank\" in \"river bank,\" it can immediately see \"river\" and understand context. With 8 attention heads running in parallel, different heads learn different patterns - syntax, semantics, long-range dependencies.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>2. Feed-Forward Network</strong>: After attention enriches each word's representation, a simple neural network processes each position independently to add non-linear transformations.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Both components use <strong>residual connections</strong> (add input to output) and <strong>layer normalization</strong> (stabilize training).</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Decoder (6 Stacked Layers)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each decoder layer has three components:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>1. Masked Self-Attention</strong>: The decoder looks at its own partial output, but with a critical restriction - position 5 can only see positions 1-4, not future positions. This prevents \"cheating\" during training and ensures sequential generation during inference.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>2. Cross-Attention</strong>: This is where the magic happens. The decoder queries the encoder's output to figure out which source words matter for the current generation step. When translating \"bank\" to \"banque,\" cross-attention focuses the decoder on \"bank\" in the English source.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>3. Feed-Forward Network</strong>: Same as encoder - position-wise processing.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">The Attention Mechanism (Scaled Dot-Product)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The mathematical formula is simple but powerful:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention(Q, K, V) = softmax(QK^T / √d_k) × V</strong></p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Breaking this down:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Q (Query)</strong>: \"What am I looking for?\"</li>\n<li class=\"whitespace-normal break-words\"><strong>K (Key)</strong>: \"What information do I have?\"</li>\n<li class=\"whitespace-normal break-words\"><strong>V (Value)</strong>: \"What's the actual information?\"</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">The model computes similarity scores between queries and keys (QK^T), scales them by √d_k to prevent gradients from vanishing, applies softmax to get weights (which sum to 1), then takes a weighted sum of values.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Why scaling by √d_k matters</strong>: Without it, dot products get very large for high-dimensional vectors, pushing softmax into regions with tiny gradients where learning stops.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Multi-Head Attention (8 Heads)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Instead of one attention mechanism, they use 8 parallel ones with different learned parameters. It's like having 8 different students read the same sentence, each focusing on different aspects:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Head 1: syntactic relationships</li>\n<li class=\"whitespace-normal break-words\">Head 2: semantic similarity</li>\n<li class=\"whitespace-normal break-words\">Head 3: coreference (pronouns)</li>\n<li class=\"whitespace-normal break-words\">Head 4: positional patterns</li>\n<li class=\"whitespace-normal break-words\">And so on...</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Each head uses d_k = d_v = 64 dimensions (512 total / 8 heads). Total compute cost stays similar to single-head attention with full 512 dimensions.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Positional Encoding (Critical Innovation)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Problem</strong>: Attention has no inherent sense of word order. \"Dog bites man\" vs. \"Man bites dog\" would look identical to pure attention.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Solution</strong>: Add positional information to word embeddings using sine and cosine functions:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</li>\n<li class=\"whitespace-normal break-words\">PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Why sinusoidal functions? They create smooth patterns where position k+offset can be represented as a linear function of position k. This theoretically allows the model to extrapolate to longer sequences than seen during training.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">They also tested learned positional embeddings and got nearly identical results, but kept sinusoids for potential extrapolation benefits.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Why This Works Better Than RNNs</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">1. Parallelization</h3>\n<p class=\"whitespace-normal break-words\"><strong>RNN</strong>: Must process 50 words sequentially = 50 sequential operations\n<strong>Transformer</strong>: Processes all 50 words simultaneously = O(1) sequential operations</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This makes training <strong>dramatically</strong> faster. What took days now takes hours.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">2. Constant Path Length</h3>\n<p class=\"whitespace-normal break-words\"><strong>RNN</strong>: Connecting word 1 to word 50 requires signal to pass through 49 intermediate hidden states = O(n) path length\n<strong>Transformer</strong>: Every word directly attends to every other word = O(1) path length</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Shorter paths mean easier learning of long-range dependencies.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">3. Computational Complexity</h3>\n<p class=\"whitespace-normal break-words\">For typical sentence lengths (n &lt; d, where n = sequence length, d = dimension):</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\"><strong>Self-attention</strong>: O(n² × d) - quadratic in sequence length but parallelizable</li>\n<li class=\"whitespace-normal break-words\"><strong>RNN</strong>: O(n × d²) - linear in sequence but sequential</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Self-attention is faster when sentences are shorter than the model's hidden dimension (which is almost always true - sentences are typically 20-50 words, dimensions are 512-1024).</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">4. Better Long-Range Modeling</h3>\n<p class=\"whitespace-normal break-words\">RNNs struggle to remember information from 30+ steps ago even with LSTMs. Transformers directly connect all positions, so remembering is trivial - it's just an attention weight.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Training Recipe</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Dataset</strong>: WMT 2014 English-German (4.5M sentence pairs) and English-French (36M sentence pairs)</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Tokenization</strong>: Byte-pair encoding with 37K token vocabulary (shared between source and target for German, 32K for French)</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Hardware</strong>: 8 NVIDIA P100 GPUs</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Base model: 0.4 seconds per step, trained 100K steps (12 hours total)</li>\n<li class=\"whitespace-normal break-words\">Big model: 1.0 seconds per step, trained 300K steps (3.5 days total)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Hyperparameters</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">N = 6 layers (both encoder and decoder)</li>\n<li class=\"whitespace-normal break-words\">d_model = 512 (base) or 1024 (big)</li>\n<li class=\"whitespace-normal break-words\">d_ff = 2048 (base) or 4096 (big) - feed-forward inner dimension</li>\n<li class=\"whitespace-normal break-words\">h = 8 (base) or 16 (big) attention heads</li>\n<li class=\"whitespace-normal break-words\">Dropout = 0.1 (base) or 0.3 (big)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Optimizer</strong>: Adam with custom learning rate schedule:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Warm up: Linear increase for first 4,000 steps</li>\n<li class=\"whitespace-normal break-words\">Then decay: Proportional to inverse square root of step number</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This warmup is crucial - starting with high learning rates destabilizes training for Transformers.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Regularization</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Dropout on attention weights, feed-forward outputs, and embeddings</li>\n<li class=\"whitespace-normal break-words\">Label smoothing (ε = 0.1) - makes model less confident, improves generalization</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Results (Mind-Blowing for 2017)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">Machine Translation</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>English-to-German</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Previous best: 26.36 BLEU (ConvS2S ensemble)</li>\n<li class=\"whitespace-normal break-words\">Transformer big: <strong>28.4 BLEU</strong> (+2.0 improvement!)</li>\n<li class=\"whitespace-normal break-words\">Training cost: <strong>1/10th</strong> of previous state-of-the-art</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>English-to-French</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Previous best: 41.29 BLEU (ConvS2S ensemble)</li>\n<li class=\"whitespace-normal break-words\">Transformer big: <strong>41.8 BLEU</strong></li>\n<li class=\"whitespace-normal break-words\">Training cost: <strong>1/4th</strong> of previous state-of-the-art</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">Even the base Transformer model (smaller, faster) beat all previous single models and most ensembles.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">English Constituency Parsing (Generalization Test)</h3></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">To prove this wasn't just good for translation, they tested on a completely different task - parsing sentence structure.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>WSJ only (40K sentences)</strong>: 91.3 F1 score - competitive with specialized models</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Semi-supervised (17M sentences)</strong>: 92.7 F1 score - beat almost everything except one specialized generative model</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This demonstrated that Transformers <strong>generalize</strong> across different NLP tasks, not just translation.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Model Variations (Ablation Studies)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">They tested different configurations to understand what matters:</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention heads</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">1 head: 0.9 BLEU worse than 8 heads</li>\n<li class=\"whitespace-normal break-words\">16-32 heads: Quality drops (too much parallelism fragments learning)</li>\n<li class=\"whitespace-normal break-words\"><strong>Sweet spot: 8 heads</strong></li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Model size</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Bigger models consistently better (as expected)</li>\n<li class=\"whitespace-normal break-words\">Big model (213M params) outperformed base (65M params)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Attention key dimension</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Reducing d_k hurts performance</li>\n<li class=\"whitespace-normal break-words\">Suggests determining compatibility isn't trivial - dot product works but needs sufficient dimensionality</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Dropout</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Critical for avoiding overfitting</li>\n<li class=\"whitespace-normal break-words\">Without dropout, performance degraded significantly</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Positional encoding</strong>:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Sinusoidal vs. learned embeddings performed nearly identically</li>\n<li class=\"whitespace-normal break-words\">Kept sinusoidal for theoretical extrapolation properties</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">Why This Paper Changed Everything</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">1. <strong>Killed RNNs for NLP</strong></h3>\n<p class=\"whitespace-normal break-words\">Within 2 years, virtually all state-of-the-art NLP models used Transformers. RNNs became legacy technology.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">2. <strong>Enabled Massive Scaling</strong></h3>\n<p class=\"whitespace-normal break-words\">The parallelization breakthrough meant you could train much larger models on much more data. This led directly to:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">BERT (2018) - encoder-only Transformer</li>\n<li class=\"whitespace-normal break-words\">GPT-2/3 (2019/2020) - decoder-only Transformer</li>\n<li class=\"whitespace-normal break-words\">T5 (2019) - encoder-decoder Transformer</li>\n<li class=\"whitespace-normal break-words\">Modern LLMs with billions/trillions of parameters</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">3. <strong>Attention Became Universal</strong></h3>\n<p class=\"whitespace-normal break-words\">Self-attention is now used everywhere - not just NLP, but computer vision (Vision Transformers), speech (Whisper), multimodal models (CLIP, DALL-E), reinforcement learning, protein folding (AlphaFold)...</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h3 class=\"text-lg font-bold text-text-100 mt-1 -mb-1.5\">4. <strong>Established Core Principles</strong></h3>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Attention &gt; Recurrence for sequence modeling</li>\n<li class=\"whitespace-normal break-words\">Parallelization matters more than architectural complexity</li>\n<li class=\"whitespace-normal break-words\">Positional encoding can replace sequential processing</li>\n<li class=\"whitespace-normal break-words\">Multi-head attention captures diverse patterns</li>\n<li class=\"whitespace-normal break-words\">Residual connections + layer norm stabilize deep networks</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><h2 class=\"text-xl font-bold text-text-100 mt-1 -mb-0.5\">The Limitations They Identified (Foreshadowing Future Work)</h2></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Quadratic cost</strong>: Self-attention is O(n²) in sequence length. For very long sequences (thousands of tokens), this becomes prohibitive. They noted that restricted attention to local neighborhoods could help.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This limitation drove later innovations:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Sparse attention patterns (Reformer, Longformer)</li>\n<li class=\"whitespace-normal break-words\">Linear attention approximations</li>\n<li class=\"whitespace-normal break-words\">Flash Attention (memory optimization you learned about)</li>\n</ul></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\"><strong>Sequential decoding</strong>: Despite parallelizing training, generation is still sequential (one token at a time). Each new token requires a full forward pass.</p></div></div><div><div class=\"grid-cols-1 grid gap-2.5 [&amp;_&gt;_*]:min-w-0\"><p class=\"whitespace-normal break-words\">This led to:</p>\n<ul class=\"[&amp;:not(:last-child)_ul]:pb-1 [&amp;:not(:last-child)_ol]:pb-1 list-disc space-y-2.5 pl-7\">\n<li class=\"whitespace-normal break-words\">Speculative decoding</li>\n<li class=\"whitespace-normal break-words\">Non-autoregressive models</li>\n<li class=\"whitespace-normal break-words\">Continuous efforts to speed up inference</li></ul></div></div>"
      }
    },
    "2": {
      "readingsComplete": [
        0,
        1,
        2,
        3
      ],
      "notes": "",
      "lastModified": 1762700801317,
      "readingUserNotes": {
        "0": "<h1>Understanding LLM Inference: A Technical Walkthrough</h1>\n<p>Let me walk you through how large language models actually generate text and why this process is so challenging to optimize.</p>\n<h2>How LLMs Generate Text</h2>\n<p>When you send a prompt to an LLM like GPT or Claude, the model goes through two distinct phases. First is the <strong>prefill phase</strong>, where the model processes your entire input prompt all at once. Think of this like the model reading and understanding your question - it can process all the input tokens in parallel, which is great for GPU utilization because GPUs excel at doing many calculations simultaneously. This is mathematically a matrix-matrix operation that keeps the GPU busy and efficient.</p>\n<p>But then comes the <strong>decode phase</strong>, which is where things get tricky. Here, the model generates the response one token at a time, and each new token depends on every token that came before it. This sequential nature means the GPU can't parallelize the work effectively - it's more like a matrix-vector operation that leaves the GPU underutilized. Even worse, the bottleneck isn't the actual computation speed; it's how fast data can be moved from memory to the GPU. This is called a \"memory-bound\" operation, and it's the root cause of many inference challenges.</p>\n<h2>The Memory Problem</h2>\n<p>The GPU needs to store two main things: the <strong>model weights</strong> (the parameters learned during training) and something called the <strong>KV cache</strong>. For a 7 billion parameter model like Llama 2 in 16-bit precision, the weights alone take about 14GB. But the KV cache is where things get interesting and problematic.</p>\n<p>During the decode phase, each new token needs to reference the \"key\" and \"value\" tensors from all previous tokens - both from your original prompt and from every token generated so far. To avoid recomputing these tensors over and over (which would be incredibly wasteful), the model caches them in GPU memory. This cache grows with every token generated, and when you're processing multiple requests in a batch, each request needs its own separate KV cache. The memory usage scales linearly with both batch size and sequence length, which quickly becomes unsustainable.</p>\n<p>Here's the real kicker: traditional systems often over-provision this cache memory. Since you don't know how long a response will be, systems allocate space for the maximum possible sequence length (say, 2,048 tokens) even if the actual response is only 100 tokens. This leads to massive memory waste and fragmentation, limiting how many requests you can handle simultaneously.</p>\n<h2>Spreading the Load: Model Parallelization</h2>\n<p>When a single GPU can't handle your model, you need to distribute it across multiple GPUs. <strong>Pipeline parallelism</strong> divides the model vertically by layers - maybe the first 25% of layers run on GPU 1, the next 25% on GPU 2, and so on. The problem is that data flows sequentially through these stages, creating \"pipeline bubbles\" where GPUs sit idle waiting for the previous stage to finish. You can mitigate this somewhat by splitting your batch into micro-batches that flow through the pipeline more continuously, but you can't eliminate the inefficiency entirely.</p>\n<p><strong>Tensor parallelism</strong> takes a different approach by splitting individual layers horizontally across GPUs. In multi-head attention, different attention heads can run on different GPUs simultaneously. In the feed-forward layers, you can split the weight matrices so different GPUs compute different parts of the same operation in parallel. This is more efficient than pipeline parallelism because the work is truly parallel rather than sequential, though it requires more communication between GPUs. There's also <strong>sequence parallelism</strong> for operations like LayerNorm that can be split across the sequence dimension when tensor parallelism isn't applicable.</p>\n<h2>Smarter Attention Mechanisms</h2>\n<p>The standard <strong>multi-head attention</strong> mechanism creates separate key and value matrices for each attention head, which contributes significantly to that KV cache memory problem. <strong>Multi-Query Attention (MQA)</strong> offers a clever optimization: share the same key and value matrices across all attention heads while still keeping separate query matrices. This drastically reduces the KV cache size and improves memory bandwidth utilization. The computation is identical, but you're reading far less data from memory. The trade-off is that models need to be trained (or at least fine-tuned) with MQA enabled, and there's a potential accuracy drop.</p>\n<p><strong>Grouped-Query Attention (GQA)</strong> strikes a middle ground. Instead of one shared KV pair (MQA) or separate pairs for each head (MHA), it groups attention heads and shares KV pairs within groups. This balances memory efficiency with model quality. Models trained with standard MHA can even be \"uptrained\" to use GQA with just a fraction of the original training compute.</p>\n<p><strong>FlashAttention</strong> takes yet another approach. Instead of optimizing what computations you do, it optimizes <em>how</em> you do them by reordering operations to minimize memory transfers. Traditional implementations compute attention layer-by-layer, constantly reading from and writing to GPU memory. FlashAttention \"fuses\" multiple operations together and uses a technique called \"tiling\" to compute small chunks of the output completely before moving to the next chunk. This keeps data in the GPU's faster cache memory levels longer, dramatically reducing the slow memory I/O operations. The brilliant part? It's mathematically identical to standard attention, so you can drop it into existing models with zero modifications.</p>\n<h2>Better Cache Management</h2>\n<p><strong>PagedAttention</strong> solves the memory waste problem by borrowing a concept from operating systems. Just like your OS doesn't require programs to use contiguous RAM, PagedAttention splits the KV cache into fixed-size blocks that can be stored non-contiguously in memory. A block table keeps track of where everything is. As new tokens generate, new blocks are allocated only as needed. This eliminates the over-provisioning waste (no more allocating for 2,048 tokens when you only need 100) and the fragmentation issues (all blocks are the same size). The result is significantly more efficient memory use, allowing for much larger batch sizes and higher throughput.</p>\n<h2>Compressing the Model Itself</h2>\n<p><strong>Quantization</strong> reduces the precision of the model's numbers. Most models train with 32-bit or 16-bit floating-point precision, but research shows they work fine with 8-bit integers or even lower. This means the same model takes up half or a quarter of the memory, and you can transfer more data over the same bandwidth. Modern GPUs have specialized hardware for low-precision arithmetic, making these operations faster too. The challenge is that while weights are straightforward to quantize (they're fixed after training), activations (the intermediate values during computation) are trickier because they contain outliers that need careful handling to avoid accuracy loss.</p>\n<p><strong>Sparsity</strong> involves pruning the model - replacing near-zero values with actual zeros. Sparse matrices can be stored in compressed formats that take less space, and GPUs have hardware acceleration for certain structured sparsity patterns (like making exactly two out of every four values zero). This is still an active research area for LLMs, but it's promising for future improvements.</p>\n<p><strong>Distillation</strong> takes a completely different approach: train a smaller \"student\" model to mimic a larger \"teacher\" model's behavior. The student learns not just from labeled data but from trying to match the teacher's outputs and sometimes even its intermediate reasoning steps. DistilBERT famously compressed BERT by 40% while retaining 97% of its capabilities at 60% faster speeds. The challenge for LLMs is finding suitable teacher models, since many state-of-the-art models have licenses that prohibit using their outputs to train other models.</p>\n<h2>Smarter Serving Strategies</h2>\n<p>Even with all these optimizations, LLM inference is still typically memory-bandwidth bound - you're limited by how fast you can load weights from memory. So the key is to do as much work as possible with those weights once they're loaded.</p>\n<p><strong>In-flight batching</strong> handles the problem of variable-length outputs. Traditional batching waits for the longest request in a batch to finish before moving to the next batch, wasting GPU time. In-flight batching is more dynamic: as soon as any request finishes, it's immediately replaced with a new request from the queue. Other requests stay \"in flight\" and continue processing. This keeps the GPU consistently busy even when handling diverse workloads - from short chatbot responses to lengthy document summaries.</p>\n<p><strong>Speculative inference</strong> tries to generate multiple tokens in parallel by using a \"draft\" approach. A smaller, faster model generates a draft continuation of several tokens. Then the main model verifies these draft tokens in parallel. If the verification matches the draft, you accept those tokens (saving time because you generated multiple tokens in one pass). If there's a mismatch, you discard everything after the first wrong token and try again. The trick is choosing the right draft strategy - you can use a smaller model, or fine-tune multiple prediction heads, or use other clever approaches. When the draft accuracy is good, this can significantly speed up generation.</p>\n<h2>Putting It All Together</h2>\n<p>The key insight is that optimizing LLM inference isn't about one silver bullet - it's about combining multiple techniques at different levels of the stack. You might use quantization to compress the model, GQA to reduce KV cache size, FlashAttention to optimize memory I/O, PagedAttention to manage cache efficiently, in-flight batching to maximize GPU utilization, and tensor parallelism to scale across GPUs. Each technique addresses a different bottleneck, and together they enable serving large models efficiently enough to be practical in production environments.</p>",
        "1": "<h2>Understanding LLM Agents</h2>\n<p>LLM agents are systems that use large language models to reason through complex problems, create plans, and use tools or APIs to complete tasks. They're particularly valuable for generative AI applications like smart chatbots, automated code generation, and workflow automation. These agents represent one part of the broader agentic AI landscape, which also includes agents powered by computer vision, speech models, and reinforcement learning. The key defining characteristic of LLM agents is their ability to break down complex problems, reason about them, and orchestrate the right tools to solve them rather than following rigid, predefined rules.</p>\n<p>LLM agents operate in two primary application spaces: workflows and chatbots. Workflow agents are designed for offline batch jobs and automated processes, essentially supercharging traditional robotic process automation pipelines. These workflows break complex tasks into predefined, constrained paths dictated by business logic, with LLMs handling the ambiguity and decision-making within each subtask. A prime example is insurance claims processing, where traditional RPA systems struggle with unstructured data from diverse document formats. LLM agents can flexibly process these varied inputs, adapt workflows dynamically based on claim specifics, identify potential fraud, adjust to regulatory changes, and analyze complex scenarios to recommend actions based on policy and historical data. The overall flow remains predetermined by the system architect, but LLMs inject intelligence into handling the nuances of each step.</p>\n<p>Chatbot agents fall into two categories based on response latency and task complexity: exploratory agents and assistive agents. Exploratory agents tackle complex, multistep tasks that take significant time to execute, operating essentially as independent problem-solvers. Users submit tasks and expect complete solutions rather than iterative interaction, accepting higher latency in exchange for comprehensive results. OpenAI's Deep Research and Perplexity's Deep Research exemplify this category, where agents reason through complex problems autonomously to deliver final solutions. Assistive agents, conversely, are built for collaborative human-in-the-loop experiences with lower latency requirements. They use narrow sets of cohesive tools to solve smaller, boilerplate-style problems, letting users focus on architecting larger solutions. Examples include document authoring assistants, coding assistants, personal AI assistants, and tax filing helpers. These agents work alongside users who validate decisions and guide the process rather than operating independently.</p>\n<h2>LLM Reasoning and Test-Time Scaling</h2>\n<p>Reasoning in LLMs means thinking about problems in a logical, sensible way before generating answers. Over recent years, numerous reasoning frameworks have emerged alongside specialized reasoning models like DeepSeek-R1. Understanding these developments requires recognizing three broad reasoning categories: long thinking, searching for the best solution, and think-critique-improve. All three techniques work by scaling test-time compute - improving response quality and enabling more complex problem-solving by generating more tokens during inference. While these techniques are complementary and applicable across different problem spaces, their design differences make them suited to various challenges and use cases.</p>\n<p>Long thinking approaches prompt models to reason step-by-step before generating final answers, with chain-of-thought prompting being the most straightforward implementation. The ReAct framework iterates on this by combining reasoning and action for multi-step decision-making, where reasoning traces help develop strategic plans by breaking complex problems into manageable tasks, and action steps execute those plans by interfacing with external tools. Self-reflection techniques introduced critique loops that force agents to analyze and reassess their reasoning, enabling self-correction for more reliable answers. DeepSeek-R1 represents a major advancement in this category, using reinforcement learning to autonomously explore and refine reasoning strategies, improving both consistency and depth of thought. This makes it one of the most interesting implementations of long-chain, multi-step reasoning available. Long thinking approaches excel at working through complex problems like answering multi-hop questions from financial reports or solving logical reasoning challenges, ultimately enabling deeper problem understanding.</p>\n<p>Searching for the best solution addresses tasks with multiple valid solutions rather than single correct answers. Tree-of-thought and graph-of-thought techniques enable LLMs to reason through multiple reasoning directions simultaneously. Best-of-N approaches rely on a simple but powerful principle: if you give the model many attempts, it's more likely to generate the correct response. This means asking the same question repeatedly until getting it right or at least maximizing the probability of correctness. Research has used extremely high values of N for problems like code generation, though generating high volumes of responses is only part of the solution. The system needs a way to select the best solution from those N attempts, which introduces the verification problem. For some cases, verification is straightforward - does the code run and pass tests? For others, it requires more complex processes like reward models or sophisticated verification systems. Search techniques like beam search and lookahead search provide structured ways to explore solution spaces and select optimal paths.</p>\n<p>Think-critique-improve approaches take an interactive, collaborative process rather than extended individual thinking. The pipeline works in stages: first, generate N samples similar to best-of-N approaches; second, generate feedback for each sample using a specialized model, filtering non-useful responses and selecting top-k based on heuristics; third, use a specialized editor model to incorporate feedback by editing the base model's responses for each of the N samples; finally, select the final response from the N feedback-incorporated and edited responses using a selection model. This method resembles a group collaborating on a problem rather than one person thinking alone for extended periods. While other methods rely on verifiable problems like code, math, or logical reasoning during training or implementation, think-critique-improve excels at open-ended problems where there isn't necessarily one right answer but rather better or worse approaches.</p>\n<h2>Practical Implications</h2>\n<p>The choice between reasoning approaches depends on your specific problem characteristics. For complex problems requiring deep understanding and step-by-step logic, long thinking approaches like chain-of-thought or DeepSeek-R1's reinforcement learning-tuned reasoning work best. For problems with multiple valid solutions or where correctness is verifiable, search-based techniques like best-of-N provide robust results by exploring the solution space. For open-ended creative or subjective tasks, think-critique-improve offers iterative refinement through collaborative feedback processes. Understanding these distinctions helps architects choose appropriate reasoning strategies for their agent applications, balancing factors like latency requirements, problem complexity, solution verifiability, and whether tasks are exploratory or assistive in nature. The rapid evolution of agent frameworks and reasoning models makes this an exciting but complex space, where matching techniques to use cases determines success in building reliable, effective LLM agents.</p>",
        "2": "<h2>Understanding Reasoning Models and Test-Time Computation</h2>\n<p>Reasoning models represent a transformative advancement in language model capabilities. These models leverage test-time computation scaling laws, meaning they spend more time generating tokens and internally reasoning about various aspects of a problem before producing the final answer. This extended thinking process makes them exceptionally skilled at tasks demanding deep critical thinking and reasoning, such as mathematics and coding. The paradigm shift involves moving from models that simply predict the next token to models that explicitly work through problems step-by-step, showing their reasoning process. NVIDIA stands at the forefront of this advancement with its Nemotron family of models, which are among the most open and efficient models designed for agentic AI. These models are trained with open training data and AI techniques, providing full visibility, enabling better compliance, and ensuring trustworthy AI deployment. This transparency and accessibility democratize reasoning model development, making it possible for practitioners to train their own reasoning-capable models.</p>\n<p>A key innovation in the Llama Nemotron models is their dynamic reasoning toggle, which allows users to switch between standard chat mode (reasoning off) and advanced reasoning mode (reasoning on) during inference through a simple instruction in the system prompt. This controllable reasoning provides remarkable flexibility for optimized resource utilization. Users can engage deep reasoning capabilities for complex tasks like scientific analysis or coding that genuinely require step-by-step thinking, while reverting to lightweight mode for simpler interactions to reduce latency and computational costs. This toggle is implemented through system prompts like \"detailed thinking on\" or \"detailed thinking off,\" giving fine-grained control over when the model should show its work versus when it should provide direct answers. This design recognizes that not every query requires extended reasoning, and forcing reasoning for simple questions wastes resources and increases latency unnecessarily.</p>\n<h2>The Llama Nemotron Post-Training Dataset</h2>\n<p>To empower the developer community, NVIDIA has open-sourced a substantial portion of the data used in the post-training pipeline of the Llama Nemotron models. The Llama Nemotron Post-Training Dataset contains over 32 million samples across diverse areas including math (22 million samples), coding (10 million samples), science, instruction following, chat, and safety. This massive dataset provides a foundation for practitioners to train their own reasoning models and is key to teaching models how to control their reasoning mode, mirroring Llama Nemotron capabilities. The dataset is meticulously synthesized to enhance reasoning capabilities, organized into distinct subsets for supervised fine-tuning or reinforcement learning, and encompasses samples from various problem domains.</p>\n<p>All samples in the dataset are in JSON lines format and contain rich metadata including license type, source model, and which Llama Nemotron models used that sample during training. Each sample consists of a prompt in multi-turn chat format, an expected response output, a reasoning field indicating whether the sample demonstrates reasoning-on or reasoning-off mode, a system prompt for controlling reasoning mode, category information, license details, the generator model used to synthesize the sample, and versioning information. For reasoning-on samples, the output contains detailed chain-of-thought traces enclosed in special <code>&lt;think&gt;&lt;/think&gt;</code> tags followed by the final response. For reasoning-off samples, the output contains direct responses without explicit reasoning traces. This dual-mode structure in the training data is what enables the model to learn controllable reasoning behavior during fine-tuning.</p>\n<h2>Training Your Own Reasoning Model in Three Steps</h2>\n<p>Training a reasoning model typically involves data curation, fine-tuning, and evaluation. NVIDIA provides a proven recipe that enables training a model on a single GPU in just 48 hours, making reasoning model development accessible without requiring massive computational resources. Several key considerations inform this approach. First, dataset composition matters - while the full dataset is large, you need to curate a focused subset emphasizing reasoning, prioritizing samples that align with your domain-specific tasks and potentially augmenting with your own domain data. Second, base model selection is critical because teaching small models to reason is challenging. Models of at least 8 billion parameters are recommended, with Llama 3.1 8B Instruct being a proven choice. Third, the fine-tuning technique must be practical - while fully fine-tuning all weights of an 8-billion parameter model requires at least eight GPUs and significant time, parameter-efficient fine-tuning using LoRA adapters achieves comparable results on a single NVIDIA H100 GPU in 48 hours.</p>\n<p>Step one involves processing data with NVIDIA NeMo Curator to create high-quality training data. The recommended approach starts by selecting appropriate subsets - using Llama Nemotron Nano samples that are pre-vetted and focusing on math and chat subsets for strong domain-agnostic reasoning. Filtering is crucial: remove non-English samples through language identification, discard math samples without proper answer formatting, exclude refusal samples with empty thinking tags, and restrict samples to reasonable token limits like 8,192 or 16,384 tokens. After filtering, apply a chat template to format samples consistently with system, user, and assistant roles, add control statements to system prompts to signal reasoning mode, and implement curriculum learning by sorting samples in increasing order of difficulty. The curriculum learning approach involves splitting data into reasoning-on and reasoning-off buckets, sorting each by completion length as a proxy for difficulty, and interleaving samples to gradually introduce complexity. NVIDIA provides code on GitHub implementing this pipeline efficiently, running locally on modest hardware without requiring GPUs. The pipeline demonstrates facilities like language identification and distributed processing to quickly prepare subsets of the dataset for fine-tuning. Following the recommended approach produces approximately 1.7 million curated samples ready for training.</p>\n<p>Step two involves the actual training process using NVIDIA NeMo Framework. Experiments across models ranging from 3 billion to 8 billion parameters with LoRA ranks from 16 to 128 found that Llama 3.1 8B Instruct with LoRA rank 64 provided the sweet spot for strong reasoning performance. Key factors contributing to successful training include using a high learning rate to accelerate convergence, implementing curriculum learning with progressively harder samples to significantly improve stability and final performance, and maintaining a batch size of at least 256. The specific hyperparameters that worked well include LoRA rank 64 with alpha 128, learning rate of 0.0001 with cosine scheduling and 5% warmup, weight decay of 0.001, batch size of 256 using gradient accumulation, and training for at least 2,000 steps. Training on a single NVIDIA H100 80GB GPU takes around 30 hours, with consistent reasoning behavior emerging after just 13 hours of training (after processing 100,000 to 130,000 samples). For GPUs with less than 80GB memory, you can reduce on-device batch size and increase gradient accumulation steps to maintain effective batch size while working within memory constraints. NVIDIA provides a Jupyter notebook on GitHub that sets up the training pipeline with appropriate hyperparameters, walks through available settings, and provides options for full model fine-tuning instead of parameter-efficient methods.</p>\n<p>Step three focuses on evaluation to confirm that reasoning capabilities have been learned. The recommended approach includes benchmarking against the base model with side-by-side comparisons on reasoning-heavy tasks to assess improvement, evaluating on standard benchmarks like MMLU, GPQA Diamond, GPQA Main, or OpenBookQA to gauge overall capabilities while also testing on domain-specific data for production behavior insights, and manually inspecting outputs for both reasoning-on and reasoning-off modes to verify controllability and consistency. NVIDIA provides scripts demonstrating dataset download and preparation, model deployment using Triton Inference Server with OpenAI API endpoints, and running relevant benchmarks. The evaluation process involves preparing datasets in the proper format with questions, choices, and correct answers, deploying models through the chat completions endpoint with appropriate system prompts for reasoning mode control, and comparing model responses against ground truth to calculate accuracy.</p>\n<h2>Results and Practical Implications</h2>\n<p>The results from this 48-hour training approach are impressive. The trained LoRA adapter significantly outperforms the base instruct model on various benchmarks, sometimes by as much as 10 percentage points. These improvements are particularly notable because the model was trained for only 48 hours on a relatively small number of training samples using a single GPU. LLM scaling laws predict that increasing the number of training samples and allotted training time could train even stronger reasoning models. The evaluation showed consistent gains across GPQA Diamond, GPQA Main, and MMLU benchmarks, demonstrating that the reasoning capabilities generalize across different types of reasoning tasks.</p>\n<p>The practical implications are significant for practitioners. First, training reasoning models is now accessible without requiring massive computational infrastructure - a single high-end GPU suffices. Second, the open-source nature of the dataset, tools, and code removes barriers to entry, allowing developers to experiment with reasoning model training and adapt the approach to their specific domains. Third, the controllable reasoning toggle provides deployment flexibility, letting applications decide when to use expensive reasoning versus fast direct responses based on query complexity. Fourth, since the example model was trained only on math and chat data, its reasoning abilities are generic, but introducing domain-specific data enables training models proficient in specific problem domains relevant to particular applications or business needs.</p>\n<p>The training recipe demonstrates that supervised fine-tuning alone can instill strong reasoning capabilities, though the approach notes that reinforcement learning is also an option and recent work suggests multi-pass approaches (supervised fine-tuning followed by reinforcement learning) yield the best results. The curriculum learning strategy proved particularly important, with the ordered presentation of progressively harder samples significantly improving training stability and final performance. The emergence of consistent reasoning behavior after processing just 100,000 samples suggests that models can learn reasoning patterns relatively quickly once exposed to high-quality examples with explicit reasoning traces. This accessibility transforms reasoning model development from a capability limited to large research labs with massive compute budgets into something individual developers and smaller organizations can accomplish over a weekend, democratizing access to this transformative capability and enabling domain-specific reasoning applications across industries.</p>",
        "3": "<h2>System Overview and Architecture</h2>\n<p>This example demonstrates a multi-turn conversational AI system built on a retrieval-augmented generation pipeline that maintains conversation history and accesses a knowledge base through vector databases. The chain server is the core component that stores both the conversation history and the knowledge base in separate vector stores, retrieving them at runtime to understand contextual queries that reference previous parts of the conversation. This architecture enables the chatbot to handle follow-up questions and maintain coherent multi-turn dialogues where later questions depend on context established earlier in the conversation.</p>\n<p>The system uses a dual vector store design to separate concerns and optimize retrieval. The first vector store, called multi_turn_rag, is dedicated to storing ingested documents from the knowledge base. Users can upload PDF and text files that get processed and stored here. The second vector store, called conv_store, is dedicated to conversation history and stores each previously asked query along with the model's generated answer as text entries. Both vector stores are integrated into a LangChain LCEL chain as LangChain Retrievers, creating a unified pipeline. When a user submits a query, it passes through both retrievers simultaneously - one retriever fetches relevant context from the document knowledge base while the other retrieves the closest-matching conversation history entries. These retrieved chunks are then combined and injected into the language model prompt, giving the model both factual grounding from documents and conversational context from previous exchanges.</p>\n<p>The example uses the Llama 2 70B model accessed through NVIDIA API Catalog endpoints rather than requiring local model deployment, significantly simplifying infrastructure requirements. The system uses the nvolveqa_40k embedding model for converting text into vectors, leverages the LangChain framework for orchestrating the RAG pipeline, and uses Milvus as the vector database backend. The architecture does not require multiple GPUs or TensorRT-LLM optimization since inference happens through API calls. The sample includes a web application that communicates with the chain server, which in turn sends inference requests to the NVIDIA API Catalog endpoint. Optionally, the system can integrate with NVIDIA Riva for automatic speech recognition to transcribe spoken questions and text-to-speech to speak answers aloud, creating a voice-enabled conversational interface.</p>\n<h2>Setup and Deployment Process</h2>\n<p>Setting up the system involves several prerequisites and configuration steps. First, you need to clone the Generative AI examples Git repository using Git LFS to ensure all large files are properly downloaded. You'll need Docker Engine and Docker Compose installed to run the containerized components. If you want voice capabilities, you can optionally enable NVIDIA Riva by either launching a local Riva server following the Quick Start Guide with ASR and TTS services enabled for your desired languages, or by using a hosted Riva API endpoint with appropriate API keys and function IDs configured in the environment file.</p>\n<p>The most critical setup step is obtaining an API key for the Llama 2 70B model endpoint from NVIDIA's API Catalog. You navigate to the build.ngc.nvidia.com platform, find the Llama 2 70B model card, and generate an API key that begins with \"nvapi-\". This same API key can be used across different model endpoints in the catalog. Once you have the key, you edit the deploy/compose/compose.env file to add it as the NVIDIA_API_KEY environment variable. The deployment process then involves building the Docker containers using the provided compose file specifically for the multi-turn chatbot application, starting those containers which launches both the chain server and the RAG playground web interface, separately starting the Milvus vector database with its dependencies (MinIO for object storage and etcd for metadata), and finally confirming all containers are running properly.</p>\n<h2>Using the System</h2>\n<p>After deployment, users access the chat server through a web interface where they can interact with the conversational AI. The workflow begins by uploading PDF or text files to the knowledge base, which get processed and stored in the document vector store. When asking questions, users can enable the \"Use knowledge base\" checkbox to have the system retrieve relevant information from uploaded documents alongside conversation history. The system's prompt is specifically tuned to act as a document chatbot, meaning it's optimized for answering questions about the content in the knowledge base while maintaining conversational coherence across multiple turns. As the conversation progresses, each question-answer pair gets stored in the conversation history vector store, allowing the system to reference earlier parts of the dialogue when interpreting subsequent questions. This creates a natural conversational flow where users can ask follow-up questions, request clarifications, or build on previous topics without needing to repeat context, making the interaction feel more like chatting with a knowledgeable assistant than querying a static database.</p>"
      },
      "readingCompletedAt": {
        "0": 1762696229599,
        "1": 1762700576467,
        "2": 1762700710333,
        "3": 1762700801317
      },
      "readingNotes": {
        "0": "<h1>Understanding LLM Inference: A Technical Walkthrough</h1>\n<p>Let me walk you through how large language models actually generate text and why this process is so challenging to optimize.</p>\n<h2>How LLMs Generate Text</h2>\n<p>When you send a prompt to an LLM like GPT or Claude, the model goes through two distinct phases. First is the <strong>prefill phase</strong>, where the model processes your entire input prompt all at once. Think of this like the model reading and understanding your question - it can process all the input tokens in parallel, which is great for GPU utilization because GPUs excel at doing many calculations simultaneously. This is mathematically a matrix-matrix operation that keeps the GPU busy and efficient.</p>\n<p>But then comes the <strong>decode phase</strong>, which is where things get tricky. Here, the model generates the response one token at a time, and each new token depends on every token that came before it. This sequential nature means the GPU can't parallelize the work effectively - it's more like a matrix-vector operation that leaves the GPU underutilized. Even worse, the bottleneck isn't the actual computation speed; it's how fast data can be moved from memory to the GPU. This is called a \"memory-bound\" operation, and it's the root cause of many inference challenges.</p>\n<h2>The Memory Problem</h2>\n<p>The GPU needs to store two main things: the <strong>model weights</strong> (the parameters learned during training) and something called the <strong>KV cache</strong>. For a 7 billion parameter model like Llama 2 in 16-bit precision, the weights alone take about 14GB. But the KV cache is where things get interesting and problematic.</p>\n<p>During the decode phase, each new token needs to reference the \"key\" and \"value\" tensors from all previous tokens - both from your original prompt and from every token generated so far. To avoid recomputing these tensors over and over (which would be incredibly wasteful), the model caches them in GPU memory. This cache grows with every token generated, and when you're processing multiple requests in a batch, each request needs its own separate KV cache. The memory usage scales linearly with both batch size and sequence length, which quickly becomes unsustainable.</p>\n<p>Here's the real kicker: traditional systems often over-provision this cache memory. Since you don't know how long a response will be, systems allocate space for the maximum possible sequence length (say, 2,048 tokens) even if the actual response is only 100 tokens. This leads to massive memory waste and fragmentation, limiting how many requests you can handle simultaneously.</p>\n<h2>Spreading the Load: Model Parallelization</h2>\n<p>When a single GPU can't handle your model, you need to distribute it across multiple GPUs. <strong>Pipeline parallelism</strong> divides the model vertically by layers - maybe the first 25% of layers run on GPU 1, the next 25% on GPU 2, and so on. The problem is that data flows sequentially through these stages, creating \"pipeline bubbles\" where GPUs sit idle waiting for the previous stage to finish. You can mitigate this somewhat by splitting your batch into micro-batches that flow through the pipeline more continuously, but you can't eliminate the inefficiency entirely.</p>\n<p><strong>Tensor parallelism</strong> takes a different approach by splitting individual layers horizontally across GPUs. In multi-head attention, different attention heads can run on different GPUs simultaneously. In the feed-forward layers, you can split the weight matrices so different GPUs compute different parts of the same operation in parallel. This is more efficient than pipeline parallelism because the work is truly parallel rather than sequential, though it requires more communication between GPUs. There's also <strong>sequence parallelism</strong> for operations like LayerNorm that can be split across the sequence dimension when tensor parallelism isn't applicable.</p>\n<h2>Smarter Attention Mechanisms</h2>\n<p>The standard <strong>multi-head attention</strong> mechanism creates separate key and value matrices for each attention head, which contributes significantly to that KV cache memory problem. <strong>Multi-Query Attention (MQA)</strong> offers a clever optimization: share the same key and value matrices across all attention heads while still keeping separate query matrices. This drastically reduces the KV cache size and improves memory bandwidth utilization. The computation is identical, but you're reading far less data from memory. The trade-off is that models need to be trained (or at least fine-tuned) with MQA enabled, and there's a potential accuracy drop.</p>\n<p><strong>Grouped-Query Attention (GQA)</strong> strikes a middle ground. Instead of one shared KV pair (MQA) or separate pairs for each head (MHA), it groups attention heads and shares KV pairs within groups. This balances memory efficiency with model quality. Models trained with standard MHA can even be \"uptrained\" to use GQA with just a fraction of the original training compute.</p>\n<p><strong>FlashAttention</strong> takes yet another approach. Instead of optimizing what computations you do, it optimizes <em>how</em> you do them by reordering operations to minimize memory transfers. Traditional implementations compute attention layer-by-layer, constantly reading from and writing to GPU memory. FlashAttention \"fuses\" multiple operations together and uses a technique called \"tiling\" to compute small chunks of the output completely before moving to the next chunk. This keeps data in the GPU's faster cache memory levels longer, dramatically reducing the slow memory I/O operations. The brilliant part? It's mathematically identical to standard attention, so you can drop it into existing models with zero modifications.</p>\n<h2>Better Cache Management</h2>\n<p><strong>PagedAttention</strong> solves the memory waste problem by borrowing a concept from operating systems. Just like your OS doesn't require programs to use contiguous RAM, PagedAttention splits the KV cache into fixed-size blocks that can be stored non-contiguously in memory. A block table keeps track of where everything is. As new tokens generate, new blocks are allocated only as needed. This eliminates the over-provisioning waste (no more allocating for 2,048 tokens when you only need 100) and the fragmentation issues (all blocks are the same size). The result is significantly more efficient memory use, allowing for much larger batch sizes and higher throughput.</p>\n<h2>Compressing the Model Itself</h2>\n<p><strong>Quantization</strong> reduces the precision of the model's numbers. Most models train with 32-bit or 16-bit floating-point precision, but research shows they work fine with 8-bit integers or even lower. This means the same model takes up half or a quarter of the memory, and you can transfer more data over the same bandwidth. Modern GPUs have specialized hardware for low-precision arithmetic, making these operations faster too. The challenge is that while weights are straightforward to quantize (they're fixed after training), activations (the intermediate values during computation) are trickier because they contain outliers that need careful handling to avoid accuracy loss.</p>\n<p><strong>Sparsity</strong> involves pruning the model - replacing near-zero values with actual zeros. Sparse matrices can be stored in compressed formats that take less space, and GPUs have hardware acceleration for certain structured sparsity patterns (like making exactly two out of every four values zero). This is still an active research area for LLMs, but it's promising for future improvements.</p>\n<p><strong>Distillation</strong> takes a completely different approach: train a smaller \"student\" model to mimic a larger \"teacher\" model's behavior. The student learns not just from labeled data but from trying to match the teacher's outputs and sometimes even its intermediate reasoning steps. DistilBERT famously compressed BERT by 40% while retaining 97% of its capabilities at 60% faster speeds. The challenge for LLMs is finding suitable teacher models, since many state-of-the-art models have licenses that prohibit using their outputs to train other models.</p>\n<h2>Smarter Serving Strategies</h2>\n<p>Even with all these optimizations, LLM inference is still typically memory-bandwidth bound - you're limited by how fast you can load weights from memory. So the key is to do as much work as possible with those weights once they're loaded.</p>\n<p><strong>In-flight batching</strong> handles the problem of variable-length outputs. Traditional batching waits for the longest request in a batch to finish before moving to the next batch, wasting GPU time. In-flight batching is more dynamic: as soon as any request finishes, it's immediately replaced with a new request from the queue. Other requests stay \"in flight\" and continue processing. This keeps the GPU consistently busy even when handling diverse workloads - from short chatbot responses to lengthy document summaries.</p>\n<p><strong>Speculative inference</strong> tries to generate multiple tokens in parallel by using a \"draft\" approach. A smaller, faster model generates a draft continuation of several tokens. Then the main model verifies these draft tokens in parallel. If the verification matches the draft, you accept those tokens (saving time because you generated multiple tokens in one pass). If there's a mismatch, you discard everything after the first wrong token and try again. The trick is choosing the right draft strategy - you can use a smaller model, or fine-tune multiple prediction heads, or use other clever approaches. When the draft accuracy is good, this can significantly speed up generation.</p>\n<h2>Putting It All Together</h2>\n<p>The key insight is that optimizing LLM inference isn't about one silver bullet - it's about combining multiple techniques at different levels of the stack. You might use quantization to compress the model, GQA to reduce KV cache size, FlashAttention to optimize memory I/O, PagedAttention to manage cache efficiently, in-flight batching to maximize GPU utilization, and tensor parallelism to scale across GPUs. Each technique addresses a different bottleneck, and together they enable serving large models efficiently enough to be practical in production environments.</p>",
        "1": "<h2>Understanding LLM Agents</h2>\n<p>LLM agents are systems that use large language models to reason through complex problems, create plans, and use tools or APIs to complete tasks. They're particularly valuable for generative AI applications like smart chatbots, automated code generation, and workflow automation. These agents represent one part of the broader agentic AI landscape, which also includes agents powered by computer vision, speech models, and reinforcement learning. The key defining characteristic of LLM agents is their ability to break down complex problems, reason about them, and orchestrate the right tools to solve them rather than following rigid, predefined rules.</p>\n<p>LLM agents operate in two primary application spaces: workflows and chatbots. Workflow agents are designed for offline batch jobs and automated processes, essentially supercharging traditional robotic process automation pipelines. These workflows break complex tasks into predefined, constrained paths dictated by business logic, with LLMs handling the ambiguity and decision-making within each subtask. A prime example is insurance claims processing, where traditional RPA systems struggle with unstructured data from diverse document formats. LLM agents can flexibly process these varied inputs, adapt workflows dynamically based on claim specifics, identify potential fraud, adjust to regulatory changes, and analyze complex scenarios to recommend actions based on policy and historical data. The overall flow remains predetermined by the system architect, but LLMs inject intelligence into handling the nuances of each step.</p>\n<p>Chatbot agents fall into two categories based on response latency and task complexity: exploratory agents and assistive agents. Exploratory agents tackle complex, multistep tasks that take significant time to execute, operating essentially as independent problem-solvers. Users submit tasks and expect complete solutions rather than iterative interaction, accepting higher latency in exchange for comprehensive results. OpenAI's Deep Research and Perplexity's Deep Research exemplify this category, where agents reason through complex problems autonomously to deliver final solutions. Assistive agents, conversely, are built for collaborative human-in-the-loop experiences with lower latency requirements. They use narrow sets of cohesive tools to solve smaller, boilerplate-style problems, letting users focus on architecting larger solutions. Examples include document authoring assistants, coding assistants, personal AI assistants, and tax filing helpers. These agents work alongside users who validate decisions and guide the process rather than operating independently.</p>\n<h2>LLM Reasoning and Test-Time Scaling</h2>\n<p>Reasoning in LLMs means thinking about problems in a logical, sensible way before generating answers. Over recent years, numerous reasoning frameworks have emerged alongside specialized reasoning models like DeepSeek-R1. Understanding these developments requires recognizing three broad reasoning categories: long thinking, searching for the best solution, and think-critique-improve. All three techniques work by scaling test-time compute - improving response quality and enabling more complex problem-solving by generating more tokens during inference. While these techniques are complementary and applicable across different problem spaces, their design differences make them suited to various challenges and use cases.</p>\n<p>Long thinking approaches prompt models to reason step-by-step before generating final answers, with chain-of-thought prompting being the most straightforward implementation. The ReAct framework iterates on this by combining reasoning and action for multi-step decision-making, where reasoning traces help develop strategic plans by breaking complex problems into manageable tasks, and action steps execute those plans by interfacing with external tools. Self-reflection techniques introduced critique loops that force agents to analyze and reassess their reasoning, enabling self-correction for more reliable answers. DeepSeek-R1 represents a major advancement in this category, using reinforcement learning to autonomously explore and refine reasoning strategies, improving both consistency and depth of thought. This makes it one of the most interesting implementations of long-chain, multi-step reasoning available. Long thinking approaches excel at working through complex problems like answering multi-hop questions from financial reports or solving logical reasoning challenges, ultimately enabling deeper problem understanding.</p>\n<p>Searching for the best solution addresses tasks with multiple valid solutions rather than single correct answers. Tree-of-thought and graph-of-thought techniques enable LLMs to reason through multiple reasoning directions simultaneously. Best-of-N approaches rely on a simple but powerful principle: if you give the model many attempts, it's more likely to generate the correct response. This means asking the same question repeatedly until getting it right or at least maximizing the probability of correctness. Research has used extremely high values of N for problems like code generation, though generating high volumes of responses is only part of the solution. The system needs a way to select the best solution from those N attempts, which introduces the verification problem. For some cases, verification is straightforward - does the code run and pass tests? For others, it requires more complex processes like reward models or sophisticated verification systems. Search techniques like beam search and lookahead search provide structured ways to explore solution spaces and select optimal paths.</p>\n<p>Think-critique-improve approaches take an interactive, collaborative process rather than extended individual thinking. The pipeline works in stages: first, generate N samples similar to best-of-N approaches; second, generate feedback for each sample using a specialized model, filtering non-useful responses and selecting top-k based on heuristics; third, use a specialized editor model to incorporate feedback by editing the base model's responses for each of the N samples; finally, select the final response from the N feedback-incorporated and edited responses using a selection model. This method resembles a group collaborating on a problem rather than one person thinking alone for extended periods. While other methods rely on verifiable problems like code, math, or logical reasoning during training or implementation, think-critique-improve excels at open-ended problems where there isn't necessarily one right answer but rather better or worse approaches.</p>\n<h2>Practical Implications</h2>\n<p>The choice between reasoning approaches depends on your specific problem characteristics. For complex problems requiring deep understanding and step-by-step logic, long thinking approaches like chain-of-thought or DeepSeek-R1's reinforcement learning-tuned reasoning work best. For problems with multiple valid solutions or where correctness is verifiable, search-based techniques like best-of-N provide robust results by exploring the solution space. For open-ended creative or subjective tasks, think-critique-improve offers iterative refinement through collaborative feedback processes. Understanding these distinctions helps architects choose appropriate reasoning strategies for their agent applications, balancing factors like latency requirements, problem complexity, solution verifiability, and whether tasks are exploratory or assistive in nature. The rapid evolution of agent frameworks and reasoning models makes this an exciting but complex space, where matching techniques to use cases determines success in building reliable, effective LLM agents.</p>",
        "2": "<h2>Understanding Reasoning Models and Test-Time Computation</h2>\n<p>Reasoning models represent a transformative advancement in language model capabilities. These models leverage test-time computation scaling laws, meaning they spend more time generating tokens and internally reasoning about various aspects of a problem before producing the final answer. This extended thinking process makes them exceptionally skilled at tasks demanding deep critical thinking and reasoning, such as mathematics and coding. The paradigm shift involves moving from models that simply predict the next token to models that explicitly work through problems step-by-step, showing their reasoning process. NVIDIA stands at the forefront of this advancement with its Nemotron family of models, which are among the most open and efficient models designed for agentic AI. These models are trained with open training data and AI techniques, providing full visibility, enabling better compliance, and ensuring trustworthy AI deployment. This transparency and accessibility democratize reasoning model development, making it possible for practitioners to train their own reasoning-capable models.</p>\n<p>A key innovation in the Llama Nemotron models is their dynamic reasoning toggle, which allows users to switch between standard chat mode (reasoning off) and advanced reasoning mode (reasoning on) during inference through a simple instruction in the system prompt. This controllable reasoning provides remarkable flexibility for optimized resource utilization. Users can engage deep reasoning capabilities for complex tasks like scientific analysis or coding that genuinely require step-by-step thinking, while reverting to lightweight mode for simpler interactions to reduce latency and computational costs. This toggle is implemented through system prompts like \"detailed thinking on\" or \"detailed thinking off,\" giving fine-grained control over when the model should show its work versus when it should provide direct answers. This design recognizes that not every query requires extended reasoning, and forcing reasoning for simple questions wastes resources and increases latency unnecessarily.</p>\n<h2>The Llama Nemotron Post-Training Dataset</h2>\n<p>To empower the developer community, NVIDIA has open-sourced a substantial portion of the data used in the post-training pipeline of the Llama Nemotron models. The Llama Nemotron Post-Training Dataset contains over 32 million samples across diverse areas including math (22 million samples), coding (10 million samples), science, instruction following, chat, and safety. This massive dataset provides a foundation for practitioners to train their own reasoning models and is key to teaching models how to control their reasoning mode, mirroring Llama Nemotron capabilities. The dataset is meticulously synthesized to enhance reasoning capabilities, organized into distinct subsets for supervised fine-tuning or reinforcement learning, and encompasses samples from various problem domains.</p>\n<p>All samples in the dataset are in JSON lines format and contain rich metadata including license type, source model, and which Llama Nemotron models used that sample during training. Each sample consists of a prompt in multi-turn chat format, an expected response output, a reasoning field indicating whether the sample demonstrates reasoning-on or reasoning-off mode, a system prompt for controlling reasoning mode, category information, license details, the generator model used to synthesize the sample, and versioning information. For reasoning-on samples, the output contains detailed chain-of-thought traces enclosed in special <code>&lt;think&gt;&lt;/think&gt;</code> tags followed by the final response. For reasoning-off samples, the output contains direct responses without explicit reasoning traces. This dual-mode structure in the training data is what enables the model to learn controllable reasoning behavior during fine-tuning.</p>\n<h2>Training Your Own Reasoning Model in Three Steps</h2>\n<p>Training a reasoning model typically involves data curation, fine-tuning, and evaluation. NVIDIA provides a proven recipe that enables training a model on a single GPU in just 48 hours, making reasoning model development accessible without requiring massive computational resources. Several key considerations inform this approach. First, dataset composition matters - while the full dataset is large, you need to curate a focused subset emphasizing reasoning, prioritizing samples that align with your domain-specific tasks and potentially augmenting with your own domain data. Second, base model selection is critical because teaching small models to reason is challenging. Models of at least 8 billion parameters are recommended, with Llama 3.1 8B Instruct being a proven choice. Third, the fine-tuning technique must be practical - while fully fine-tuning all weights of an 8-billion parameter model requires at least eight GPUs and significant time, parameter-efficient fine-tuning using LoRA adapters achieves comparable results on a single NVIDIA H100 GPU in 48 hours.</p>\n<p>Step one involves processing data with NVIDIA NeMo Curator to create high-quality training data. The recommended approach starts by selecting appropriate subsets - using Llama Nemotron Nano samples that are pre-vetted and focusing on math and chat subsets for strong domain-agnostic reasoning. Filtering is crucial: remove non-English samples through language identification, discard math samples without proper answer formatting, exclude refusal samples with empty thinking tags, and restrict samples to reasonable token limits like 8,192 or 16,384 tokens. After filtering, apply a chat template to format samples consistently with system, user, and assistant roles, add control statements to system prompts to signal reasoning mode, and implement curriculum learning by sorting samples in increasing order of difficulty. The curriculum learning approach involves splitting data into reasoning-on and reasoning-off buckets, sorting each by completion length as a proxy for difficulty, and interleaving samples to gradually introduce complexity. NVIDIA provides code on GitHub implementing this pipeline efficiently, running locally on modest hardware without requiring GPUs. The pipeline demonstrates facilities like language identification and distributed processing to quickly prepare subsets of the dataset for fine-tuning. Following the recommended approach produces approximately 1.7 million curated samples ready for training.</p>\n<p>Step two involves the actual training process using NVIDIA NeMo Framework. Experiments across models ranging from 3 billion to 8 billion parameters with LoRA ranks from 16 to 128 found that Llama 3.1 8B Instruct with LoRA rank 64 provided the sweet spot for strong reasoning performance. Key factors contributing to successful training include using a high learning rate to accelerate convergence, implementing curriculum learning with progressively harder samples to significantly improve stability and final performance, and maintaining a batch size of at least 256. The specific hyperparameters that worked well include LoRA rank 64 with alpha 128, learning rate of 0.0001 with cosine scheduling and 5% warmup, weight decay of 0.001, batch size of 256 using gradient accumulation, and training for at least 2,000 steps. Training on a single NVIDIA H100 80GB GPU takes around 30 hours, with consistent reasoning behavior emerging after just 13 hours of training (after processing 100,000 to 130,000 samples). For GPUs with less than 80GB memory, you can reduce on-device batch size and increase gradient accumulation steps to maintain effective batch size while working within memory constraints. NVIDIA provides a Jupyter notebook on GitHub that sets up the training pipeline with appropriate hyperparameters, walks through available settings, and provides options for full model fine-tuning instead of parameter-efficient methods.</p>\n<p>Step three focuses on evaluation to confirm that reasoning capabilities have been learned. The recommended approach includes benchmarking against the base model with side-by-side comparisons on reasoning-heavy tasks to assess improvement, evaluating on standard benchmarks like MMLU, GPQA Diamond, GPQA Main, or OpenBookQA to gauge overall capabilities while also testing on domain-specific data for production behavior insights, and manually inspecting outputs for both reasoning-on and reasoning-off modes to verify controllability and consistency. NVIDIA provides scripts demonstrating dataset download and preparation, model deployment using Triton Inference Server with OpenAI API endpoints, and running relevant benchmarks. The evaluation process involves preparing datasets in the proper format with questions, choices, and correct answers, deploying models through the chat completions endpoint with appropriate system prompts for reasoning mode control, and comparing model responses against ground truth to calculate accuracy.</p>\n<h2>Results and Practical Implications</h2>\n<p>The results from this 48-hour training approach are impressive. The trained LoRA adapter significantly outperforms the base instruct model on various benchmarks, sometimes by as much as 10 percentage points. These improvements are particularly notable because the model was trained for only 48 hours on a relatively small number of training samples using a single GPU. LLM scaling laws predict that increasing the number of training samples and allotted training time could train even stronger reasoning models. The evaluation showed consistent gains across GPQA Diamond, GPQA Main, and MMLU benchmarks, demonstrating that the reasoning capabilities generalize across different types of reasoning tasks.</p>\n<p>The practical implications are significant for practitioners. First, training reasoning models is now accessible without requiring massive computational infrastructure - a single high-end GPU suffices. Second, the open-source nature of the dataset, tools, and code removes barriers to entry, allowing developers to experiment with reasoning model training and adapt the approach to their specific domains. Third, the controllable reasoning toggle provides deployment flexibility, letting applications decide when to use expensive reasoning versus fast direct responses based on query complexity. Fourth, since the example model was trained only on math and chat data, its reasoning abilities are generic, but introducing domain-specific data enables training models proficient in specific problem domains relevant to particular applications or business needs.</p>\n<p>The training recipe demonstrates that supervised fine-tuning alone can instill strong reasoning capabilities, though the approach notes that reinforcement learning is also an option and recent work suggests multi-pass approaches (supervised fine-tuning followed by reinforcement learning) yield the best results. The curriculum learning strategy proved particularly important, with the ordered presentation of progressively harder samples significantly improving training stability and final performance. The emergence of consistent reasoning behavior after processing just 100,000 samples suggests that models can learn reasoning patterns relatively quickly once exposed to high-quality examples with explicit reasoning traces. This accessibility transforms reasoning model development from a capability limited to large research labs with massive compute budgets into something individual developers and smaller organizations can accomplish over a weekend, democratizing access to this transformative capability and enabling domain-specific reasoning applications across industries.</p>",
        "3": "<h2>System Overview and Architecture</h2>\n<p>This example demonstrates a multi-turn conversational AI system built on a retrieval-augmented generation pipeline that maintains conversation history and accesses a knowledge base through vector databases. The chain server is the core component that stores both the conversation history and the knowledge base in separate vector stores, retrieving them at runtime to understand contextual queries that reference previous parts of the conversation. This architecture enables the chatbot to handle follow-up questions and maintain coherent multi-turn dialogues where later questions depend on context established earlier in the conversation.</p>\n<p>The system uses a dual vector store design to separate concerns and optimize retrieval. The first vector store, called multi_turn_rag, is dedicated to storing ingested documents from the knowledge base. Users can upload PDF and text files that get processed and stored here. The second vector store, called conv_store, is dedicated to conversation history and stores each previously asked query along with the model's generated answer as text entries. Both vector stores are integrated into a LangChain LCEL chain as LangChain Retrievers, creating a unified pipeline. When a user submits a query, it passes through both retrievers simultaneously - one retriever fetches relevant context from the document knowledge base while the other retrieves the closest-matching conversation history entries. These retrieved chunks are then combined and injected into the language model prompt, giving the model both factual grounding from documents and conversational context from previous exchanges.</p>\n<p>The example uses the Llama 2 70B model accessed through NVIDIA API Catalog endpoints rather than requiring local model deployment, significantly simplifying infrastructure requirements. The system uses the nvolveqa_40k embedding model for converting text into vectors, leverages the LangChain framework for orchestrating the RAG pipeline, and uses Milvus as the vector database backend. The architecture does not require multiple GPUs or TensorRT-LLM optimization since inference happens through API calls. The sample includes a web application that communicates with the chain server, which in turn sends inference requests to the NVIDIA API Catalog endpoint. Optionally, the system can integrate with NVIDIA Riva for automatic speech recognition to transcribe spoken questions and text-to-speech to speak answers aloud, creating a voice-enabled conversational interface.</p>\n<h2>Setup and Deployment Process</h2>\n<p>Setting up the system involves several prerequisites and configuration steps. First, you need to clone the Generative AI examples Git repository using Git LFS to ensure all large files are properly downloaded. You'll need Docker Engine and Docker Compose installed to run the containerized components. If you want voice capabilities, you can optionally enable NVIDIA Riva by either launching a local Riva server following the Quick Start Guide with ASR and TTS services enabled for your desired languages, or by using a hosted Riva API endpoint with appropriate API keys and function IDs configured in the environment file.</p>\n<p>The most critical setup step is obtaining an API key for the Llama 2 70B model endpoint from NVIDIA's API Catalog. You navigate to the build.ngc.nvidia.com platform, find the Llama 2 70B model card, and generate an API key that begins with \"nvapi-\". This same API key can be used across different model endpoints in the catalog. Once you have the key, you edit the deploy/compose/compose.env file to add it as the NVIDIA_API_KEY environment variable. The deployment process then involves building the Docker containers using the provided compose file specifically for the multi-turn chatbot application, starting those containers which launches both the chain server and the RAG playground web interface, separately starting the Milvus vector database with its dependencies (MinIO for object storage and etcd for metadata), and finally confirming all containers are running properly.</p>\n<h2>Using the System</h2>\n<p>After deployment, users access the chat server through a web interface where they can interact with the conversational AI. The workflow begins by uploading PDF or text files to the knowledge base, which get processed and stored in the document vector store. When asking questions, users can enable the \"Use knowledge base\" checkbox to have the system retrieve relevant information from uploaded documents alongside conversation history. The system's prompt is specifically tuned to act as a document chatbot, meaning it's optimized for answering questions about the content in the knowledge base while maintaining conversational coherence across multiple turns. As the conversation progresses, each question-answer pair gets stored in the conversation history vector store, allowing the system to reference earlier parts of the dialogue when interpreting subsequent questions. This creates a natural conversational flow where users can ask follow-up questions, request clarifications, or build on previous topics without needing to repeat context, making the interaction feel more like chatting with a knowledgeable assistant than querying a static database.</p>"
      },
      "subtopicStudyGuides": {
        "0": "<p></p><p></p><h2>Understanding Prompt Engineering Fundamentals</h2><p></p><p></p><p></p><p></p><p>Prompt engineering is the practice of designing natural language inputs to guide LLMs toward desired outputs. Unlike traditional programming where you write explicit code, prompting involves crafting requests that leverage the model's training to perform specific tasks. A well-designed prompt typically includes clear instructions about what you want, relevant context or background information, the specific input data to process, the desired output format, and optionally some examples demonstrating the task. The key principles are clarity and specificity - vague prompts yield vague results, so instead of asking \"analyze this report,\" you'd specify exactly what metrics to extract and how to format them. You can also improve results by assigning the model a role or persona, like \"you are a senior financial analyst,\" which primes it to adopt that expertise level and communication style. Setting explicit constraints about what not to include is equally important. Prompt engineering is fundamentally empirical - you test variations, measure outcomes, and iteratively refine based on what works for your specific use case.</p><p></p><p></p><p></p><p></p><h2>Chain-of-Thought Prompting</h2><p></p><p></p><p></p><p></p><p>Chain-of-thought prompting encourages models to show their reasoning process step-by-step before reaching a conclusion, dramatically improving performance on tasks requiring multi-step reasoning, arithmetic, or complex decision-making. The technique works because it makes reasoning transparent and debuggable, reduces errors by decomposing complexity, and allows verification of intermediate steps. There are several approaches to implementing CoT. In manual or few-shot CoT, you provide examples that explicitly show the step-by-step reasoning process - for instance, when calculating compound annual growth rate, you'd show each calculation step before arriving at the final answer. Zero-shot CoT is even simpler - just adding \"Let's think step by step\" to your prompt often triggers systematic reasoning without needing examples. For more structured problems, you can force a specific reasoning framework by outlining exactly what factors to consider and in what order. Advanced patterns include self-consistency CoT where you have the model solve the same problem multiple different ways and then synthesize the results, or least-to-most prompting where you break complex problems into sequential subproblems. Chain-of-thought is particularly effective with larger models and becomes essential when dealing with quantitative analysis, logical deduction, or any task where the path to the answer matters as much as the answer itself.</p><p></p><p></p><p></p><p></p><h2>Few-Shot and Zero-Shot Learning Strategies</h2><p></p><p></p><p></p><p></p><p>Few-shot learning involves providing a small number of examples - typically one to ten - that demonstrate the task format and desired output style. This is crucial when working with small datasets or specialized domains where you can't afford to fine-tune a model. The structure follows a pattern of context and instructions, followed by several input-output example pairs, then your actual query. The quality and diversity of examples matters more than quantity - three carefully chosen examples covering different edge cases often outperform ten similar ones. Format consistency is critical across all examples, and the order matters too, with the most relevant examples placed closest to your actual query. For specialized domains like financial analysis, your examples should demonstrate not just the format but also the domain-specific reasoning and terminology you expect. Zero-shot learning relies purely on detailed instructions without examples, which works well when the task is straightforward, when you want to avoid biasing the model toward specific patterns, or when you simply don't have good examples yet. Successful zero-shot prompts compensate for the lack of examples with ultra-detailed instructions that specify exactly how to handle the input, what the output should contain, and what format to use. The choice between few-shot and zero-shot depends on task complexity, availability of good examples, and whether you need to establish specific patterns or conventions.</p><p></p><p></p><p></p><p></p><h2>Working with Small Datasets and Specialized Domains</h2><p></p><p></p><p></p><p></p><p>When you have limited training data but need specialized behavior, prompt engineering becomes your primary tool for achieving good results. One powerful technique is using prompts for data augmentation - you can ask the model to generate synthetic examples based on a few real ones, varying factors like tone, complexity, or scenario type. This helps expand small datasets for testing or training downstream systems. Prompts can also standardize data labeling by providing clear annotation schemas with a few examples, then having the model consistently label larger datasets. Template-based extraction is another key pattern where you create reusable templates that specify exactly what structured information to extract from unstructured text, including the output format (often JSON), required fields, handling rules for missing data, and domain-specific conventions. For domains with specialized knowledge not in the model's training data, retrieval-augmented generation becomes essential - you retrieve relevant documents from your knowledge base and inject them as context in the prompt, instructing the model to answer based only on the provided information and to cite sources.</p><p></p><p></p><p></p><p></p><p>Domain specialization requires deliberately building expertise into your prompts through several strategies. Knowledge injection involves providing explicit background about domain concepts, terminology, and rules before asking the model to perform the task. For example, when analyzing clinical trials, you'd explain what each trial phase means, what statistical thresholds matter, and what regulatory requirements apply. Creating domain-specific constraints is equally important - spelling out the rules and standards that apply in your field, like GAAP compliance rules for financial analysis or nomenclature standards for scientific writing. Building standardized vocabularies ensures consistent terminology across all outputs, which matters especially in technical fields where precision is critical. You can also construct multi-step reasoning pipelines where each prompt builds on the previous output - first extracting raw data, then calculating derived metrics, then generating insights, and finally synthesizing a report. This decomposition helps manage complexity and makes each step verifiable.</p><p></p><p></p><p></p><p></p><h2>Testing, Evaluation, and Practical Implementation</h2><p></p><p></p><p></p><p></p><p>Measuring prompt effectiveness requires both quantitative and qualitative assessment. Quantitatively, you track accuracy for classification or extraction tasks using standard metrics like precision and recall, measure consistency by running the same prompt multiple times to ensure similar outputs, monitor latency for user experience, and optimize token efficiency. Qualitatively, you evaluate whether outputs actually address the question, contain all required elements, present accurate facts and sound reasoning, and match the desired style and format. A/B testing different prompt variations on the same dataset helps identify what actually works better rather than relying on intuition. Common failure modes include inconsistent outputs, which you fix by adding more constraints or structured output formats; hallucinated facts, addressed by explicitly instructing the model to acknowledge uncertainty and by grounding responses with retrieved documents; format violations, solved with multiple format examples and clear separators; and length issues, handled by specifying word counts or showing examples of appropriate length.</p><p></p><p></p><p></p><p></p><p>Practical implementation means building reusable infrastructure around your prompts. Create a template library where each template has a clear purpose, specified inputs and outputs, the actual prompt text with variables, and documented evaluation criteria. Version your prompts like code, tracking what changed in each iteration and measuring the performance impact - you might see that adding few-shot examples boosted accuracy by 13 percent, then specifying JSON output added another 3 percent. Build error handling directly into prompts by instructing the model how to handle ambiguous data, missing information, contradictions, or uncertainty. The goal is creating robust, maintainable prompt systems that others can use and improve.</p><p></p><p></p>",
        "1": "<h2>Understanding Shot-Based Learning Paradigms</h2>\n<p>Zero-shot, one-shot, and few-shot learning represent different approaches to adapting pre-trained language models to new tasks without fine-tuning. These techniques are called \"shot-based\" learning because \"shot\" refers to the number of examples you provide - zero examples, one example, or a few examples. The fundamental idea is that large language models have absorbed so much knowledge during pre-training that they can often perform new tasks simply by being shown what you want, rather than requiring extensive task-specific training. This adaptability is one of the most powerful properties of modern LLMs and makes them dramatically more flexible than traditional machine learning models that need hundreds or thousands of labeled examples for each new task. Understanding when and how to use each approach is essential because the right choice depends on your specific constraints around data availability, task complexity, performance requirements, and computational resources.</p>\n<h2>Zero-Shot Learning: Adapting Without Examples</h2>\n<p>Zero-shot learning means asking the model to perform a task without providing any examples at all - you rely purely on clear, detailed instructions. The model uses only its pre-training knowledge to understand and execute the task. This works because during pre-training on massive text corpora, the model encountered countless variations of similar tasks and learned general patterns of language understanding, reasoning, and generation. For zero-shot to be effective, your instructions must be exceptionally clear and specific. Instead of just saying \"classify this email,\" you'd write \"Classify this customer email into one of three categories: billing inquiry, technical support, or feature request. Consider the main intent of the email and ignore secondary topics.\" The model's success depends entirely on how well you describe what you want and how similar your task is to patterns the model saw during training.</p>\n<p>Zero-shot learning is most appropriate when you have no labeled examples available yet, when the task is relatively straightforward and doesn't require specialized conventions, when you want to avoid biasing the model toward specific patterns that might not generalize, or when you're doing rapid prototyping and iteration. It's particularly effective for common tasks like summarization, basic classification, question answering, or translation - things the model has seen many variations of during training. The advantages are speed and simplicity - you can immediately start using the model without gathering examples, and you avoid the risk of overfitting to a small set of examples that might not represent the full task distribution. However, zero-shot has clear limitations. Performance is typically lower than few-shot approaches, especially for specialized domains or tasks with specific formatting requirements. The model might not understand subtle distinctions or domain-specific conventions without examples to demonstrate them.</p>\n<h2>One-Shot Learning: The Single Example Approach</h2>\n<p>One-shot learning provides exactly one example of the task before asking the model to perform it on new inputs. This single example serves as a template that shows the model exactly what format and style you expect. The example includes both the input and the desired output, making it crystal clear what transformation or analysis you want. One-shot learning occupies an interesting middle ground - it's more guidance than zero-shot but doesn't consume as many tokens as few-shot learning with multiple examples. The single example often dramatically improves performance over zero-shot, especially for tasks where format or style matters significantly.</p>\n<p>The key to effective one-shot learning is choosing the right example. It should be representative of typical inputs the model will encounter, demonstrate any non-obvious formatting or structural requirements, show the appropriate level of detail and tone, and ideally be somewhat complex to showcase how to handle nuance. For instance, if you're extracting financial metrics from earnings reports, your one example should show a realistic report excerpt with multiple metrics present and demonstrate exactly how to format the extracted data. One-shot works well when you have very limited data - maybe you only have one labeled example so far, when task requirements are moderately complex but can be captured in a single good example, when you're trying to minimize prompt length for cost or latency reasons, or when there's a clear canonical format you can demonstrate once.</p>\n<p>The advantage of one-shot over zero-shot is substantial improvement in format consistency and understanding of task requirements, while the advantage over few-shot is efficiency - fewer tokens used means lower cost and faster processing. However, one-shot has risks. A single example might not represent the full range of inputs or edge cases, and the model might overfit to specific details of that one example that aren't actually requirements. If your example happens to have certain characteristics that aren't universal, the model might incorrectly assume those are requirements. That's why choosing a carefully representative example matters so much.</p>\n<h2>Few-Shot Learning: Learning from Multiple Examples</h2>\n<p>Few-shot learning provides multiple examples - typically between two and ten - that demonstrate the task from different angles. Each example shows an input-output pair, and together they paint a more complete picture of what you want than any single example could. The model learns the pattern by observing consistency across examples and variation within acceptable ranges. Few-shot learning is the most powerful of these three approaches for complex or specialized tasks because multiple examples can show edge cases, demonstrate appropriate handling of different input types, establish consistent formatting conventions, and implicitly communicate subtle requirements that would be hard to describe in instructions alone.</p>\n<p>The number of examples to use depends on several factors. More examples generally improve performance but also increase token usage and cost, potentially reaching the point of diminishing returns. Task complexity matters - simple classification might work with two or three examples, while specialized extraction or analysis might benefit from five to ten. Input diversity is crucial - if your actual inputs vary widely, you need examples covering that range. The complexity of the desired output also plays a role - generating structured data with many fields requires more examples to show all the patterns. Research suggests that for many tasks, performance improves rapidly from zero to three examples, continues improving through five to seven examples, and then plateaus or improves only marginally beyond that.</p>\n<p>Constructing effective few-shot examples requires strategic thinking. Your examples should be diverse, covering different scenarios, edge cases, and input variations rather than being similar to each other. They should be representative of your actual use case, not cherry-picked easy cases. Quality matters more than quantity - three excellent examples typically outperform seven mediocre ones. Format consistency across examples is critical because the model learns what's required versus what's variable by observing what stays consistent. The order of examples can matter too, with more relevant or complex examples often placed closer to the actual query. Including examples of both common cases and edge cases helps the model learn boundaries. For specialized domains, examples should demonstrate domain-specific terminology, reasoning patterns, and conventions.</p>\n<h2>Comparing and Choosing Between Approaches</h2>\n<p>The decision between zero-shot, one-shot, and few-shot depends on multiple factors that you need to weigh against each other. Data availability is the most obvious constraint - if you have zero labeled examples, zero-shot is your only option, but as soon as you have even one good example, using it will likely improve results. Task complexity and specialization matter significantly. Simple, common tasks like basic summarization often work fine with zero-shot, while specialized tasks with specific formatting or domain requirements usually need few-shot. Performance requirements play a role too - if you need the highest possible accuracy and have examples available, few-shot is typically the best choice.</p>\n<p>Cost and latency considerations can't be ignored in production systems. Zero-shot uses the fewest tokens, making it cheapest and fastest. Few-shot can use significantly more tokens, especially with lengthy examples, impacting both cost per request and response time. If you're processing millions of requests, these differences compound. Domain specificity is another key factor. For well-known domains where the model has strong pre-training knowledge, zero-shot often suffices. For niche domains with specialized vocabulary, conventions, or reasoning patterns, few-shot becomes nearly essential. The need for format control also influences the decision - if exact formatting matters, at least one-shot is usually necessary to demonstrate it.</p>\n<p>There's also the concept of emergent few-shot abilities in larger models. Research shows that as models scale up in size, their few-shot learning capabilities improve more rapidly than their zero-shot abilities. This means that for cutting-edge large models, few-shot learning unlocks capabilities that simply don't emerge in zero-shot settings. Smaller models might show less dramatic differences between zero-shot and few-shot performance. Understanding your model's size and capabilities helps inform which approach will be most effective.</p>\n<h2>Practical Implementation Strategies</h2>\n<p>In practice, you often start with zero-shot as a baseline to understand what the model can do without any guidance, then progressively add examples to measure improvement. This experimental approach helps you find the minimum number of examples needed for acceptable performance. You might discover that zero-shot works fine and save yourself the effort of example curation, or you might find that three examples get you to target performance and additional examples don't help enough to justify the token cost. Building a library of high-quality examples for each task pays dividends because you can reuse and refine them over time.</p>\n<p>Dynamic example selection is an advanced technique where you programmatically choose which examples to include based on the specific input. For instance, if you're classifying technical documents and the current document is about databases, you'd include examples related to databases rather than generic examples. This ensures the examples are maximally relevant to each query. You can implement this with simple keyword matching or more sophisticated semantic similarity search using embeddings. Another strategy is example chaining where early examples are simpler and later examples are more complex, essentially teaching the model progressively. This can be particularly effective for complex reasoning tasks.</p>\n<p>Hybrid approaches combine these techniques strategically. You might use zero-shot for simple, high-volume queries to minimize cost, but automatically switch to few-shot when the input matches certain complexity criteria or when zero-shot confidence is low. You can also use few-shot learning to generate synthetic training data - providing a few examples and asking the model to generate hundreds more variations, which you then filter and use for other purposes. Some systems use adaptive few-shot where the number of examples adjusts based on task difficulty or past performance metrics.</p>\n<h2>Expanding Model Adaptability Through Shot-Based Learning</h2>\n<p>The true power of shot-based learning is that it makes a single pre-trained model adaptable to virtually unlimited tasks without any fine-tuning or retraining. This is a fundamental shift from traditional machine learning where each new task required collecting labeled data and training a task-specific model. With few-shot learning, you can deploy one LLM and adapt it to hundreds of different tasks simply by changing the prompt and examples. This dramatically reduces the overhead of building AI systems and allows for rapid experimentation and iteration.</p>\n<p>Shot-based learning also enables personalization and customization at scale. Different users or customers might need slightly different behaviors from the same underlying model - different classification schemes, different output formats, different domain focuses. With few-shot learning, you can maintain one model but provide each user with their own task-specific examples, essentially creating customized behavior without training separate models. This makes it economically feasible to support high degrees of personalization that would be prohibitively expensive with traditional approaches.</p>\n<p>These techniques also facilitate rapid deployment to new domains. When a new use case emerges, you don't need to collect thousands of examples and retrain - you can often get reasonable performance with just a handful of examples. This acceleration of the development cycle means you can explore many more potential applications and respond to new requirements much faster. The learning curve for non-technical users is also much gentler - providing examples is intuitive in a way that training models is not, democratizing access to AI customization.</p>\n<h2>Limitations and Considerations</h2>\n<p>Despite their power, shot-based learning techniques have important limitations to understand. They're fundamentally limited by what the model learned during pre-training - if a task requires knowledge or capabilities the model simply doesn't have, no amount of examples will create them. Few-shot learning can guide how the model applies its knowledge but can't inject entirely new knowledge. For tasks requiring substantial new knowledge or capabilities, fine-tuning or retrieval-augmented generation becomes necessary.</p>\n<p>Performance typically remains below what's achievable with fine-tuning on large task-specific datasets. If you have thousands of labeled examples and performance is critical, fine-tuning will usually outperform few-shot learning. Few-shot is best viewed as a tool for rapid adaptation and deployment, not necessarily for achieving absolute maximum performance. There's also the risk of example bias where the model picks up on spurious patterns in your examples that aren't representative of the true task. This is why example quality and diversity matter so much.</p>\n<p>Token costs can become significant, especially with few-shot learning using many or lengthy examples. Each request includes all the examples in the prompt, so these tokens are processed repeatedly. For high-volume applications, this can drive costs up substantially compared to fine-tuned models that don't require examples in each prompt. There's a trade-off between the convenience and flexibility of few-shot learning and the efficiency of fine-tuning for stable, high-volume tasks.</p>",
        "2": "<h2>Understanding Decoder-Based Architecture</h2>\n<p>Decoder-based language models represent one of the fundamental architectures in modern LLMs, with GPT (Generative Pre-trained Transformer) being the most famous example. Unlike encoder-only models like BERT that are designed to understand and encode text, or encoder-decoder models like T5 designed for sequence-to-sequence tasks, decoder-based models are specifically architected for text generation. The \"decoder\" terminology comes from the original Transformer paper where the decoder component was responsible for generating output sequences. In a decoder-only architecture, the model consists solely of decoder blocks stacked on top of each other, with each block containing a masked self-attention mechanism and a feed-forward network. The key distinguishing feature is the causal masking in the attention mechanism, which prevents the model from seeing future tokens when processing or generating text. This architectural choice makes decoder models naturally suited for autoregressive generation where each token is predicted based only on the tokens that came before it.</p>\n<p>The attention mechanism in decoder models uses what's called causal or masked self-attention. When processing a sequence of tokens, each position can only attend to itself and previous positions, never to future positions. This is implemented through an attention mask that sets the attention weights to negative infinity for future positions before the softmax operation, effectively zeroing out their influence. This causal constraint is crucial because it aligns the training objective with the inference behavior - during both training and generation, predictions are made autoregressively from left to right. The model learns to predict each token based solely on the context that would actually be available during generation. Popular decoder-based models include the GPT family from OpenAI, LLaMA from Meta, PaLM from Google, and Claude from Anthropic, among many others. These models have demonstrated remarkable capabilities in text generation, reasoning, coding, and general-purpose language understanding.</p>\n<h2>Causal Language Modeling Objective</h2>\n<p>Causal language modeling, also called next-token prediction or autoregressive language modeling, is the training objective used for decoder-based LLMs. The fundamental task is simple: given a sequence of tokens, predict the next token. During training, you take a text sequence, feed tokens one by one into the model, and at each position the model tries to predict what token comes next. The model receives immediate feedback about whether its prediction was correct because you already know the actual next token from your training data. This process repeats across billions or trillions of tokens from diverse text sources, and through this repetitive prediction task, the model learns incredibly rich representations of language, knowledge, reasoning patterns, and more.</p>\n<p>The mathematical formulation of causal language modeling is straightforward. Given a sequence of tokens x = (x₁, x₂, ..., xₙ), the model learns to maximize the probability of each token given all previous tokens: P(xᵢ | x₁, x₂, ..., xᵢ₋₁). The overall objective is to maximize the product of these conditional probabilities across the entire sequence, or equivalently, to minimize the negative log-likelihood. In practice, you compute this as a cross-entropy loss between the model's predicted probability distribution over the vocabulary and the actual next token at each position. The loss is averaged across all positions in the sequence and across all sequences in a batch. This simple objective, when applied at massive scale with enormous models and datasets, produces the sophisticated capabilities we see in modern LLMs.</p>\n<p>What makes causal language modeling so powerful is that it's a self-supervised learning task - you don't need human annotations or labels. Any text can serve as training data because the \"label\" for each prediction is simply the next token that actually appears in the text. This allows training on virtually unlimited amounts of text scraped from the internet, books, code repositories, and other sources. The model implicitly learns grammar, facts, reasoning patterns, common sense, specialized domain knowledge, and countless other aspects of language and the world simply by learning to predict what comes next. This emergent learning is why pre-training with causal language modeling has become the foundation for nearly all modern generative language models.</p>\n<h2>Data Preparation and Tokenization</h2>\n<p>Training a decoder-based LLM begins with assembling and preparing massive text datasets. Pre-training datasets typically contain hundreds of gigabytes to multiple terabytes of text from diverse sources including web pages, books, academic papers, code repositories, social media, news articles, and more. The composition of this data significantly impacts what the model learns - more code makes it better at programming, more scientific papers improve technical knowledge, more conversational text improves dialogue abilities. Curating high-quality training data involves filtering out low-quality content, removing duplicate documents, handling multiple languages appropriately, and balancing different domains and sources. Data quality matters enormously because the model learns from everything it sees, including biases, errors, and problematic content in the training data.</p>\n<p>Before text can be fed into the model, it must be converted into tokens using a tokenizer. Tokenization breaks text into smaller units that serve as the atomic elements the model processes. Modern LLMs typically use subword tokenization methods like Byte-Pair Encoding (BPE) or SentencePiece that balance between character-level and word-level tokenization. These methods split text into common subwords and characters, allowing the model to handle any possible text while keeping the vocabulary size manageable - typically 32,000 to 100,000 tokens. A token is roughly equivalent to four English characters on average, though this varies by language and content type. The tokenizer is trained on a large sample of your training data before model training begins, learning which subword units appear frequently and should be included in the vocabulary. Once trained, the tokenizer converts all training text into sequences of integer token IDs that the model can process.</p>\n<p>The tokenized text is organized into training examples of fixed sequence length, typically 2,048, 4,096, or 8,192 tokens depending on the model's maximum context window. Longer sequences allow the model to learn longer-range dependencies but require more memory and computation. Documents are concatenated together and split into these fixed-length chunks, with special tokens marking document boundaries when needed. These sequences are then batched together for efficient parallel processing on GPUs. Data preprocessing also involves shuffling the order of sequences to ensure the model sees diverse content rather than processing all documents from one source consecutively. Some training procedures use curriculum learning where easier or shorter sequences are presented earlier in training before progressing to more complex examples.</p>\n<h2>The Training Process</h2>\n<p>Training a decoder-based LLM follows the standard deep learning loop but at massive scale. You initialize the model parameters randomly or from a smaller pre-trained model, then repeatedly process batches of training sequences, compute predictions, calculate loss, and update parameters through backpropagation. For each batch, the model receives sequences of token IDs, processes them through the stacked decoder layers to produce predictions for the next token at each position, compares these predictions to the actual next tokens using cross-entropy loss, computes gradients through backpropagation, and updates all model parameters using an optimizer like AdamW. This cycle repeats millions of times over weeks or months of continuous training on clusters of GPUs or TPUs.</p>\n<p>The forward pass through the model works as follows: token IDs are converted to embedding vectors, positional encodings are added to give the model information about token positions, these embeddings flow through multiple decoder blocks where each block applies masked self-attention to let tokens attend to previous context followed by a feed-forward network to process the attended information, and finally the output goes through a linear projection and softmax to produce probability distributions over the vocabulary for predicting the next token at each position. During training, you process entire sequences in parallel - even though the predictions are autoregressive, you can compute the loss for all positions simultaneously because you know all the actual next tokens. This parallel processing during training is much more efficient than the sequential generation required during inference.</p>\n<p>The loss calculation focuses on how well the model predicts each next token. At each position in the sequence, the model outputs a probability distribution over the entire vocabulary. The cross-entropy loss measures how much probability the model assigned to the actual next token - when the model assigns high probability to the correct token, the loss is low, and when it assigns low probability, the loss is high. The gradient of this loss with respect to all model parameters is computed through backpropagation, flowing backwards through all the layers. These gradients indicate how to adjust each parameter to reduce the loss, and the optimizer uses them to update the parameters by a small amount in the direction that should improve predictions. The learning rate controls how large these updates are - typically starting larger and decreasing over time through a learning rate schedule.</p>\n<h2>Training Infrastructure and Scaling</h2>\n<p>Training large language models requires specialized infrastructure and techniques to handle the computational demands. Pre-training models with billions or hundreds of billions of parameters requires distributing the computation across hundreds or thousands of GPUs. This involves several forms of parallelism working together. Data parallelism replicates the model across multiple devices, with each device processing different batches of data and gradients being synchronized periodically. Model parallelism splits the model itself across devices because it's too large to fit on a single GPU - this includes tensor parallelism that splits individual layers horizontally and pipeline parallelism that splits the model vertically across layers. Efficient training requires carefully orchestrating these parallel strategies to maximize GPU utilization while minimizing communication overhead between devices.</p>\n<p>Memory management is a critical challenge in training large models. Beyond the model parameters themselves, training requires storing activations from the forward pass for use in backpropagation, gradients for all parameters, and optimizer states like momentum buffers that can be even larger than the model. Techniques like gradient checkpointing trade computation for memory by recomputing some activations during the backward pass instead of storing them. Mixed-precision training uses 16-bit floating-point for most calculations to reduce memory and speed up computation, while maintaining 32-bit precision for critical operations to preserve training stability. Gradient accumulation allows training with effective batch sizes larger than what fits in memory by processing multiple micro-batches and accumulating their gradients before doing a parameter update.</p>\n<p>Training stability is crucial because training runs can last weeks or months and failures are costly. Models can diverge or collapse if learning rates are too high or if numerical instabilities occur. Techniques for maintaining stability include careful learning rate scheduling that warms up gradually at the start, gradient clipping to prevent extremely large updates, layer normalization within the model architecture to keep activations in reasonable ranges, and careful initialization of parameters. Monitoring training metrics like loss curves, gradient norms, and activation statistics helps identify problems early. Checkpointing saves model state periodically so training can resume from the last checkpoint if failures occur. Training large models is an engineering challenge as much as a machine learning one, requiring expertise in distributed systems, hardware optimization, and debugging at scale.</p>\n<h2>Pre-Training vs Fine-Tuning</h2>\n<p>Pre-training and fine-tuning represent two distinct phases in the life of a decoder-based LLM. Pre-training is the initial phase where the model learns from massive amounts of diverse text using the causal language modeling objective. This phase is extremely expensive, potentially costing millions of dollars in compute for state-of-the-art models, and takes weeks or months on large GPU clusters. Pre-training produces a foundation model with broad capabilities and knowledge but without specific task optimization or behavioral alignment. The pre-trained model has learned the patterns and structure of language, accumulated factual knowledge from its training data, and developed reasoning capabilities, but it simply predicts the next token without understanding helpfulness, safety, or user intent.</p>\n<p>Fine-tuning takes a pre-trained model and continues training it on a smaller, more focused dataset to specialize it for specific tasks or align it with desired behaviors. Supervised fine-tuning uses curated datasets of high-quality examples showing desired input-output pairs - for chatbots, this might be conversations demonstrating helpful, harmless, and honest responses. Instruction tuning is a form of fine-tuning that trains models to follow natural language instructions across diverse tasks, using datasets with instructions paired with appropriate responses. This teaches the model to behave as a helpful assistant rather than just completing text. Fine-tuning typically uses much smaller datasets than pre-training - thousands to hundreds of thousands of examples rather than billions - and much lower learning rates to avoid catastrophic forgetting where the model loses its pre-trained knowledge.</p>\n<p>Reinforcement Learning from Human Feedback (RLHF) is an advanced fine-tuning technique that has become crucial for modern conversational LLMs. After supervised fine-tuning, human evaluators rank or rate different model outputs for the same prompt, and this preference data is used to train a reward model that predicts what humans prefer. The LLM is then further trained using reinforcement learning to maximize this reward signal, essentially optimizing for generating outputs humans rate highly. This process helps align models with human values and preferences in ways that are difficult to capture with supervised learning alone. RLHF is computationally intensive because it involves training two models and using reinforcement learning, but it significantly improves output quality, safety, and alignment.</p>\n<h2>When and Why to Train Decoder-Based LLMs</h2>\n<p>The decision to train a decoder-based LLM from scratch versus using existing models is significant and depends on multiple factors. Training from scratch makes sense when you need complete control over the model architecture and training data, when you're working in a specialized domain with proprietary data that provides competitive advantage, when you have unique requirements that existing models don't meet, or when you're conducting research into new architectures or training methods. Organizations with massive computational resources and unique data assets might train proprietary models to differentiate their products. Research institutions train models to advance the field's understanding of how these systems work and how to improve them.</p>\n<p>However, training from scratch is rarely the right choice for most organizations and use cases. The computational costs are prohibitive - training a competitive large language model can cost millions of dollars in compute resources. The expertise required spans machine learning, distributed systems, data engineering, and more. The time investment is substantial, with training runs lasting weeks or months. For most practical applications, starting with an existing pre-trained model and fine-tuning it for your specific needs is far more efficient and cost-effective. Many high-quality open-source models are available from organizations like Meta, Mistral, and others that provide excellent starting points.</p>\n<p>Fine-tuning existing models is appropriate when you need to specialize behavior for a specific domain or task, when you want to adapt the model's style or formatting to your needs, when you need to align the model with your organization's values and guidelines, or when you want to improve performance on tasks where the base model is weak. Fine-tuning requires much less compute - often doable on a single GPU or small cluster - and much less data than pre-training. It allows you to leverage the broad knowledge and capabilities learned during pre-training while adding the specific behaviors or knowledge you need. Techniques like parameter-efficient fine-tuning (PEFT) methods such as LoRA make fine-tuning even more accessible by updating only a small fraction of parameters.</p>\n<h2>Practical Training Considerations</h2>\n<p>Several practical considerations affect training outcomes and efficiency. The model size must balance capability with computational constraints - larger models are more capable but exponentially more expensive to train and deploy. Common sizes range from 7 billion to 70 billion parameters for models that can be practically deployed, with some frontier models reaching hundreds of billions. Scaling laws help predict how performance improves with model size, dataset size, and compute budget, guiding decisions about resource allocation. These laws suggest that optimal performance comes from scaling model and data together rather than just making models larger.</p>\n<p>Hyperparameters significantly impact training success. Learning rate is perhaps most critical - too high causes instability or divergence, too low results in slow learning or getting stuck. Learning rate schedules typically warm up gradually, maintain a constant rate, then decay as training progresses. Batch size affects both convergence speed and final performance, with larger batches being more computationally efficient but potentially leading to different optima. Sequence length determines how much context the model can consider - longer is better for learning long-range dependencies but requires more memory. The number of training steps or tokens determines how much data the model sees - contemporary models are often trained on hundreds of billions to trillions of tokens.</p>\n<p>Evaluation during training helps assess progress and detect problems. Perplexity measures how surprised the model is by test data - lower is better and indicates better predictive performance. However, perplexity doesn't capture all aspects of model quality. Downstream task evaluations assess performance on specific benchmarks for reading comprehension, reasoning, knowledge, and other capabilities. These evaluations help determine when to stop training and which checkpoint achieves the best balance of capabilities. Comparing performance across different domains helps identify whether the model is learning broadly or overfitting to certain types of content. Monitoring these metrics throughout training informs decisions about hyperparameter adjustments or whether to continue training.</p>\n<h2>Key Takeaways for Exam Preparation</h2>\n<p>The essential concepts to master are understanding that decoder-based LLMs use causal or masked self-attention to enable autoregressive generation, knowing that causal language modeling trains models to predict the next token given previous tokens through a simple self-supervised objective that scales to massive datasets, and recognizing that this training objective produces broad capabilities including language understanding, knowledge, and reasoning without task-specific supervision. You should understand the distinction between pre-training on massive diverse data to build foundation models versus fine-tuning on smaller focused datasets to specialize or align behavior. Know the key components of the training process including tokenization, batching, forward pass through stacked decoder blocks, loss calculation with cross-entropy, backpropagation, and parameter updates.</p>\n<p>Be familiar with the infrastructure challenges including distributed training across many GPUs using data and model parallelism, memory management techniques like mixed-precision training and gradient checkpointing, and stability considerations like learning rate scheduling and gradient clipping. Understand when training from scratch versus fine-tuning makes sense based on resources, data, and requirements. Recognize that causal language modeling's simplicity and self-supervised nature make it remarkably powerful for learning from unlabeled text at scale. The bottom line is that decoder-based LLMs trained with causal language modeling have become the dominant paradigm for generative AI because the architecture naturally supports generation, the training objective learns from unlimited text without labels, and the resulting models demonstrate emergent capabilities across countless tasks. Understanding how to train these models, even if you never train one from scratch, is fundamental to working effectively with modern LLMs.</p>",
        "3": "<p></p><p></p><h2>Understanding LLM-Wrapping Modules</h2><p></p><p></p><p></p><p></p><p>LLM-wrapping modules are structured software layers built around language models to add control, validation, and constraints that raw LLM outputs don't provide. While LLMs are incredibly powerful at generating fluent text, they're fundamentally probabilistic systems that can produce inconsistent formats, hallucinate facts, violate constraints, or generate outputs that don't meet application requirements. A wrapping module sits between the user and the LLM, intercepting inputs to add context or validation, and processing outputs to enforce structure, verify correctness, or handle errors. This architectural pattern transforms unreliable generative models into dependable system components that can be integrated into production applications with confidence. The wrapper essentially creates a contract - guaranteeing that outputs meet certain specifications regardless of the underlying model's probabilistic nature.</p><p></p><p></p><p></p><p></p><p>The need for wrapping modules emerges from the gap between what LLMs naturally produce and what applications require. LLMs generate free-form text based on learned patterns and probabilities, but real applications need structured data in specific formats, factual accuracy verified against sources, outputs that respect business rules and constraints, consistent behavior across similar inputs, and graceful error handling when the model can't perform as expected. Without wrapping modules, you're left with the raw output of a language model that might be formatted differently each time, contain made-up information presented as fact, violate domain constraints you care about, or simply fail in unpredictable ways. The wrapper provides the engineering discipline and structure needed to deploy LLMs reliably in production systems where consistency and correctness matter.</p><p></p><p></p><p></p><p></p><h2>Input Validation and Preprocessing</h2><p></p><p></p><p></p><p></p><p>The first line of defense in an LLM wrapper is input validation, which ensures that user inputs meet basic requirements before reaching the model. Input validation catches malformed requests, excessively long inputs that would exceed context limits, potentially harmful content that shouldn't be processed, and inputs missing required information. This preprocessing stage can reject invalid requests early with clear error messages rather than wasting compute on processing them and potentially getting unhelpful outputs. Input validation might check that required fields are present, that text lengths fall within acceptable ranges, that file uploads are in supported formats, and that inputs don't contain patterns associated with prompt injection attacks or other security concerns.</p><p></p><p></p><p></p><p></p><p>Beyond basic validation, input preprocessing can enrich or transform inputs to improve model performance. This includes normalizing text by fixing encoding issues or standardizing formats, adding relevant context from knowledge bases or user history, restructuring inputs to match templates the model handles well, and injecting domain-specific instructions or constraints. For example, if a user asks a question about a company, the wrapper might retrieve relevant documents from a database and inject them as context before the question reaches the model. If the user's input is ambiguous, the wrapper might add clarifying information based on the conversation history or user profile. This preprocessing stage makes the LLM's job easier by providing well-structured, context-rich inputs rather than raw user text.</p><p></p><p></p><p></p><p></p><p>Input validation also serves a security function by defending against adversarial inputs designed to manipulate model behavior. Prompt injection attacks try to override system instructions by embedding adversarial instructions in user inputs, potentially causing the model to ignore its guidelines or leak sensitive information. The wrapper can implement defenses like separating user input from system instructions using clear delimiters or special tokens, scanning inputs for patterns associated with injection attempts, limiting the length or complexity of user inputs to reduce attack surface, and applying input sanitization to escape or remove potentially dangerous content. While no defense is perfect, careful input validation significantly raises the bar for attacks and protects against common exploitation attempts.</p><p></p><p></p><p></p><p></p><h2>Output Validation and Post-Processing</h2><p></p><p></p><p></p><p></p><p>Output validation examines model-generated responses to ensure they meet application requirements before presenting them to users. This is critical because LLMs can generate outputs that look plausible but violate important constraints. Output validation checks might verify that the output follows the required format or schema, contains all required fields and information, doesn't include disallowed content or patterns, stays within length or complexity bounds, and maintains consistency with previous interactions or known facts. When validation fails, the wrapper can retry with modified prompts, fall back to alternative approaches, or return error messages instead of showing users invalid outputs. This validation layer transforms unreliable model outputs into dependable application responses.</p><p></p><p></p><p></p><p></p><p>Structured output validation is particularly important when the model should return data in specific formats like JSON, XML, or CSV. Even with careful prompting, LLMs sometimes generate malformed structures - missing closing braces, incorrect nesting, invalid field names, or extraneous text outside the structure. The wrapper can parse the output and validate it against a schema, checking that all required fields are present with correct types, that values fall within acceptable ranges, that the structure is properly formed, and that no unexpected fields appear. When validation fails, the wrapper might attempt to repair minor issues automatically, extract valid portions from partially correct outputs, or regenerate with more explicit formatting instructions. Schema validation tools like JSON Schema or Pydantic models make this systematic and maintainable.</p><p></p><p></p><p></p><p></p><p>Factual validation addresses the hallucination problem where models confidently state incorrect information. While perfect fact-checking is impossible, wrappers can implement various verification strategies. Citation verification checks that the model's claims can be traced to provided source documents, using retrieval systems to find supporting evidence and flagging claims without support. Consistency checking compares the model's output against known facts from databases or knowledge graphs, identifying contradictions. Cross-verification generates the same answer multiple times and checks for consistency, since hallucinations often vary across samples while correct answers are stable. Confidence calibration has the model express uncertainty and sets thresholds for when to show versus suppress low-confidence outputs. External API verification calls trusted external services to check specific facts like dates, calculations, or current information. These techniques don't eliminate hallucinations but significantly reduce their impact by catching them before they reach users.</p><p></p><p></p><p></p><p></p><h2>Constrained Decoding Techniques</h2><p></p><p></p><p></p><p></p><p>Constrained decoding modifies the generation process itself to enforce requirements, preventing invalid outputs rather than detecting them after generation. This is more efficient and reliable than generate-then-validate approaches because it guarantees outputs meet constraints rather than hoping they do and retrying when they don't. Constrained decoding works by manipulating the probability distributions the model uses to select tokens during generation, either by setting probabilities of invalid tokens to zero so they can't be selected, or by biasing probabilities toward valid tokens so they're more likely. This maintains the model's language generation capabilities while steering it toward valid outputs.</p><p></p><p></p><p></p><p></p><p>Format-constrained decoding enforces structural requirements like JSON or XML during generation. As the model generates tokens, the wrapper tracks the current parse state and determines what tokens would be valid next based on the format grammar. For example, if generating JSON and the model just output an opening brace, the wrapper knows the next valid tokens are quote marks for a key or a closing brace for an empty object, so it masks all other tokens. Grammar-based constrained decoding uses formal grammars defining valid structures and ensures the model only generates token sequences that conform to the grammar. This guarantees syntactically correct outputs without post-processing. Libraries like Outlines and Guidance provide tools for implementing grammar-based constrained decoding, making it accessible without implementing low-level token manipulation.</p><p></p><p></p><p></p><p></p><p>Value-constrained decoding enforces semantic requirements beyond pure syntax. This includes restricting numeric values to valid ranges, ensuring generated entities exist in databases or knowledge bases, limiting outputs to whitelisted values for categorical fields, or preventing generation of certain disallowed terms or phrases. For instance, if generating a medical dosage, the wrapper can constrain the numeric value to medically safe ranges and ensure the unit is valid for that medication. If generating a product recommendation, it can restrict options to items actually in inventory. These semantic constraints require domain knowledge encoded in the wrapper, creating a bridge between the model's general capabilities and domain-specific requirements.</p><p></p><p></p><p></p><p></p><p>Length and structure constraints control the shape and size of outputs. This includes enforcing maximum token or word counts to keep responses concise, requiring minimum lengths to ensure completeness, controlling sentence or paragraph structure for readability, and enforcing specific templates or patterns. For example, generating a product description might require exactly three bullet points, each between ten and twenty words. The wrapper can track progress during generation and influence token selection to meet these structural requirements. Combining format, value, and structure constraints creates powerful control over model outputs while preserving naturalness and fluency.</p><p></p><p></p><p></p><p></p><h2>Reducing Hallucinations Through System Design</h2><p></p><p></p><p></p><p></p><p>Beyond post-hoc validation, wrapper design choices can fundamentally reduce hallucination rates. Retrieval-augmented generation is perhaps the most powerful technique, where the wrapper retrieves relevant documents from trusted sources before generation and injects them as context. This grounds the model's responses in factual material, reducing reliance on potentially incorrect memorized information. The model is instructed to answer based only on the provided context and to acknowledge when information isn't present rather than guessing. This architectural pattern dramatically reduces hallucinations for factual questions by replacing parametric knowledge (what the model memorized) with non-parametric knowledge (what's explicitly provided). The wrapper handles the retrieval, ranking, and context injection, making this transparent to the underlying model.</p><p></p><p></p><p></p><p></p><p>Decomposing complex questions into simpler sub-questions reduces hallucination by limiting how much the model must reason about simultaneously. The wrapper breaks down questions like \"Compare the revenue growth and market positioning of these three companies over five years\" into atomic queries for each company and metric, processes them separately, and synthesizes results. Each individual query is simpler and less likely to trigger hallucinations than the complex multi-part question. The wrapper maintains state across sub-questions, handles dependencies where later questions need earlier answers, and assembles the final response. This decomposition also makes it easier to validate individual pieces before combining them.</p><p></p><p></p><p></p><p></p><p>Tool-augmented generation gives models access to external tools for specific capabilities where hallucination is unacceptable. Instead of asking the model to perform calculations, the wrapper detects when math is needed and routes it to a calculator. Instead of asking for current information, the wrapper can call web search or database APIs. Instead of generating code execution results, the wrapper can actually run the code. The model's role becomes orchestrating tool use and interpreting results rather than attempting tasks where it's unreliable. The wrapper implements the tool interfaces, decides when to invoke tools versus letting the model answer directly, and combines tool results with model-generated explanations. This hybrid architecture leverages the model's language understanding and reasoning while delegating tasks requiring perfect accuracy to specialized systems.</p><p></p><p></p><p></p><p></p><p>Confidence scoring and selective abstention recognize that models shouldn't always try to answer. The wrapper can prompt the model to express uncertainty alongside answers, evaluate confidence based on factors like consistency across samples or length of reasoning chains, and set thresholds for when to show answers versus admitting uncertainty. Rather than hallucinating when unsure, the model can say \"I don't have enough information\" or \"I'm not confident about this.\" The wrapper might then route uncertain queries to fallback mechanisms like human review, alternative data sources, or more capable models. This selective abstention, while seemingly reducing capability, actually improves user trust by avoiding confidently stated falsehoods.</p><p></p><p></p><p></p><p></p><h2>Improving Consistency Through Wrappers</h2><p></p><p></p><p></p><p></p><p>Consistency problems manifest in several ways with raw LLMs - formatting varies across responses, terminology shifts unpredictably, tone fluctuates, and similar questions get different answers. Wrappers address these through several mechanisms. Template-based generation uses fixed templates for response structure while allowing the model to fill in variable content. For example, product descriptions might always include sections for features, benefits, and specifications in that order, with the model generating appropriate content for each. The wrapper enforces the template structure, guaranteeing consistent organization even as content varies. Templates can be hierarchical, with high-level structure enforced rigidly and lower-level details generated flexibly.</p><p></p><p></p><p></p><p></p><p>Response caching and retrieval creates consistency by recognizing when new inputs are similar to previous ones and reusing or adapting previous outputs. The wrapper maintains a cache of input-output pairs and uses semantic similarity to find relevant previous responses. For inputs sufficiently similar to cached examples, it can return the cached response directly or use it as a strong hint for generating the new response. This ensures that repeated or similar questions receive consistent answers rather than varying based on sampling randomness. The cache also improves latency and reduces costs by avoiding redundant model calls. Careful cache management is needed to avoid serving stale responses when information updates or to handle requests that superficially resemble but differ meaningfully from cached examples.</p><p></p><p></p><p></p><p></p><p>State management across multi-turn interactions maintains consistency over conversations. The wrapper tracks conversation history, user preferences, established facts, and commitments made in earlier turns. When generating new responses, it injects relevant state to ensure consistency with previous messages. If the model called the user \"Sarah\" in message two, the wrapper ensures message five continues using \"Sarah.\" If the model recommended approach A earlier, the wrapper ensures later messages don't contradict that without explicitly acknowledging the change. This conversational coherence transforms independent model calls into cohesive interactions. The wrapper decides what state is relevant for each turn, manages state growth to avoid exceeding context limits, and handles state conflicts when new information contradicts earlier established facts.</p><p></p><p></p><p></p><p></p><p>Terminology and style guides are encoded in the wrapper to enforce consistent language use. The wrapper can maintain domain-specific vocabularies with preferred and disallowed terms, tone guidelines for formal versus casual situations, formatting conventions for citations, dates, or numerical values, and brand voice requirements. These are injected into prompts to guide generation, and outputs are post-processed to enforce compliance. For instance, if a company always refers to customers as \"members,\" the wrapper can ensure outputs use that terminology consistently. If medical writing requires specific abbreviation standards, the wrapper enforces them. This linguistic consistency is crucial for professional applications where inconsistent terminology confuses users or damages brand perception.</p><p></p><p></p><p></p><p></p><h2>Building Better User Experiences</h2><p></p><p></p><p></p><p></p><p>Wrappers significantly improve user experience through thoughtful design. Progressive disclosure shows information in stages rather than overwhelming users with everything at once. For complex queries requiring extensive responses, the wrapper might generate an outline or summary first, let users indicate what they want expanded, then generate detailed content for selected sections. This gives users control over depth and relevance while reducing wasted generation on information they don't need. The wrapper manages the state across these interactions, tracks what's been shown versus what's available, and generates appropriate responses for drill-down requests.</p><p></p><p></p><p></p><p></p><p>Error handling and graceful degradation ensure users get helpful feedback even when things go wrong. Rather than showing cryptic errors or raw exceptions, the wrapper translates failures into user-friendly messages. If the model times out, the wrapper might offer a simplified query or cached response. If input validation fails, it explains specifically what's wrong and how to fix it. If output validation catches a problem, it might show a partial response with disclaimers or offer to try again with different parameters. The wrapper implements retry logic with exponential backoff for transient failures, circuit breakers to fail fast when services are down, and fallback chains to try progressively simpler approaches. This resilience means users see reliable behavior even when underlying systems are unreliable.</p><p></p><p></p><p></p><p></p><p>Streaming and progressive generation improve perceived responsiveness for long outputs. Rather than waiting for complete generation, the wrapper streams tokens to users as they're generated, creating the impression of faster response even though total latency is unchanged. The wrapper manages the streaming connection, handles interruptions gracefully, and ensures partial outputs are useful even if generation stops early. For structured outputs, the wrapper might buffer generation to ensure valid structure boundaries before streaming, so users never see partial JSON objects or incomplete sentences. Streaming combined with progressive disclosure creates highly responsive interfaces where users feel in control and can interrupt or redirect long generations.</p><p></p><p></p><p></p><p></p><p>Context-aware personalization leverages user history and preferences to improve relevance. The wrapper maintains user profiles with preferences, expertise levels, past interactions, and feedback. When processing requests, it automatically tailors complexity to the user's expertise, prioritizes information matching their interests, adapts tone to their preferences, and avoids repeating information from recent interactions. This personalization happens transparently - users don't explicitly configure it but experience increasingly relevant responses over time. The wrapper implements privacy controls around what's tracked and retained, gives users control over their profiles, and ensures personalization doesn't create filter bubbles or limit access to diverse information.</p><p></p><p></p><p></p><p></p><h2>Practical Implementation Patterns</h2><p></p><p></p><p></p><p></p><p>Implementing effective LLM wrappers requires thoughtful software architecture. The pipeline pattern chains together preprocessing, generation, validation, and post-processing stages in a modular way. Each stage has clear inputs and outputs and can be developed and tested independently. Stages can be conditionally executed based on previous results - if input validation fails, generation is skipped entirely. Stages can be ordered differently for different use cases, and new stages can be inserted without disrupting existing ones. This modularity makes wrappers maintainable and evolvable as requirements change or new techniques emerge.</p><p></p><p></p><p></p><p></p><p>The strategy pattern implements alternative approaches for different situations. The wrapper might have multiple strategies for handling the same request - a fast cached response for common queries, retrieval-augmented generation for factual questions, pure model generation for creative tasks, and tool-augmented generation for computational problems. The wrapper selects strategies based on input characteristics, performance requirements, or previous success rates. Strategies can be chained, where the wrapper tries a fast approach first and falls back to slower but more capable approaches if needed. This flexibility allows optimizing the trade-off between latency, cost, and quality for each specific request.</p><p></p><p></p><p></p><p></p><p>The adapter pattern standardizes interfaces across different underlying models. Applications shouldn't depend on specific model APIs because models change frequently. The wrapper provides a consistent interface while handling model-specific details like prompt formatting, API conventions, token limits, and output parsing. This abstraction lets you swap models without changing application code, run A/B tests comparing models, or route different request types to different models. The adapter also normalizes responses, converting model-specific output formats into consistent application-level data structures. This decoupling is essential for maintainability as the LLM landscape evolves rapidly.</p><p></p><p></p><p></p><p></p><p>Monitoring and observability are critical for production wrappers. The wrapper should instrument every stage, logging inputs, outputs, latencies, validation results, retry attempts, and errors. This telemetry enables debugging problems, optimizing performance, detecting emerging issues, and measuring the impact of changes. Key metrics include success rates at each stage, end-to-end latency percentiles, validation failure rates by type, retry frequencies, fallback invocations, and cost per request. Dashboards visualize these metrics in real-time, alerting on anomalies. A/B testing frameworks built into the wrapper let you experiment with different prompts, validation rules, or strategies while measuring impact on quality metrics. This data-driven approach to wrapper development ensures continuous improvement based on actual production behavior rather than assumptions.</p><p></p><p></p><p></p><p></p><h2>Advanced Techniques and Considerations</h2><p></p><p></p><p></p><p></p><p>Self-consistency checking improves reliability by generating multiple independent responses and using agreement as a quality signal. The wrapper calls the model multiple times with the same input but different random seeds, compares the results, and uses various strategies to combine them. For factual questions, it might use majority voting where the most common answer is selected. For numerical answers, it might average results or flag cases where variance is too high. For open-ended generation, it might show the most coherent option or synthesize information present across multiple samples. This redundancy costs more compute but significantly reduces the likelihood of returning hallucinated or low-quality outputs. The wrapper manages parallel generation, implements comparison logic, and decides when agreement is sufficient versus when uncertainty should be flagged.</p><p></p><p></p><p></p><p></p><p>Constitutional AI principles can be implemented in wrappers to align outputs with values and guidelines. The wrapper can implement multi-stage generation where the model first generates a response, then critiques it against constitutional principles like harmfulness or bias, then revises based on the critique. This self-improvement loop happens within the wrapper, transparent to users. The wrapper provides the principles and orchestrates the generate-critique-revise cycle, potentially iterating multiple times. Alternatively, the wrapper might use a separate critic model to evaluate outputs against principles, rejecting or requesting revisions when principles are violated. This architectural approach to alignment is more flexible than relying solely on model training because principles can be updated without retraining.</p><p></p><p></p><p></p><p></p><p>Adaptive prompting adjusts generation strategy based on real-time feedback. If the wrapper detects that validation is frequently failing for certain input types, it can modify prompts to emphasize those requirements. If users often request clarification on specific topics, the wrapper can proactively include more detail in those areas. If certain phrasings consistently produce better outputs, the wrapper can reinforce those patterns. This adaptation can be manual, where developers adjust prompts based on monitoring data, or automated using reinforcement learning or bandit algorithms that explore prompt variations and optimize for validation success and user satisfaction. The wrapper maintains prompt variants, tracks their performance, and manages the exploration-exploitation trade-off.</p><p></p><p></p>"
      },
      "subtopicSummaries": {
        "2": "<h2>Understanding Decoder-Based Architecture</h2>\n<p>Decoder-based language models represent one of the fundamental architectures in modern LLMs, with GPT (Generative Pre-trained Transformer) being the most famous example. Unlike encoder-only models like BERT that are designed to understand and encode text, or encoder-decoder models like T5 designed for sequence-to-sequence tasks, decoder-based models are specifically architected for text generation. The \"decoder\" terminology comes from the original Transformer paper where the decoder component was responsible for generating output sequences. In a decoder-only architecture, the model consists solely of decoder blocks stacked on top of each other, with each block containing a masked self-attention mechanism and a feed-forward network. The key distinguishing feature is the causal masking in the attention mechanism, which prevents the model from seeing future tokens when processing or generating text. This architectural choice makes decoder models naturally suited for autoregressive generation where each token is predicted based only on the tokens that came before it.</p>\n<p>The attention mechanism in decoder models uses what's called causal or masked self-attention. When processing a sequence of tokens, each position can only attend to itself and previous positions, never to future positions. This is implemented through an attention mask that sets the attention weights to negative infinity for future positions before the softmax operation, effectively zeroing out their influence. This causal constraint is crucial because it aligns the training objective with the inference behavior - during both training and generation, predictions are made autoregressively from left to right. The model learns to predict each token based solely on the context that would actually be available during generation. Popular decoder-based models include the GPT family from OpenAI, LLaMA from Meta, PaLM from Google, and Claude from Anthropic, among many others. These models have demonstrated remarkable capabilities in text generation, reasoning, coding, and general-purpose language understanding.</p>\n<h2>Causal Language Modeling Objective</h2>\n<p>Causal language modeling, also called next-token prediction or autoregressive language modeling, is the training objective used for decoder-based LLMs. The fundamental task is simple: given a sequence of tokens, predict the next token. During training, you take a text sequence, feed tokens one by one into the model, and at each position the model tries to predict what token comes next. The model receives immediate feedback about whether its prediction was correct because you already know the actual next token from your training data. This process repeats across billions or trillions of tokens from diverse text sources, and through this repetitive prediction task, the model learns incredibly rich representations of language, knowledge, reasoning patterns, and more.</p>\n<p>The mathematical formulation of causal language modeling is straightforward. Given a sequence of tokens x = (x₁, x₂, ..., xₙ), the model learns to maximize the probability of each token given all previous tokens: P(xᵢ | x₁, x₂, ..., xᵢ₋₁). The overall objective is to maximize the product of these conditional probabilities across the entire sequence, or equivalently, to minimize the negative log-likelihood. In practice, you compute this as a cross-entropy loss between the model's predicted probability distribution over the vocabulary and the actual next token at each position. The loss is averaged across all positions in the sequence and across all sequences in a batch. This simple objective, when applied at massive scale with enormous models and datasets, produces the sophisticated capabilities we see in modern LLMs.</p>\n<p>What makes causal language modeling so powerful is that it's a self-supervised learning task - you don't need human annotations or labels. Any text can serve as training data because the \"label\" for each prediction is simply the next token that actually appears in the text. This allows training on virtually unlimited amounts of text scraped from the internet, books, code repositories, and other sources. The model implicitly learns grammar, facts, reasoning patterns, common sense, specialized domain knowledge, and countless other aspects of language and the world simply by learning to predict what comes next. This emergent learning is why pre-training with causal language modeling has become the foundation for nearly all modern generative language models.</p>\n<h2>Data Preparation and Tokenization</h2>\n<p>Training a decoder-based LLM begins with assembling and preparing massive text datasets. Pre-training datasets typically contain hundreds of gigabytes to multiple terabytes of text from diverse sources including web pages, books, academic papers, code repositories, social media, news articles, and more. The composition of this data significantly impacts what the model learns - more code makes it better at programming, more scientific papers improve technical knowledge, more conversational text improves dialogue abilities. Curating high-quality training data involves filtering out low-quality content, removing duplicate documents, handling multiple languages appropriately, and balancing different domains and sources. Data quality matters enormously because the model learns from everything it sees, including biases, errors, and problematic content in the training data.</p>\n<p>Before text can be fed into the model, it must be converted into tokens using a tokenizer. Tokenization breaks text into smaller units that serve as the atomic elements the model processes. Modern LLMs typically use subword tokenization methods like Byte-Pair Encoding (BPE) or SentencePiece that balance between character-level and word-level tokenization. These methods split text into common subwords and characters, allowing the model to handle any possible text while keeping the vocabulary size manageable - typically 32,000 to 100,000 tokens. A token is roughly equivalent to four English characters on average, though this varies by language and content type. The tokenizer is trained on a large sample of your training data before model training begins, learning which subword units appear frequently and should be included in the vocabulary. Once trained, the tokenizer converts all training text into sequences of integer token IDs that the model can process.</p>\n<p>The tokenized text is organized into training examples of fixed sequence length, typically 2,048, 4,096, or 8,192 tokens depending on the model's maximum context window. Longer sequences allow the model to learn longer-range dependencies but require more memory and computation. Documents are concatenated together and split into these fixed-length chunks, with special tokens marking document boundaries when needed. These sequences are then batched together for efficient parallel processing on GPUs. Data preprocessing also involves shuffling the order of sequences to ensure the model sees diverse content rather than processing all documents from one source consecutively. Some training procedures use curriculum learning where easier or shorter sequences are presented earlier in training before progressing to more complex examples.</p>\n<h2>The Training Process</h2>\n<p>Training a decoder-based LLM follows the standard deep learning loop but at massive scale. You initialize the model parameters randomly or from a smaller pre-trained model, then repeatedly process batches of training sequences, compute predictions, calculate loss, and update parameters through backpropagation. For each batch, the model receives sequences of token IDs, processes them through the stacked decoder layers to produce predictions for the next token at each position, compares these predictions to the actual next tokens using cross-entropy loss, computes gradients through backpropagation, and updates all model parameters using an optimizer like AdamW. This cycle repeats millions of times over weeks or months of continuous training on clusters of GPUs or TPUs.</p>\n<p>The forward pass through the model works as follows: token IDs are converted to embedding vectors, positional encodings are added to give the model information about token positions, these embeddings flow through multiple decoder blocks where each block applies masked self-attention to let tokens attend to previous context followed by a feed-forward network to process the attended information, and finally the output goes through a linear projection and softmax to produce probability distributions over the vocabulary for predicting the next token at each position. During training, you process entire sequences in parallel - even though the predictions are autoregressive, you can compute the loss for all positions simultaneously because you know all the actual next tokens. This parallel processing during training is much more efficient than the sequential generation required during inference.</p>\n<p>The loss calculation focuses on how well the model predicts each next token. At each position in the sequence, the model outputs a probability distribution over the entire vocabulary. The cross-entropy loss measures how much probability the model assigned to the actual next token - when the model assigns high probability to the correct token, the loss is low, and when it assigns low probability, the loss is high. The gradient of this loss with respect to all model parameters is computed through backpropagation, flowing backwards through all the layers. These gradients indicate how to adjust each parameter to reduce the loss, and the optimizer uses them to update the parameters by a small amount in the direction that should improve predictions. The learning rate controls how large these updates are - typically starting larger and decreasing over time through a learning rate schedule.</p>\n<h2>Training Infrastructure and Scaling</h2>\n<p>Training large language models requires specialized infrastructure and techniques to handle the computational demands. Pre-training models with billions or hundreds of billions of parameters requires distributing the computation across hundreds or thousands of GPUs. This involves several forms of parallelism working together. Data parallelism replicates the model across multiple devices, with each device processing different batches of data and gradients being synchronized periodically. Model parallelism splits the model itself across devices because it's too large to fit on a single GPU - this includes tensor parallelism that splits individual layers horizontally and pipeline parallelism that splits the model vertically across layers. Efficient training requires carefully orchestrating these parallel strategies to maximize GPU utilization while minimizing communication overhead between devices.</p>\n<p>Memory management is a critical challenge in training large models. Beyond the model parameters themselves, training requires storing activations from the forward pass for use in backpropagation, gradients for all parameters, and optimizer states like momentum buffers that can be even larger than the model. Techniques like gradient checkpointing trade computation for memory by recomputing some activations during the backward pass instead of storing them. Mixed-precision training uses 16-bit floating-point for most calculations to reduce memory and speed up computation, while maintaining 32-bit precision for critical operations to preserve training stability. Gradient accumulation allows training with effective batch sizes larger than what fits in memory by processing multiple micro-batches and accumulating their gradients before doing a parameter update.</p>\n<p>Training stability is crucial because training runs can last weeks or months and failures are costly. Models can diverge or collapse if learning rates are too high or if numerical instabilities occur. Techniques for maintaining stability include careful learning rate scheduling that warms up gradually at the start, gradient clipping to prevent extremely large updates, layer normalization within the model architecture to keep activations in reasonable ranges, and careful initialization of parameters. Monitoring training metrics like loss curves, gradient norms, and activation statistics helps identify problems early. Checkpointing saves model state periodically so training can resume from the last checkpoint if failures occur. Training large models is an engineering challenge as much as a machine learning one, requiring expertise in distributed systems, hardware optimization, and debugging at scale.</p>\n<h2>Pre-Training vs Fine-Tuning</h2>\n<p>Pre-training and fine-tuning represent two distinct phases in the life of a decoder-based LLM. Pre-training is the initial phase where the model learns from massive amounts of diverse text using the causal language modeling objective. This phase is extremely expensive, potentially costing millions of dollars in compute for state-of-the-art models, and takes weeks or months on large GPU clusters. Pre-training produces a foundation model with broad capabilities and knowledge but without specific task optimization or behavioral alignment. The pre-trained model has learned the patterns and structure of language, accumulated factual knowledge from its training data, and developed reasoning capabilities, but it simply predicts the next token without understanding helpfulness, safety, or user intent.</p>\n<p>Fine-tuning takes a pre-trained model and continues training it on a smaller, more focused dataset to specialize it for specific tasks or align it with desired behaviors. Supervised fine-tuning uses curated datasets of high-quality examples showing desired input-output pairs - for chatbots, this might be conversations demonstrating helpful, harmless, and honest responses. Instruction tuning is a form of fine-tuning that trains models to follow natural language instructions across diverse tasks, using datasets with instructions paired with appropriate responses. This teaches the model to behave as a helpful assistant rather than just completing text. Fine-tuning typically uses much smaller datasets than pre-training - thousands to hundreds of thousands of examples rather than billions - and much lower learning rates to avoid catastrophic forgetting where the model loses its pre-trained knowledge.</p>\n<p>Reinforcement Learning from Human Feedback (RLHF) is an advanced fine-tuning technique that has become crucial for modern conversational LLMs. After supervised fine-tuning, human evaluators rank or rate different model outputs for the same prompt, and this preference data is used to train a reward model that predicts what humans prefer. The LLM is then further trained using reinforcement learning to maximize this reward signal, essentially optimizing for generating outputs humans rate highly. This process helps align models with human values and preferences in ways that are difficult to capture with supervised learning alone. RLHF is computationally intensive because it involves training two models and using reinforcement learning, but it significantly improves output quality, safety, and alignment.</p>\n<h2>When and Why to Train Decoder-Based LLMs</h2>\n<p>The decision to train a decoder-based LLM from scratch versus using existing models is significant and depends on multiple factors. Training from scratch makes sense when you need complete control over the model architecture and training data, when you're working in a specialized domain with proprietary data that provides competitive advantage, when you have unique requirements that existing models don't meet, or when you're conducting research into new architectures or training methods. Organizations with massive computational resources and unique data assets might train proprietary models to differentiate their products. Research institutions train models to advance the field's understanding of how these systems work and how to improve them.</p>\n<p>However, training from scratch is rarely the right choice for most organizations and use cases. The computational costs are prohibitive - training a competitive large language model can cost millions of dollars in compute resources. The expertise required spans machine learning, distributed systems, data engineering, and more. The time investment is substantial, with training runs lasting weeks or months. For most practical applications, starting with an existing pre-trained model and fine-tuning it for your specific needs is far more efficient and cost-effective. Many high-quality open-source models are available from organizations like Meta, Mistral, and others that provide excellent starting points.</p>\n<p>Fine-tuning existing models is appropriate when you need to specialize behavior for a specific domain or task, when you want to adapt the model's style or formatting to your needs, when you need to align the model with your organization's values and guidelines, or when you want to improve performance on tasks where the base model is weak. Fine-tuning requires much less compute - often doable on a single GPU or small cluster - and much less data than pre-training. It allows you to leverage the broad knowledge and capabilities learned during pre-training while adding the specific behaviors or knowledge you need. Techniques like parameter-efficient fine-tuning (PEFT) methods such as LoRA make fine-tuning even more accessible by updating only a small fraction of parameters.</p>\n<h2>Practical Training Considerations</h2>\n<p>Several practical considerations affect training outcomes and efficiency. The model size must balance capability with computational constraints - larger models are more capable but exponentially more expensive to train and deploy. Common sizes range from 7 billion to 70 billion parameters for models that can be practically deployed, with some frontier models reaching hundreds of billions. Scaling laws help predict how performance improves with model size, dataset size, and compute budget, guiding decisions about resource allocation. These laws suggest that optimal performance comes from scaling model and data together rather than just making models larger.</p>\n<p>Hyperparameters significantly impact training success. Learning rate is perhaps most critical - too high causes instability or divergence, too low results in slow learning or getting stuck. Learning rate schedules typically warm up gradually, maintain a constant rate, then decay as training progresses. Batch size affects both convergence speed and final performance, with larger batches being more computationally efficient but potentially leading to different optima. Sequence length determines how much context the model can consider - longer is better for learning long-range dependencies but requires more memory. The number of training steps or tokens determines how much data the model sees - contemporary models are often trained on hundreds of billions to trillions of tokens.</p>\n<p>Evaluation during training helps assess progress and detect problems. Perplexity measures how surprised the model is by test data - lower is better and indicates better predictive performance. However, perplexity doesn't capture all aspects of model quality. Downstream task evaluations assess performance on specific benchmarks for reading comprehension, reasoning, knowledge, and other capabilities. These evaluations help determine when to stop training and which checkpoint achieves the best balance of capabilities. Comparing performance across different domains helps identify whether the model is learning broadly or overfitting to certain types of content. Monitoring these metrics throughout training informs decisions about hyperparameter adjustments or whether to continue training.</p>\n<h2>Key Takeaways for Exam Preparation</h2>\n<p>The essential concepts to master are understanding that decoder-based LLMs use causal or masked self-attention to enable autoregressive generation, knowing that causal language modeling trains models to predict the next token given previous tokens through a simple self-supervised objective that scales to massive datasets, and recognizing that this training objective produces broad capabilities including language understanding, knowledge, and reasoning without task-specific supervision. You should understand the distinction between pre-training on massive diverse data to build foundation models versus fine-tuning on smaller focused datasets to specialize or align behavior. Know the key components of the training process including tokenization, batching, forward pass through stacked decoder blocks, loss calculation with cross-entropy, backpropagation, and parameter updates.</p>\n<p>Be familiar with the infrastructure challenges including distributed training across many GPUs using data and model parallelism, memory management techniques like mixed-precision training and gradient checkpointing, and stability considerations like learning rate scheduling and gradient clipping. Understand when training from scratch versus fine-tuning makes sense based on resources, data, and requirements. Recognize that causal language modeling's simplicity and self-supervised nature make it remarkably powerful for learning from unlabeled text at scale. The bottom line is that decoder-based LLMs trained with causal language modeling have become the dominant paradigm for generative AI because the architecture naturally supports generation, the training objective learns from unlimited text without labels, and the resulting models demonstrate emergent capabilities across countless tasks. Understanding how to train these models, even if you never train one from scratch, is fundamental to working effectively with modern LLMs.</p>"
      }
    },
    "3": {
      "readingsComplete": [],
      "notes": "",
      "lastModified": 1762707755312,
      "readingUserNotes": {
        "0": ""
      }
    },
    "4": {
      "readingsComplete": [],
      "notes": "",
      "lastModified": 1762725143304,
      "readingUserNotes": {
        "0": ""
      },
      "subtopicSummaries": {
        "0": ""
      },
      "subtopicStudyGuides": {
        "0": "<h2>Introduction to Model Compression and Optimization</h2>\n<p>Large language models <mark>present significant deployment challenges due to their massive parameter counts and computational requirements</mark>. A model like GPT-3 with 175 billion parameters requires approximately 350GB of memory just to store the weights in standard 16-bit floating-point format, making deployment on edge devices or even standard GPU hardware impractical. <mark>Model optimization techniques address these challenges by reducing the memory footprint and computational demands while attempting to preserve model accuracy</mark>. The three primary techniques you'll need to understand for the certification are <b>pruning,</b> <b>sparsity</b>, and <b>quantization (both weights and activations)</b>, each of which exploits different properties of neural networks to achieve compression and acceleration.</p>\n<h2>Understanding Neural Network Pruning</h2>\n<p><mark>Pruning is the process of removing parameters or connections from a neural network that contribute minimally to the model's overall performance.</mark> The fundamental insight behind pruning is that neural networks, especially large language models, are often overparameterized and contain significant redundancy. During training, many weights remain close to zero or contribute negligibly to the final output, suggesting they can be removed without substantial accuracy loss. There are two main categories of pruning: <b>unstructured </b>and <b>structured pruning.</b></p>\n<p><mark><b>Unstructured pruning</b> removes individual weights based on magnitude or other importance criteria, creating sparse weight matrices with irregular patterns of zero values.</mark> While this achieves high compression rates, it requires specialized sparse matrix libraries and hardware support to realize actual speedups, since standard dense matrix operations don't benefit from scattered zero values.<mark> <b>Structured pruning</b>, conversely, removes entire channels, filters, attention heads, or layers according to structured patterns.</mark> This approach results in smaller dense matrices that can be efficiently executed on standard hardware without requiring sparse kernels. For example, you might prune entire attention heads in a transformer model or remove complete channels from a feedforward layer, reducing the actual dimensions of the weight matrices.</p>\n<p>The pruning process typically follows an iterative workflow. First, you train a model to convergence using standard methods. Then you apply a pruning criterion to identify and remove the least important parameters, often based on magnitude (removing weights with absolute values below a threshold) or more sophisticated metrics like gradient-based importance scores. <b>After pruning, the model's accuracy typically degrades, so you perform fine-tuning or retraining to allow the remaining weights to compensate for the removed parameters</b>. This pruning-and-retraining cycle can be repeated iteratively to achieve progressively higher sparsity levels. Modern approaches like magnitude pruning, movement pruning, and lottery ticket hypothesis-based methods each offer different trade-offs between compression rate and accuracy retention.</p>\n<h2>Sparsity and Its Role in Model Efficiency</h2>\n<p><b><mark>Sparsity</mark></b> refers to the <b>proportion of zero or near-zero values in a tensor</b>, and it's intimately connected with pruning—pruning creates sparsity, and sparsity enables computational savings. A weight matrix with 90% sparsity means that 90% of its values are zero, requiring storage for only 10% of the original parameters. However, realizing the benefits of sparsity depends critically on the sparsity pattern and available hardware support. Random unstructured sparsity, where zeros are scattered throughout the matrix, requires sparse matrix storage formats like Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) and specialized compute kernels to skip zero operations.</p>\n<p>Modern GPUs and accelerators increasingly include hardware support for structured sparsity patterns. NVIDIA's Ampere architecture and newer GPUs support 2:4 structured sparsity, where exactly 2 out of every 4 consecutive values are zero in a specific pattern. This enables the hardware to achieve 2x theoretical speedup in matrix multiplications because it can predictably skip operations involving zeros without the overhead of checking each element. The Tensor Cores in these GPUs are specifically designed to exploit this pattern, making structured sparsity much more practical than unstructured sparsity for inference acceleration.</p>\n<p>The interaction between sparsity and hardware is crucial for the certification exam. You need to understand<b> that high sparsity doesn't automatically translate to faster inference—the sparsity must be in a pattern that your target hardware can exploit</b>. CPU implementations might benefit more from moderate structured sparsity with simpler patterns, while GPU implementations can leverage higher sparsity levels but only if the pattern aligns with the hardware's capabilities. This is why methods that train models with target sparsity patterns from scratch (sparse training) or that prune to hardware-friendly patterns are particularly valuable for production deployments.</p>\n<h2>Weight Quantization Fundamentals</h2>\n<p><mark>Weight quantization reduces the numerical precision used to represent model parameters, typically converting from 32-bit or 16-bit floating-point values to lower-bit representations like 8-bit integers (INT8), 4-bit integers (INT4), or even binary values</mark>. The motivation is straightforward: reducing bit-width proportionally <b>reduces memory requirements and can significantly accelerate computation on hardware with specialized low-precision operations</b>. A model quantized from FP32 to INT8 reduces memory footprint by 4x and can achieve substantial speedup on hardware with INT8 acceleration support.</p>\n<p>There are several quantization approaches you should understand. <b>Post-training quantization (PTQ)</b> applies quantization to an already-trained model without additional training. The simplest form is symmetric quantization, where you map the range of floating-point values symmetrically to the integer range. For example, INT8 provides 256 distinct values (-128 to 127), so you determine a scaling factor by analyzing the distribution of weights in each layer or tensor and map the floating-point range to this integer range. Asymmetric quantization adds a zero-point offset to better handle distributions that aren't centered around zero, capturing a wider effective range of values.</p>\n<p><b>Quantization-aware training (QAT) </b>incorporates quantization into the training process itself, allowing the model to learn weights that are more robust to quantization error. During training, you simulate quantization in the forward pass while using standard floating-point operations for backward propagation and weight updates. This approach typically achieves better accuracy than post-training quantization, especially at lower bit-widths like 4-bit or binary quantization, because the model learns to work within the constraints of quantized representations from the beginning.</p>\n<p>The key concepts for the exam include understanding per-tensor versus per-channel quantization. Per-tensor quantization uses a single scaling factor for an entire weight matrix, while per-channel quantization uses different scaling factors for each output channel, better accommodating varying magnitude distributions across channels at the cost of slightly more complex implementation. You should also understand calibration—the process of analyzing activation distributions using representative data to determine optimal scaling factors and zero-points for each quantized layer. Proper calibration is critical for maintaining accuracy in post-training quantization scenarios.</p>\n<h2>Activation Quantization and Dynamic Range Challenges</h2>\n<p>While weight quantization is relatively straightforward because weights are static and known after training, activation quantization presents unique challenges because activations are data-dependent and vary with different inputs. Activations represent the intermediate outputs flowing through the network during inference, and quantizing them is essential for end-to-end accelerated inference on hardware that operates in reduced precision. If you quantize weights to INT8 but keep activations in FP32, you lose most of the potential speedup because the hardware must still perform mixed-precision operations.</p>\n<p>The primary challenge with activation quantization is determining appropriate quantization parameters without knowing in advance what values will appear during inference. The solution is calibration using a representative dataset. You run a subset of your data through the model while tracking the distribution of activations at each layer, recording statistics like minimum and maximum values or computing percentiles to exclude outliers. These statistics inform the scaling factors and zero-points used during actual quantized inference. Different calibration methods exist, including min-max calibration (using observed extremes), entropy calibration (minimizing KL divergence between original and quantized distributions), and percentile calibration (using 99th or 99.9th percentiles to handle outliers).</p>\n<p>Dynamic quantization represents a middle-ground approach where weights are quantized statically but activations are quantized dynamically during inference based on the actual range of values in each batch or sequence. This adds small overhead for computing quantization parameters on-the-fly but provides better accuracy for models with highly variable activation distributions, such as language models processing different lengths and types of text. Static quantization, conversely, precomputes all quantization parameters including those for activations, offering maximum performance but requiring careful calibration to prevent accuracy degradation.</p>\n<h2>Hardware Acceleration and Optimization Synergies</h2>\n<p>Understanding how these techniques interact with specific hardware is crucial for the certification. Modern AI accelerators like NVIDIA's Tensor Cores, Google's TPUs, and specialized inference chips from companies like Qualcomm and Apple include dedicated circuits for low-precision matrix operations. NVIDIA A100 and H100 GPUs provide substantial throughput improvements for INT8 and INT4 operations compared to FP16 or FP32, but only when both weights and activations are quantized and when the operations are properly formatted to utilize Tensor Cores.</p>\n<p>The combination of pruning, sparsity, and quantization can provide multiplicative benefits. For instance, applying 90% structured sparsity (10% remaining weights) combined with INT8 quantization reduces memory by approximately 40x compared to the original FP32 dense model (10x from sparsity, 4x from quantization). However, you must understand the practical limitations—achieving these benefits requires that your inference framework supports sparse quantized operations, which isn't universally available across all frameworks and hardware platforms.</p>\n<p>Different deployment scenarios favor different optimization strategies. Edge devices with limited memory but moderate compute capabilities might prioritize aggressive quantization (INT4 or lower) with moderate pruning. Cloud GPU deployments might use INT8 quantization with structured sparsity patterns aligned to Tensor Core requirements. CPU deployments might benefit most from structured pruning that reduces the actual FLOP count rather than relying on specialized sparse kernels. Understanding these trade-offs and knowing which optimizations work best for different hardware targets is essential knowledge for the certification.</p>\n<h2>Practical Implementation Considerations and Trade-offs</h2>\n<p>When implementing these techniques, several practical considerations determine success. First,<mark> accuracy degradation must be carefully monitored and managed</mark>. Each optimization technique individually causes some accuracy loss, and combining them can compound these effects. The general principle is to apply optimizations conservatively, validating accuracy at each step against your acceptable threshold. Some models are naturally more robust to quantization and pruning—for example, models with significant overparameterization typically tolerate higher compression rates than smaller, more efficient architectures.</p>\n<p>The optimization workflow typically follows a specific order. Start with quantization-aware training or post-training quantization since this provides immediate memory benefits and establishes your accuracy baseline with quantized operations. Then apply pruning, as attempting to quantize an already-pruned model can be more challenging due to the reduced capacity. Finally, optimize the sparsity pattern if needed to align with your target hardware's requirements. Some frameworks like PyTorch provide tools like torch.quantization and torch.nn.utils.prune, while NVIDIA offers TensorRT for inference optimization and quantization, and frameworks like ONNX Runtime support cross-platform optimization.</p>\n<p>You should understand common pitfalls and their solutions. Quantizing the first and last layers of a network often causes disproportionate accuracy loss, so these are sometimes kept in higher precision. Attention mechanisms in transformers can be particularly sensitive to quantization because they compute similarity scores that rely on precise dot products. Outlier features in activations can severely impact quantization quality—recent research has identified that language models often have high-magnitude outlier features in specific channels, requiring special handling through techniques like per-channel quantization or mixed-precision schemes. For the exam, you should know that successful optimization requires iteration, profiling, and validation, not just applying techniques blindly.</p>\n<h2>Conclusion and Key Takeaways</h2>\n<p>For the NVIDIA certification, you should synthesize understanding across these dimensions: <mark>the mathematical foundations of each technique, their computational and memory impacts, hardware-specific considerations, practical implementation approaches, and the trade-offs between accuracy and efficiency</mark>. Remember that pruning reduces parameter count, sparsity enables computational skipping, weight quantization reduces precision of learned parameters, and activation quantization enables end-to-end low-precision inference. Modern deployment scenarios typically combine these techniques, requiring you to understand their interactions and optimization for specific hardware platforms. Your ability to reason about which techniques apply to different scenarios, understand their limitations, and recognize when specialized hardware support is required will be tested throughout the certification exam.</p>",
        "1": "<h2>Overview of Quantization Strategy Selection</h2>\n<p>Choosing the appropriate quantization strategy is a critical decision that balances multiple competing factors: the computational capabilities of your target hardware, the memory constraints of your deployment environment, the accuracy requirements of your task, and the time and resources available for optimization. Unlike a one-size-fits-all approach, effective quantization requires understanding the specific characteristics of your model, your data distribution, and your hardware platform. For the NVIDIA certification, you need to develop a decision framework that considers these factors systematically, recognizing that a quantization strategy optimal for inference on an A100 GPU might differ significantly from one designed for edge deployment or CPU inference.</p>\n<p>The three primary quantization strategies—post-training quantization (PTQ), quantization-aware training (QAT), and activation quantization schemes—each occupy different points in the trade-off space between ease of implementation, accuracy preservation, and performance optimization. Post-training quantization offers the fastest path to deployment, requiring no retraining but potentially sacrificing accuracy, especially at aggressive bit-widths. Quantization-aware training provides superior accuracy at low bit-widths but demands significant computational resources for retraining. Understanding when to apply each strategy, and how to combine them effectively, forms the core of this certification topic. You must also understand how these strategies interact with specific hardware features like NVIDIA's Tensor Cores, which provide dramatic acceleration for quantized operations but only when data and operations are properly formatted.</p>\n<h2>Post-Training Quantization: Principles and Implementation</h2>\n<p>Post-training quantization converts a fully-trained floating-point model to lower precision without additional training, making it the most accessible quantization approach. PTQ works by analyzing the distribution of weights and activations in the pre-trained model, determining appropriate scaling factors and zero-points, and converting the model to operate in reduced precision. The fundamental challenge is that neural networks trained in floating-point develop weight and activation distributions optimized for that precision, and naively reducing precision introduces quantization error that can significantly degrade accuracy.</p>\n<p>The PTQ process begins with weight quantization, which is relatively straightforward because weights are static. For each weight tensor, you determine the range of values (typically the minimum and maximum weight values) and map this range to your target integer representation. For INT8 quantization, you have 256 distinct values (-128 to 127 for signed integers), so you compute a scale factor: scale = (max_value - min_value) / 255. Each floating-point weight is then converted to INT8 by: quantized_weight = round((original_weight - zero_point) / scale). The zero_point handles asymmetric distributions where the range isn't centered around zero. For symmetric quantization (common in many implementations), you simplify by ensuring the range is symmetric around zero and eliminating the zero_point term.</p>\n<p>Activation quantization in PTQ requires calibration because activations are data-dependent. The calibration process involves running a representative subset of your data (typically 100-1000 samples) through the model while collecting statistics about activation distributions at each layer. These statistics inform your quantization parameters. The simplest approach, min-max calibration, uses the observed minimum and maximum activation values, but this can be fragile because outliers might force a wide quantization range, reducing precision for the majority of values. More sophisticated approaches like percentile calibration use the 99th or 99.9th percentile instead of absolute extremes, or entropy minimization methods that choose quantization parameters to minimize the KL divergence between the original and quantized distributions.</p>\n<p>For the certification, you should understand layer-wise versus channel-wise quantization. Layer-wise (or per-tensor) quantization uses a single scaling factor for an entire activation tensor, while channel-wise (or per-channel) quantization uses different scaling factors for each channel, accommodating the fact that different channels often have vastly different magnitude distributions. Per-channel quantization typically preserves accuracy better but adds slight complexity to the implementation. NVIDIA's TensorRT, a key tool for the exam, supports both approaches and automatically selects the appropriate granularity during its optimization process.</p>\n<h2>Quantization-Aware Training: Advanced Accuracy Preservation</h2>\n<p>Quantization-aware training addresses PTQ's accuracy limitations by incorporating quantization into the training process itself, allowing the model to learn parameters that are inherently robust to quantization error. QAT simulates quantization during forward propagation while maintaining full precision for backward propagation and weight updates. This approach enables the model to find regions of the loss landscape that are \"flat\" with respect to quantization—areas where rounding to lower precision causes minimal accuracy impact.</p>\n<p>The core mechanism of QAT involves inserting fake quantization operations throughout the model. These operations simulate the rounding and clipping effects of actual quantization while keeping all tensors in floating-point format for compatibility with standard training infrastructure. During the forward pass, weights and activations pass through these fake quantization nodes: the values are quantized to the target bit-width and immediately dequantized back to floating-point. Mathematically, for a weight w: fake_quantized_w = dequantize(quantize(w)) = scale × round(w / scale). The gradient flows through these operations using the straight-through estimator, which treats the non-differentiable round() operation as having a derivative of 1, allowing backpropagation to proceed normally.</p>\n<p>QAT typically begins from a pre-trained floating-point model rather than training from scratch, since starting from random initialization in a constrained quantized space is extremely challenging. The QAT fine-tuning process usually requires 5-20% of the original training time—much less than full training but still significant. During QAT, you progressively introduce quantization, often starting with higher bit-widths (like INT16) and gradually reducing to your target precision (INT8 or INT4), giving the model time to adapt. Learning rates during QAT should be lower than initial training, typically 1/100th to 1/10th of the original learning rate, because the model is fine-tuning within a pre-trained parameter space.</p>\n<p>The benefits of QAT become especially pronounced at aggressive quantization levels. While PTQ to INT8 often works reasonably well for many models, PTQ to INT4 or lower typically causes severe accuracy degradation. QAT enables these aggressive quantization levels by learning to compensate for quantization error during training. For the exam, you should know that QAT is essential when targeting bit-widths below INT8, when working with smaller models that have less inherent redundancy, or when your task requires accuracy very close to the full-precision baseline. However, QAT requires access to training data and computational resources for retraining, which may not always be available in production scenarios.</p>\n<h2>Activation Quantization Strategies and Dynamic Range Management</h2>\n<p>Activation quantization presents unique challenges because activations vary with input data, and different inputs can produce wildly different activation distributions. While weight quantization is \"static\" (weights don't change during inference), activation quantization can be implemented as either static or dynamic, each with distinct trade-offs. Static activation quantization precomputes all quantization parameters during calibration and uses fixed scales and zero-points during inference, maximizing performance but requiring careful calibration. Dynamic activation quantization computes quantization parameters on-the-fly for each input or batch, adapting to the actual range of values present but adding computational overhead.</p>\n<p>For transformer-based language models, which dominate modern LLM applications, activation quantization is particularly challenging due to several factors. First, transformers process variable-length sequences, creating diverse activation distributions depending on sequence length and content. Second, research has identified that transformers exhibit systematic outlier features—specific channels in the hidden states that occasionally take on extreme values orders of magnitude larger than typical activations. These outliers, if included in the quantization range, force very coarse quantization of the remaining (normal) features, degrading accuracy. If excluded, they can cause clipping that damages the model's ability to process certain inputs.</p>\n<p>Several strategies address these challenges. Mixed-precision quantization maintains most layers in INT8 but keeps problematic layers (like the first embedding layer, final classification layer, or layers containing severe outliers) in higher precision like FP16. Per-channel or per-token quantization granularity helps by adapting quantization parameters to the specific statistics of each channel or token position. SmoothQuant, a recent technique particularly relevant for LLMs, migrates the quantization difficulty from activations to weights by mathematically transforming the model to smooth activation distributions at the cost of making weight distributions slightly harder to quantize—a favorable trade since weights are easier to quantize accurately. For the certification, understand that activation quantization often requires more sophisticated strategies than weight quantization, and modern frameworks provide specialized techniques for handling transformer-specific challenges.</p>\n<h2>NVIDIA A100 and H100 Tensor Core Architecture</h2>\n<p>Understanding the specific capabilities and requirements of NVIDIA's flagship datacenter GPUs is essential for the certification. The A100 and H100 GPUs feature Tensor Cores, specialized processing units designed for matrix multiplication with support for multiple precision formats. These Tensor Cores provide the massive throughput advantages that make quantization worthwhile, but they have specific requirements about data layout and operation types that you must satisfy to achieve peak performance.</p>\n<p>NVIDIA A100's third-generation Tensor Cores support several precision formats: FP64, TF32 (a special format for FP32 that uses FP16's 10-bit mantissa with FP32's 8-bit exponent), FP16, BF16 (Brain Float 16), INT8, INT4, and binary operations. The performance characteristics vary dramatically across these formats. On A100, INT8 Tensor Core operations achieve up to 624 TOPS (trillion operations per second), compared to 312 TFLOPS for FP16 and 19.5 TFLOPS for FP32 on Tensor Cores. This means INT8 operations can theoretically run 2x faster than FP16 and about 32x faster than FP32, though real-world speedups depend on memory bandwidth and other bottlenecks.</p>\n<p>H100, NVIDIA's latest generation, provides even more dramatic capabilities. H100's fourth-generation Tensor Cores deliver 3,958 TOPS for INT8 and support for FP8 (a new 8-bit floating-point format), which provides better dynamic range than INT8 for many applications while maintaining similar performance characteristics. H100 also features transformer-specific optimizations that accelerate attention mechanisms, which are particularly beneficial for LLM inference. The key insight for the exam is that both architectures achieve these massive throughput numbers only when operations are properly formatted to utilize Tensor Cores—this requires that both inputs and outputs are appropriately sized (multiples of certain dimensions) and that you're using libraries like cuBLAS, cuDNN, or TensorRT that know how to invoke Tensor Core operations.</p>\n<p>To leverage Tensor Cores effectively, you need to understand their dimension requirements. Tensor Cores perform matrix multiplication on tiles of specific sizes. For INT8 on A100, the optimal tile size is typically 32×8×16 or similar dimensions. Your matrices should ideally have dimensions that are multiples of these tile sizes to avoid inefficient padding and to maximize hardware utilization. TensorRT automatically handles much of this optimization, but understanding the underlying requirements helps you make better decisions about model architecture and quantization strategy.</p>\n<h2>Precision Format Selection: FP16, BF16, INT8, and Beyond</h2>\n<p>Selecting the appropriate precision format requires understanding the characteristics of each format and how they align with your model's numerical requirements. FP16 (IEEE 754 half-precision floating-point) uses 1 sign bit, 5 exponent bits, and 10 mantissa bits, providing a dynamic range from approximately 6×10^-8 to 65,504. FP16's appeal is that it's a drop-in replacement for FP32 in most models with minimal accuracy loss, and it provides 2x memory reduction and substantial compute acceleration on modern GPUs. However, FP16's limited range can cause issues with gradient underflow during training, though this is less problematic for inference.</p>\n<p>BF16 (Brain Float 16), developed by Google and now widely supported by NVIDIA, uses 1 sign bit, 8 exponent bits (same as FP32), and 7 mantissa bits. This trades precision for range compared to FP16—BF16 can represent the same range of magnitudes as FP32 (up to 3.4×10^38) but with reduced precision. For many deep learning applications, particularly training, BF16's increased range makes it more stable than FP16 while maintaining similar memory and compute benefits. For inference, both FP16 and BF16 typically provide nearly lossless accuracy compared to FP32 for well-trained models.</p>\n<p>INT8 represents a more aggressive quantization, using 8 bits to represent integer values typically in the range -128 to 127 for signed INT8. Unlike floating-point formats, INT8 requires explicit quantization parameters (scale and zero-point) and can introduce noticeable accuracy degradation if not carefully implemented. However, INT8 provides 4x memory reduction versus FP32 and 2x versus FP16, along with substantial compute acceleration. The certification exam will likely test your understanding of when INT8 is appropriate: it works well for models with sufficient capacity, when using proper calibration techniques, and particularly when deploying models trained with QAT.</p>\n<p>FP8, newly introduced with H100, represents an interesting middle ground. NVIDIA defines two FP8 formats: E4M3 (4 exponent bits, 3 mantissa bits) optimized for forward pass, and E5M2 (5 exponent bits, 2 mantissa bits) optimized for gradients in backward pass. FP8 provides better numerical behavior than INT8 for many applications while offering similar performance and memory benefits. For inference-focused applications, E4M3 FP8 can be an attractive alternative to INT8, especially for models where INT8 quantization proves challenging. Understanding that precision format selection isn't just about maximizing compression but about matching the numerical requirements of your model and task is crucial for the exam.</p>\n<h2>Measuring Accuracy Trade-offs and Validation Methodologies</h2>\n<p>Quantifying the accuracy impact of quantization is essential for making informed decisions and validating that your quantized model meets deployment requirements. The measurement methodology depends on your task type and metrics. For language models, you typically evaluate perplexity on held-out text, along with task-specific metrics like accuracy, F1 score, or BLEU score for machine translation. For classification models, you measure top-1 and top-5 accuracy, precision, recall, and F1 scores. The key principle is to use the same evaluation protocol for both your original and quantized models to ensure fair comparison.</p>\n<p>When measuring accuracy degradation, you should understand what level of degradation is acceptable, which varies by application. For many industrial applications, a 1-2% absolute accuracy drop is considered acceptable if it enables significant deployment benefits. For safety-critical applications, even 0.5% degradation might be unacceptable. Beyond average accuracy, you should examine per-class or per-sample performance to identify if quantization affects certain categories disproportionately. Some samples may be more sensitive to quantization than others, and understanding these failure modes helps you decide whether to proceed with quantization or invest in more sophisticated techniques.</p>\n<p>Establishing proper baselines is critical. Your baseline should be the best full-precision model you can achieve, typically FP32 or sometimes FP16 if that's what was used during training. When reporting results, be explicit about what you're comparing: \"INT8 PTQ achieves 92.3% accuracy versus 93.5% for FP32 baseline\" is clear, while vague statements about \"minimal accuracy loss\" aren't useful. You should also validate on multiple datasets if possible—a quantized model might perform well on in-distribution test data but degrade more severely on out-of-distribution samples or adversarial examples.</p>\n<p>For the exam, understand the concept of Pareto frontiers in the accuracy-efficiency trade-off space. Different quantization strategies occupy different points on this frontier: FP16 provides minimal accuracy impact with moderate speedup, INT8 PTQ provides more speedup with some accuracy loss, INT8 QAT recovers much of that accuracy loss, and INT4 provides maximum compression but requires careful implementation. Your job is to select the point on this frontier that meets your deployment requirements. Tools like TensorRT provide profiling capabilities that measure actual inference latency and throughput, allowing you to quantify the efficiency gains alongside accuracy measurements.</p>\n<h2>Implementation Frameworks and Practical Tools</h2>\n<p>NVIDIA TensorRT is the primary framework you need to understand for the certification, as it's NVIDIA's optimized inference engine specifically designed for their GPUs. TensorRT automatically applies multiple optimizations including quantization, layer fusion, kernel auto-tuning, and dynamic tensor memory allocation. When you provide TensorRT with a model and calibration data, it analyzes the model, determines optimal quantization strategies per layer, and produces an optimized execution engine. TensorRT supports both PTQ and QAT workflows through different APIs.</p>\n<p>For PTQ with TensorRT, you provide a calibration dataset (typically 500-1000 samples representative of your deployment distribution), and TensorRT runs calibration using entropy minimization or percentile-based methods to determine optimal quantization parameters for each layer. TensorRT can also perform mixed-precision quantization, where it keeps certain layers in FP16 if quantizing to INT8 would cause excessive accuracy loss. The builder generates different candidate implementations for each layer, benchmarks them on your target GPU, and selects the fastest implementation that meets accuracy constraints.</p>\n<p>PyTorch provides native quantization support through torch.quantization module, offering both eager mode and FX graph mode quantization. Eager mode uses torch.nn.quantized modules as drop-in replacements for standard layers, while FX graph mode (recommended for newer code) performs graph-level transformations for more comprehensive optimization. PyTorch supports static quantization (with calibration), dynamic quantization (computing scales per-batch), and QAT. For NVIDIA GPUs, you typically use PyTorch for QAT training, then export to ONNX format and import into TensorRT for deployment, combining PyTorch's training flexibility with TensorRT's inference performance.</p>\n<p>Other relevant tools include ONNX Runtime, which provides cross-platform quantization and inference capabilities, and Hugging Face Optimum, which offers quantization-specific features for transformer models including calibration, QAT recipes, and integration with various backends. For the exam, you should understand the typical workflow: train in PyTorch or TensorFlow, optionally perform QAT in the training framework, export to ONNX or TensorRT, apply PTQ if needed, validate accuracy, and profile performance. Understanding which tools are appropriate at which stages and how they interoperate is essential knowledge.</p>\n<h2>Decision Framework for Strategy Selection</h2>\n<p>Developing a systematic approach to choosing quantization strategies is crucial for the exam. Start with your constraints: What's your target hardware? What accuracy degradation is acceptable? Do you have access to training data and compute for QAT? What's your deployment timeline? These questions guide your decisions. If you're deploying to NVIDIA A100/H100 GPUs with hard real-time requirements and can tolerate 1-2% accuracy loss, INT8 PTQ through TensorRT is often the first approach to try. If accuracy is paramount and you have training resources, start with QAT to INT8 or even FP8 on H100.</p>\n<p>The decision tree typically follows this pattern: Begin with FP16 or BF16 as a baseline—this almost always works with negligible accuracy impact and provides substantial benefits over FP32. Measure the speedup and determine if it meets your requirements. If you need further optimization, attempt INT8 PTQ using TensorRT's calibration on representative data. Carefully validate accuracy on your full test set and any critical edge cases. If accuracy is acceptable, you're done—this path requires minimal effort. If accuracy is inadequate, you have several options: improve calibration (try different calibration methods or larger calibration datasets), use mixed-precision quantization (keep problematic layers in FP16), or invest in QAT if you have the resources.</p>\n<p>For transformer-based LLMs specifically, recent research suggests specific strategies. Models larger than 7B parameters typically quantize well to INT8 with PTQ, while smaller models often require QAT for acceptable INT8 accuracy. The attention layers are usually more sensitive to quantization than feedforward layers. Techniques like SmoothQuant specifically address transformer quantization challenges and should be considered for LLM deployment. NVIDIA's FasterTransformer library provides optimized quantized transformer implementations that can serve as references.</p>\n<p>Task-specific considerations matter significantly. Computer vision models (CNNs) generally quantize very well, often achieving negligible accuracy loss with INT8 PTQ. NLP models vary—BERT-family models quantize reasonably well, GPT-family models require more care, and very large models (100B+ parameters) may need specialized techniques like mixed-precision or group-wise quantization. Generative models used for creative tasks may be more sensitive to quantization quality than discriminative classifiers, because small distribution shifts can compound over multiple generation steps.</p>\n<h2>Practical Implementation Example Workflow</h2>\n<p>To solidify understanding for the exam, walk through a concrete example workflow. Suppose you're deploying a BERT-base model for text classification on NVIDIA A100 GPUs. You start with a PyTorch model trained in FP32 achieving 95.2% accuracy. First, you convert to FP16 using PyTorch's automatic mixed precision, which requires minimal code changes—just wrapping your model and adding gradient scaling. You validate and observe 95.1% accuracy with 1.8x inference speedup. This is your new baseline.</p>\n<p>Next, you target INT8 using TensorRT PTQ. You export your FP16 PyTorch model to ONNX format, then use TensorRT's Python API to create an INT8 engine. You prepare a calibration dataset of 1000 samples from your training set, ensuring it covers the diversity of your deployment distribution. TensorRT runs calibration using entropy minimization, builds an optimized INT8 engine, and reports per-layer statistics. You run validation and observe 93.8% accuracy—a 1.4% drop from FP16. The inference latency improves to 3.2x faster than the original FP32 model.</p>\n<p>The 1.4% accuracy drop is within your acceptable range, but you notice that performance on a specific subcategory has degraded more severely. You investigate TensorRT's layer-wise sensitivity analysis and identify that the final classification layer shows high quantization sensitivity. You rebuild the TensorRT engine with mixed precision, keeping the final layer in FP16 while quantizing the rest to INT8. This recovers some accuracy (94.3%) while maintaining most of the speedup (3.0x versus 3.2x for full INT8). This acceptable trade-off meets your deployment requirements.</p>\n<p>If accuracy had been insufficient, your next step would be QAT. You would use PyTorch's quantization-aware training module, inserting fake quantization operations throughout the model. You'd fine-tune for 5 epochs with a learning rate of 1e-5 (100x lower than original training), then export to TensorRT. QAT would likely recover accuracy to within 0.5% of the original FP32 model while maintaining INT8 inference benefits. Understanding this progression—baseline to FP16, PTQ attempt, mixed-precision if needed, QAT as last resort—provides a practical framework applicable to most quantization projects.</p>\n<h2>Key Takeaways and Exam Focus Areas</h2>\n<p>For the NVIDIA Gen AI LLM Professional Certification, synthesize your knowledge around these core competencies. Understand the quantization strategy spectrum from least invasive (FP16/BF16) to most aggressive (INT4/binary), and know when each is appropriate. Master the distinction between PTQ and QAT, including their relative advantages: PTQ's ease of implementation versus QAT's superior accuracy at low bit-widths. Recognize that activation quantization often determines overall system performance and requires careful calibration or dynamic quantization strategies.</p>\n<p>Hardware-specific knowledge is critical. Know that A100 and H100 Tensor Cores provide massive acceleration for reduced-precision operations, but only when properly utilized through libraries like TensorRT or cuBLAS. Understand that precision format selection depends on both model characteristics and hardware capabilities—FP8 on H100 versus INT8 on A100, for example. Be able to discuss accuracy measurement methodologies, including appropriate metrics for different task types and the importance of establishing proper baselines.</p>\n<p>Finally, develop practical implementation knowledge. Know the typical workflow using PyTorch for training/QAT and TensorRT for optimized inference. Understand calibration's role in PTQ and the various calibration methods (min-max, percentile, entropy). Recognize when mixed-precision quantization is appropriate for models with heterogeneous layer sensitivities. The exam will likely present scenarios asking you to choose appropriate strategies given constraints like target hardware, accuracy requirements, available resources, and deployment timeline—your ability to reason through these trade-offs systematically will determine your success on this certification topic.</p>"
      }
    }
  },
  "lastExport": null
}